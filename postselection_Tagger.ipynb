{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8020d75-5631-4723-b13b-9aec28a8d977",
   "metadata": {},
   "source": [
    "**Top Tagger Framework**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd081e3",
   "metadata": {},
   "source": [
    "**Framework version January 2024**\n",
    "- Log :\n",
    "    - understood bug in data (the problem was in CRAB not here)\n",
    "    - added Snapshot \n",
    "- Planned update :\n",
    "    - standalone code, prepare a couple of input parameters and make the code working with a single command\n",
    "    \n",
    "    \n",
    "__________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98655626-5252-41be-b111-76e638cecddf",
   "metadata": {},
   "source": [
    "**Code**\n",
    "\n",
    "Folder definition on Tier:\n",
    "- in the main folder */acagnott/* added folder 'remote_folder_name';\n",
    "- in \"Snapshots\" added the folder 'remote_subfolder_name';\n",
    "- in the subfolder through dask the snapshot will be copied with name \"snap_\"+label+\"_*.root\"\n",
    "\n",
    "\n",
    "Es: se lancio \"DataMETA_2018\", gli snapshot vengono salvati in ../acagnott/Snapshot/20231229/snap_DataMET_2018_*.root\n",
    "se viene lanciato \"QCD_2018\" viene creata la cartella /acagnott/Snapshot/20231229/snap_QCDHT_100to200_2018_*.root e così via per ogni components\n",
    "\n",
    "---> Viene usato solo il giorno in modo che tutti i sample lanciati lo stesso giorno verranno salvati nella stessa cartella con nomi diversi, visto che vengono lanciati in momenti diversi della giornata lo stesso tipo di job. Forse va modificato il formato se i singoli job iniziano a durare più di un giorno dato che viene comunque lanciato un sampel per volta (in tal caso verrebbero salvati in cartelle diverse. Si potrebbe pensare di mettere la data a mano, cioé invece di usare datetime.now() si potrebbe inserire la data manualmente per fare in modo di mettere tutti i file nella stessa folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf1ce47-e2f0-48f4-8e4b-b0fa9f1bacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "sched_port = 22358 #Dask port\n",
    "nmaxpartition = 100\n",
    "distributed = False#True#\n",
    "do_histos = True\n",
    "hist_folder = \"TopTagger_JetMET_selectioncomparison\"\n",
    "do_snapshot = True\n",
    "remote_subfolder_name = datetime.now().strftime(\"%Y%m%d\") #20231229\n",
    "\n",
    "in_dataset = [\n",
    "    # \"DataMETA_2018\", \n",
    "    # \"DataMETB_2018\", \n",
    "    # \"DataMETC_2018\",\n",
    "    # \"DataMETD_2018\",\n",
    "    # \"DataSingleMuA_2018\", \n",
    "    # \"DataSingleMuB_2018\", \n",
    "    # \"DataSingleMuC_2018\",\n",
    "    # \"DataSingleMuD_2018\",\n",
    "    \n",
    "    # \"TprimeToTZ_700_2018\",\n",
    "    # \"TprimeToTZ_1000_2018\", \n",
    "    # \"TprimeToTZ_1800_2018\",\n",
    "    \n",
    "    # \"QCD_2018\",\n",
    "    # \"TT_2018\",\n",
    "    # \"ZJetsToNuNu_2018\",\n",
    "    # \"WJets_2018\",\n",
    "    \n",
    "    # \"TT_semilep_2018\",\n",
    "    # \"TT_hadr_2018\",\n",
    "    # \"TT_Mtt700to1000_2018\",\n",
    "    # \"TT_Mtt1000toInf_2018\",\n",
    "    # \"QCDHT_100to200_2018\", \"QCDHT_200to300_2018\",\"QCDHT_300to500_2018\", \"QCDHT_500to700_2018\", \"QCDHT_700to1000_2018\",\"QCDHT_1000to1500_2018\", \"QCDHT_1500to2000_2018\",\"QCDHT_2000toInf_2018\",\"TT_hadr_2018\", \"TT_semilep_2018\", \"TT_Mtt700to1000_2018\", \"TT_Mtt1000toInf_2018\",\"ZJetsToNuNu_HT100to200_2018\", \"ZJetsToNuNu_HT200to400_2018\", \"ZJetsToNuNu_HT400to600_2018\", \"ZJetsToNuNu_HT600to800_2018\", \"ZJetsToNuNu_HT800to1200_2018\", \"ZJetsToNuNu_HT1200to2500_2018\", \"ZJetsToNuNu_HT2500toInf_2018\", \"WJetsHT100to200_2018\", \"WJetsHT200to400_2018\", \"WJetsHT400to600_2018\", \"WJetsHT600to800_2018\", \"WJetsHT800to1200_2018\", \"WJetsHT1200to2500_2018\", \"WJetsHT2500toInf_2018\"\n",
    "    # \"QCDHT_100to200_2018\", \n",
    "    # \"QCDHT_200to300_2018\",\n",
    "    # \"QCDHT_300to500_2018\", \n",
    "    # \"QCDHT_500to700_2018\", \n",
    "    # \"QCDHT_700to1000_2018\", \n",
    "    # \"QCDHT_1000to1500_2018\", \n",
    "    # \"QCDHT_1500to2000_2018\",\n",
    "    # \"QCDHT_2000toInf_2018\"\n",
    "    # \"WJetsHT100to200_2018\", \n",
    "    # \"WJetsHT200to400_2018\", \n",
    "    # \"WJetsHT400to600_2018\", \n",
    "    # \"WJetsHT600to800_2018\", \n",
    "    # \"WJetsHT800to1200_2018\",\n",
    "    # \"WJetsHT1200to2500_2018\", \n",
    "    \"WJetsHT2500toInf_2018\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e16aaf3-78d5-4a3c-851c-21f771dcf586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.27/01\n",
      "/tmp/x509up_u0 /cvmfs/grid.cern.ch/etc/grid-security/certificates/\n",
      "You are producing histograms\n",
      "You are producing snapshot\n",
      "You are saving snapshots in local\n",
      "folder name : ./results/TopTagger_JetMET_selectioncomparison/\n",
      "local folder histos: ./results/TopTagger_JetMET_selectioncomparison/\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import os\n",
    "from utils.samples import *\n",
    "from utils.variables_tagger import *\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from dask.distributed import Client\n",
    "import psutil\n",
    "from psutil import cpu_percent\n",
    "ROOT.RDF.Experimental.Distributed.open_files_locally = False\n",
    "\n",
    "os.environ['X509_CERT_DIR'] = \"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "os.environ['X509_USER_PROXY'] = \"/tmp/x509up_u0\"\n",
    "print(os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\"))\n",
    "\n",
    "\n",
    "if distributed:\n",
    "    nfiles_max = 10000\n",
    "else:\n",
    "    nfiles_max = 1  #######\n",
    "\n",
    "# Cosa aggiungere, modificare la cartella sul tier in questo modo ../acagnott/Snapshot_rdf/*dataset_name*/*data di processamente con orario*/\n",
    "# insomma come fa crab\n",
    "\n",
    "\n",
    "if do_histos: print(\"You are producing histograms\")\n",
    "if do_snapshot: print(\"You are producing snapshot\")\n",
    "\n",
    "remote_folder_name = \"Snapshots\"\n",
    "#output histos folder\n",
    "folder = \"./results/\"+hist_folder+\"/\"\n",
    "# eos_folder = \"/eos/home-a/acagnott/DarkMatter/nosynch/\"+hist_folder\n",
    "\n",
    "if do_snapshot and remote_subfolder_name == datetime.now().strftime(\"%Y%m%d\") and distributed: \n",
    "    print(\"You are naming the tier subfolder using the current day \\n\")\n",
    "    print(\"Snapshots folder name : ~/acagnott/{}/{}\".format(remote_folder_name, remote_subfolder_name))\n",
    "elif do_snapshot and distributed:\n",
    "    print(\"You are naming the tier subfolder manually\")\n",
    "    print(\"Snapshots folder name : ~/acagnott/{}/{}\".format(remote_folder_name, remote_subfolder_name))\n",
    "elif do_snapshot:\n",
    "    print(\"You are saving snapshots in local\")\n",
    "    print(\"folder name : \" + folder)\n",
    "\n",
    "\n",
    "if do_histos : \n",
    "    print(\"local folder histos: {}\".format(folder))\n",
    "    # print(\"At the end of the job the histos will be transfer to eos storage\")\n",
    "    # print(\"eos folder : {}\".format(eos_folder))\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "repohisto = folder+\"plots/\"\n",
    "if not os.path.exists(repohisto):\n",
    "    os.mkdir(repohisto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe0ba0b-23a0-4b47-96f6-4dc0917ec6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating folders on Tier\n",
    "if do_snapshot and distributed:\n",
    "    tier_main_folder = \"davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/\"\n",
    "    os.popen(\"davix-mkdir davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{} -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name))\n",
    "    os.popen(\"davix-mkdir davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{} -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name, remote_subfolder_name))\n",
    "    \n",
    "# transfer function for dask worker\n",
    "def transfer_to_tier(dask_worker):\n",
    "    import os\n",
    "    os.popen('for filename in snapTagger_*.root; do davix-put $filename davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{}/$filename -E ./proxy --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/; done'.format(remote_folder_name, remote_subfolder_name))\n",
    "    return True #, os.popen(\"for filename in ./test_*.root; do echo $filename; done\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac5975c9-41cf-483d-9c51-a36a0ed97f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worker_cpu_usage(client):\n",
    "    workers = list(client.scheduler_info()[\"workers\"].keys())\n",
    "    cpu_usage = {}\n",
    "\n",
    "    for worker in workers:\n",
    "        # Ottieni l'utilizzo della CPU per ogni worker\n",
    "        cpu_percent = client.run(psutil.cpu_percent)\n",
    "\n",
    "        # Calcola la media dell'utilizzo della CPU per ogni worker\n",
    "        avg_cpu_percent = sum(cpu_percent.values()) / len(cpu_percent)\n",
    "        \n",
    "        # Memorizza i risultati nell'oggetto cpu_usage\n",
    "        cpu_usage[worker] = avg_cpu_percent\n",
    "\n",
    "    return cpu_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3271c9-56c5-4ee1-a501-f85177699ab8",
   "metadata": {},
   "source": [
    "- Import of utils from variables.py\n",
    "Cut (if any), Regions, Variables\n",
    "\n",
    "- syncro between in_dataset and sample_dict (from sample.py) to syncronize labels and ather featurs of the dataset (as sigma if needed)\n",
    "- import of samples_dict.json to load files list (path to reach them on tier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f446ef-90d9-46e8-ad16-8ded4fa2fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions to book: \n",
      "  All\n",
      "  Presel\n",
      "  PreselWpt\n",
      "  1TopLep\n",
      "  1TopLepWpt\n",
      "  1TopLep_1TopHadrRes_WPloose\n",
      "  1TopLep_1TopHadrMix_WPloose\n",
      "  1TopLep_1TopHadrMer_WPloose\n",
      "  1TopLep_1TopHadrAll_WPloose\n",
      "  1TopLep_1TopHadrResWpt_WPloose\n",
      "  1TopLep_1TopHadrMixWpt_WPloose\n",
      "  1TopLep_1TopHadrMerWpt_WPloose\n",
      "  1TopLep_1TopHadrAllWpt_WPloose\n",
      "  1TopLep_1TopHadrRes_WPmedium\n",
      "  1TopLep_1TopHadrMix_WPmedium\n",
      "  1TopLep_1TopHadrMer_WPmedium\n",
      "  1TopLep_1TopHadrAll_WPmedium\n",
      "  1TopLep_1TopHadrResWpt_WPmedium\n",
      "  1TopLep_1TopHadrMixWpt_WPmedium\n",
      "  1TopLep_1TopHadrMerWpt_WPmedium\n",
      "  1TopLep_1TopHadrAllWpt_WPmedium\n",
      "  1TopLep_1TopHadrRes_WPtight\n",
      "  1TopLep_1TopHadrMix_WPtight\n",
      "  1TopLep_1TopHadrMer_WPtight\n",
      "  1TopLep_1TopHadrAll_WPtight\n",
      "  1TopLep_1TopHadrResWpt_WPtight\n",
      "  1TopLep_1TopHadrMixWpt_WPtight\n",
      "  1TopLep_1TopHadrMerWpt_WPtight\n",
      "  1TopLep_1TopHadrAllWpt_WPtight\n",
      "Variables for histograms :\n",
      "['MET_pt', 'MET_phi', 'PuppiMET_pt', 'nGoodJet', 'nJetBtag', 'Top_mass_WPloose', 'Top_pt_WPloose', 'Top_mass_WPmedium', 'Top_pt_WPmedium', 'Top_mass_WPtight', 'Top_pt_WPtight', 'Top_mass_Resolved', 'Top_pt_Resolved', 'Top_score_Resolved', 'Top_mass_Mixed', 'Top_pt_Mixed', 'Top_score_Mixed', 'nClusterT1TopMixed', 'nClusterT2TopMixed', 'nClusterT1TopResolved', 'nClusterT2TopResolved', 'nTopClusterT1MixedReco_WPloose', 'nTopClusterT2MixedReco_WPloose', 'nTopClusterT1ResolvedReco_WPloose', 'nTopClusterT2ResolvedReco_WPloose', 'nTopClusterT1MixedReco_WPmedium', 'nTopClusterT2MixedReco_WPmedium', 'nTopClusterT1ResolvedReco_WPmedium', 'nTopClusterT2ResolvedReco_WPmedium', 'nTopClusterT1MixedReco_WPtight', 'nTopClusterT2MixedReco_WPtight', 'nTopClusterT1ResolvedReco_WPtight', 'nTopClusterT2ResolvedReco_WPtight', 'TopMixed_TopScore', 'TopMixed_Score_truthstandard', 'TopMixed_Score_truth1_incl', 'TopMixed_Score_truth2_incl', 'TopMixed_Score_truth1', 'TopMixed_Score_truth2', 'TopResolved_TopScore', 'TopResolved_Score_truthstandard', 'TopResolved_Score_truth1', 'TopResolved_Score_truth2', 'TopResolved_Score_truth1_incl', 'TopResolved_Score_truth2_incl', 'nTopClusterT1ResolvedMCTagstandard', 'nTopClusterT2ResolvedMCTagstandard', 'TopClusterT1ResolvedMCTagTrueFirstClusterstandard', 'TopClusterT2ResolvedMCTagTrueFirstClusterstandard', 'TopClusterT1ResolvedMCTagFalseFirstClusterstandard', 'TopClusterT2ResolvedMCTagFalseFirstClusterstandard', 'nTopClusterT2ResolvedTrueRecostandard_WPloose', 'nTopClusterT1ResolvedTrueRecostandard_WPloose', 'TopClusterT1ResolvedTrueRecoFirstClusterstandard_WPloose', 'TopClusterT2ResolvedTrueRecoFirstClusterstandard_WPloose', 'TopClusterT1ResolvedFalseRecoFirstClusterstandard_WPloose', 'TopClusterT2ResolvedFalseRecoFirstClusterstandard_WPloose', 'nTopClusterT2ResolvedTrueRecostandard_WPmedium', 'nTopClusterT1ResolvedTrueRecostandard_WPmedium', 'TopClusterT1ResolvedTrueRecoFirstClusterstandard_WPmedium', 'TopClusterT2ResolvedTrueRecoFirstClusterstandard_WPmedium', 'TopClusterT1ResolvedFalseRecoFirstClusterstandard_WPmedium', 'TopClusterT2ResolvedFalseRecoFirstClusterstandard_WPmedium', 'nTopClusterT2ResolvedTrueRecostandard_WPtight', 'nTopClusterT1ResolvedTrueRecostandard_WPtight', 'TopClusterT1ResolvedTrueRecoFirstClusterstandard_WPtight', 'TopClusterT2ResolvedTrueRecoFirstClusterstandard_WPtight', 'TopClusterT1ResolvedFalseRecoFirstClusterstandard_WPtight', 'TopClusterT2ResolvedFalseRecoFirstClusterstandard_WPtight', 'nTopClusterT1ResolvedMCTagtruth1', 'nTopClusterT2ResolvedMCTagtruth1', 'TopClusterT1ResolvedMCTagTrueFirstClustertruth1', 'TopClusterT2ResolvedMCTagTrueFirstClustertruth1', 'TopClusterT1ResolvedMCTagFalseFirstClustertruth1', 'TopClusterT2ResolvedMCTagFalseFirstClustertruth1', 'nTopClusterT2ResolvedTrueRecotruth1_WPloose', 'nTopClusterT1ResolvedTrueRecotruth1_WPloose', 'TopClusterT1ResolvedTrueRecoFirstClustertruth1_WPloose', 'TopClusterT2ResolvedTrueRecoFirstClustertruth1_WPloose', 'TopClusterT1ResolvedFalseRecoFirstClustertruth1_WPloose', 'TopClusterT2ResolvedFalseRecoFirstClustertruth1_WPloose', 'nTopClusterT2ResolvedTrueRecotruth1_WPmedium', 'nTopClusterT1ResolvedTrueRecotruth1_WPmedium', 'TopClusterT1ResolvedTrueRecoFirstClustertruth1_WPmedium', 'TopClusterT2ResolvedTrueRecoFirstClustertruth1_WPmedium', 'TopClusterT1ResolvedFalseRecoFirstClustertruth1_WPmedium', 'TopClusterT2ResolvedFalseRecoFirstClustertruth1_WPmedium', 'nTopClusterT2ResolvedTrueRecotruth1_WPtight', 'nTopClusterT1ResolvedTrueRecotruth1_WPtight', 'TopClusterT1ResolvedTrueRecoFirstClustertruth1_WPtight', 'TopClusterT2ResolvedTrueRecoFirstClustertruth1_WPtight', 'TopClusterT1ResolvedFalseRecoFirstClustertruth1_WPtight', 'TopClusterT2ResolvedFalseRecoFirstClustertruth1_WPtight', 'nTopClusterT1ResolvedMCTagtruth2', 'nTopClusterT2ResolvedMCTagtruth2', 'TopClusterT1ResolvedMCTagTrueFirstClustertruth2', 'TopClusterT2ResolvedMCTagTrueFirstClustertruth2', 'TopClusterT1ResolvedMCTagFalseFirstClustertruth2', 'TopClusterT2ResolvedMCTagFalseFirstClustertruth2', 'nTopClusterT2ResolvedTrueRecotruth2_WPloose', 'nTopClusterT1ResolvedTrueRecotruth2_WPloose', 'TopClusterT1ResolvedTrueRecoFirstClustertruth2_WPloose', 'TopClusterT2ResolvedTrueRecoFirstClustertruth2_WPloose', 'TopClusterT1ResolvedFalseRecoFirstClustertruth2_WPloose', 'TopClusterT2ResolvedFalseRecoFirstClustertruth2_WPloose', 'nTopClusterT2ResolvedTrueRecotruth2_WPmedium', 'nTopClusterT1ResolvedTrueRecotruth2_WPmedium', 'TopClusterT1ResolvedTrueRecoFirstClustertruth2_WPmedium', 'TopClusterT2ResolvedTrueRecoFirstClustertruth2_WPmedium', 'TopClusterT1ResolvedFalseRecoFirstClustertruth2_WPmedium', 'TopClusterT2ResolvedFalseRecoFirstClustertruth2_WPmedium', 'nTopClusterT2ResolvedTrueRecotruth2_WPtight', 'nTopClusterT1ResolvedTrueRecotruth2_WPtight', 'TopClusterT1ResolvedTrueRecoFirstClustertruth2_WPtight', 'TopClusterT2ResolvedTrueRecoFirstClustertruth2_WPtight', 'TopClusterT1ResolvedFalseRecoFirstClustertruth2_WPtight', 'TopClusterT2ResolvedFalseRecoFirstClustertruth2_WPtight', 'nTopClusterT1MixedMCTagstandard', 'nTopClusterT2MixedMCTagstandard', 'TopClusterT1MixedMCTagTrueFirstClusterstandard', 'TopClusterT2MixedMCTagTrueFirstClusterstandard', 'TopClusterT1MixedMCTagFalseFirstClusterstandard', 'TopClusterT2MixedMCTagFalseFirstClusterstandard', 'nTopClusterT2MixedTrueRecostandard_WPloose', 'nTopClusterT1MixedTrueRecostandard_WPloose', 'TopClusterT1MixedTrueRecoFirstClusterstandard_WPloose', 'TopClusterT2MixedTrueRecoFirstClusterstandard_WPloose', 'TopClusterT1MixedFalseRecoFirstClusterstandard_WPloose', 'TopClusterT2MixedFalseRecoFirstClusterstandard_WPloose', 'nTopClusterT2MixedTrueRecostandard_WPmedium', 'nTopClusterT1MixedTrueRecostandard_WPmedium', 'TopClusterT1MixedTrueRecoFirstClusterstandard_WPmedium', 'TopClusterT2MixedTrueRecoFirstClusterstandard_WPmedium', 'TopClusterT1MixedFalseRecoFirstClusterstandard_WPmedium', 'TopClusterT2MixedFalseRecoFirstClusterstandard_WPmedium', 'nTopClusterT2MixedTrueRecostandard_WPtight', 'nTopClusterT1MixedTrueRecostandard_WPtight', 'TopClusterT1MixedTrueRecoFirstClusterstandard_WPtight', 'TopClusterT2MixedTrueRecoFirstClusterstandard_WPtight', 'TopClusterT1MixedFalseRecoFirstClusterstandard_WPtight', 'TopClusterT2MixedFalseRecoFirstClusterstandard_WPtight', 'nTopClusterT1MixedMCTagtruth1', 'nTopClusterT2MixedMCTagtruth1', 'TopClusterT1MixedMCTagTrueFirstClustertruth1', 'TopClusterT2MixedMCTagTrueFirstClustertruth1', 'TopClusterT1MixedMCTagFalseFirstClustertruth1', 'TopClusterT2MixedMCTagFalseFirstClustertruth1', 'nTopClusterT2MixedTrueRecotruth1_WPloose', 'nTopClusterT1MixedTrueRecotruth1_WPloose', 'TopClusterT1MixedTrueRecoFirstClustertruth1_WPloose', 'TopClusterT2MixedTrueRecoFirstClustertruth1_WPloose', 'TopClusterT1MixedFalseRecoFirstClustertruth1_WPloose', 'TopClusterT2MixedFalseRecoFirstClustertruth1_WPloose', 'nTopClusterT2MixedTrueRecotruth1_WPmedium', 'nTopClusterT1MixedTrueRecotruth1_WPmedium', 'TopClusterT1MixedTrueRecoFirstClustertruth1_WPmedium', 'TopClusterT2MixedTrueRecoFirstClustertruth1_WPmedium', 'TopClusterT1MixedFalseRecoFirstClustertruth1_WPmedium', 'TopClusterT2MixedFalseRecoFirstClustertruth1_WPmedium', 'nTopClusterT2MixedTrueRecotruth1_WPtight', 'nTopClusterT1MixedTrueRecotruth1_WPtight', 'TopClusterT1MixedTrueRecoFirstClustertruth1_WPtight', 'TopClusterT2MixedTrueRecoFirstClustertruth1_WPtight', 'TopClusterT1MixedFalseRecoFirstClustertruth1_WPtight', 'TopClusterT2MixedFalseRecoFirstClustertruth1_WPtight', 'nTopClusterT1MixedMCTagtruth2', 'nTopClusterT2MixedMCTagtruth2', 'TopClusterT1MixedMCTagTrueFirstClustertruth2', 'TopClusterT2MixedMCTagTrueFirstClustertruth2', 'TopClusterT1MixedMCTagFalseFirstClustertruth2', 'TopClusterT2MixedMCTagFalseFirstClustertruth2', 'nTopClusterT2MixedTrueRecotruth2_WPloose', 'nTopClusterT1MixedTrueRecotruth2_WPloose', 'TopClusterT1MixedTrueRecoFirstClustertruth2_WPloose', 'TopClusterT2MixedTrueRecoFirstClustertruth2_WPloose', 'TopClusterT1MixedFalseRecoFirstClustertruth2_WPloose', 'TopClusterT2MixedFalseRecoFirstClustertruth2_WPloose', 'nTopClusterT2MixedTrueRecotruth2_WPmedium', 'nTopClusterT1MixedTrueRecotruth2_WPmedium', 'TopClusterT1MixedTrueRecoFirstClustertruth2_WPmedium', 'TopClusterT2MixedTrueRecoFirstClustertruth2_WPmedium', 'TopClusterT1MixedFalseRecoFirstClustertruth2_WPmedium', 'TopClusterT2MixedFalseRecoFirstClustertruth2_WPmedium', 'nTopClusterT2MixedTrueRecotruth2_WPtight', 'nTopClusterT1MixedTrueRecotruth2_WPtight', 'TopClusterT1MixedTrueRecoFirstClustertruth2_WPtight', 'TopClusterT2MixedTrueRecoFirstClustertruth2_WPtight', 'TopClusterT1MixedFalseRecoFirstClustertruth2_WPtight', 'TopClusterT2MixedFalseRecoFirstClustertruth2_WPtight']\n",
      "Variables for histograms 2D :\n",
      "[]\n",
      "{'TopResolved': {'standard': 'TopResolved_truth', 'truth1': 'TopResolved_truth1', 'truth2': 'TopResolved_truth2'}, 'TopMixed': {'standard': 'TopMixed_truth', 'truth1': 'TopMixed_truth1', 'truth2': 'TopMixed_truth2'}}\n",
      "Datasets to process :  ['WJetsHT2500toInf_2018']\n",
      "Dataset : WJetsHT2500toInf_2018\n",
      "# of files to process :  1\n",
      "files strings :\n",
      "  root://cms-xrd-global.cern.ch//store/user/acagnott/DM_Run3_v0/WJetsToLNu_HT-2500ToInf_TuneCP5_13TeV-madgraphMLM-pythia8/WJetsHT2500toInf_2018/231222_110712/0000/tree_hadd_23.root\n",
      "\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  3126.0\n"
     ]
    }
   ],
   "source": [
    "# Common cuts over the different regions\n",
    "cut = requirements # ---> see variables.py\n",
    "\n",
    "# Regions definition\n",
    "regions_def = regions # ---> see variables.py\n",
    "print(\"Regions to book: \")\n",
    "for r in regions_def.keys():\n",
    "    print(\"  \"+r)\n",
    "    \n",
    "# Samples dictionary --> to load the files of each samples    \n",
    "sample_file = open(\"utils/dict_samples.json\", \"rb\")\n",
    "samples = json.load(sample_file)\n",
    "sample_file.close()\n",
    "\n",
    "# WP top taggers\n",
    "Top_threshold = {\"Resolved\" : {'WPloose': \"0.24193972\", 'WPmedium': \"0.5411276\", 'WPtight': \"0.77197933\"},\n",
    "                 \"Mixed\"    :{'WPloose': \"0.2957885265350342\", 'WPmedium': \"0.7584613561630249\", 'WPtight': \"0.9129540324211121\"}}\n",
    "TopMerged_threshold = {'WPloose': \"0.85\", 'WPmedium': \"0.94\", 'WPtight': \"0.97\"}\n",
    "# Variables definition \n",
    "var = vars  # ---> variables.py\n",
    "var2d = vars2D \n",
    "\n",
    "print(\"Variables for histograms :\")\n",
    "print([v._name for v in var])\n",
    "print(\"Variables for histograms 2D :\")\n",
    "print([v._name for v in var2d])\n",
    "\n",
    "# Alternative definition of truth\n",
    "\n",
    "print(TruthType)\n",
    "\n",
    "# Snapshots branches\n",
    "\n",
    "branches = {\"isMC\",\"year\",\"HEMVeto\",\"HT_eventHT\", \"MET_pt\", \"MET_phi\", \"w_nominal\",\n",
    "            \"Top_score_Resolved\", \"Top_pt_Resolved\", \"Top_eta_Resolved\", \"Top_phi_Resolved\", \"Top_mass_Resolved\", \"Top_idx_Resolved\",\n",
    "            \"Top_score_Mixed\", \"Top_pt_Mixed\", \"Top_eta_Mixed\", \"Top_phi_Mixed\", \"Top_mass_Mixed\", \"Top_idx_Mixed\",\n",
    "            \"Top_idx_Resolved\", \"Top_idx_Mixed\", \"NGoodTopMu\",\"TopMerged_bestTopTruth\", \"TopMerged_bestTopScore\",\n",
    "            \"TopMerged_bestTopPt\", \"TopMerged_bestTopEta\", \"TopMerged_bestTopPhi\", \"TopMerged_bestTopMass\", \"TopMerged_truth_exists\",\n",
    "            # # \"BJetTopLep\", \"BJetTopLep_MCtag\", }\n",
    "            \"BJetTopMu\"}\n",
    "\n",
    "for wp in [\"WPloose\",\"WPmedium\",\"WPtight\"]:\n",
    "    branches.update([\"Top_mass_\"+wp, \"Top_pt_\"+wp, \"EventTopCategory_\"+wp])\n",
    "            \n",
    "for top in topTypes:\n",
    "    branches.update([\n",
    "        \"nClusterT2Top\"+top+\"\",\n",
    "        \"nClusterT1Top\"+top+\"\"])\n",
    "    for wp in [\"WPloose\",\"WPmedium\",\"WPtight\"]:\n",
    "        branches.update([\"nTopClusterT2\"+top+\"Reco_\"+wp,\n",
    "        \"nTopClusterT1\"+top+\"Reco_\"+wp])\n",
    "for t in TruthType.keys():\n",
    "        top = t.replace(\"Top\",\"\")\n",
    "        for tr in TruthType[t].keys():\n",
    "            branches.update([\"nTopClusterT2\"+top+\"MCTag\"+tr,\n",
    "                             \"nTopClusterT1\"+top+\"MCTag\"+tr,\n",
    "                             \"TopClusterT2\"+top+\"MCTagTrueFirstCluster\"+tr,\n",
    "                             \"TopClusterT1\"+top+\"MCTagTrueFirstCluster\"+tr,\n",
    "                             \"TopClusterT2\"+top+\"MCTagFalseFirstCluster\"+tr,\n",
    "                             \"TopClusterT1\"+top+\"MCTagFalseFirstCluster\"+tr\n",
    "                             ])\n",
    "            for wp in [\"WPloose\",\"WPmedium\",\"WPtight\"]:\n",
    "                branches.update([\"nTopClusterT2\"+top+\"TrueReco\"+tr+\"_\"+wp,\n",
    "                             \"nTopClusterT1\"+top+\"TrueReco\"+tr+\"_\"+wp,\n",
    "                             \"TopClusterT2\"+top+\"TrueRecoFirstCluster\"+tr+\"_\"+wp,\n",
    "                             \"TopClusterT1\"+top+\"TrueRecoFirstCluster\"+tr+\"_\"+wp,\n",
    "                             \"TopClusterT2\"+top+\"FalseRecoFirstCluster\"+tr+\"_\"+wp,\n",
    "                             \"TopClusterT1\"+top+\"FalseRecoFirstCluster\"+tr+\"_\"+wp])\n",
    "\n",
    "\n",
    "# Loading datasets choose at the beginning of the notebook\n",
    "datasets = []\n",
    "for in_d in in_dataset:\n",
    "    if not in_d in sample_dict.keys():\n",
    "        print(\"Check the in_dataset string... \", sample_dict.keys())\n",
    "    else : \n",
    "        datasets.append(sample_dict[in_d])\n",
    "print(\"Datasets to process : \", [d.label for d in datasets])\n",
    "\n",
    "# Loading the files strings to pass to the DataFrame\n",
    "chain = {}\n",
    "ntot_events = {}\n",
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        samples_list = d.components\n",
    "    else:\n",
    "        samples_list = [d]\n",
    "    chain[d.label] = {}\n",
    "    ntot_events[d.label] = {}\n",
    "    for s in samples_list:\n",
    "        if distributed: \n",
    "            nfiles = len(samples[d.label][s.label]['strings'])\n",
    "            \n",
    "            # for i, string in enumerate(samples[d.label][s.label]['strings']): \n",
    "            #     samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"root://stormgf2.pi.infn.it/\")\n",
    "            \n",
    "            # chain[d.label][s.label] = samples[d.label][s.label]['strings']\n",
    "            if nfiles_max>len(samples[d.label][s.label]['strings']): \n",
    "                chain[d.label][s.label] = samples[d.label][s.label]['strings']\n",
    "            else: \n",
    "                chain[d.label][s.label] = samples[d.label][s.label]['strings'][:nfiles_max]\n",
    "        else: \n",
    "            nfiles = nfiles_max\n",
    "            chain[d.label][s.label] = samples[d.label][s.label]['strings'][:nfiles]\n",
    "        if not \"Data\" in s.label: ntot_events[d.label][s.label] = np.sum(samples[d.label][s.label]['ntot'][:nfiles])\n",
    "        else: ntot_events[d.label][s.label] = None\n",
    "        print(\"Dataset : \"+s.label)\n",
    "        print(\"# of files to process : \", nfiles)\n",
    "        if distributed and nfiles_max>2:\n",
    "            print(\"files strings :\\n  {}\\n  {}\\n  ... \\n  {}\\n  {}\".format(chain[d.label][s.label][0], chain[d.label][s.label][1], chain[d.label][s.label][-2], chain[d.label][s.label][-1]))\n",
    "        else :\n",
    "            print(\"files strings :\\n  {}\".format(chain[d.label][s.label][0]))\n",
    "        print(\"# of total events in the files to process (MC only, if Data the number is None) : \", ntot_events[d.label][s.label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54586805-5e2c-4b36-a13b-11497d5f2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invLepveto = \"\"#\"!\" # per non invertirlo basta passare stinga vuota\n",
    "# met_cut = 250\n",
    "# mdphi_cut = 0\n",
    "# HLT_filter = \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight || HLT_Ele32_WPTight_Gsf || HLT_Ele115_CaloIdVT_GsfTrkIdT || HLT_Photon200 || HLT_IsoMu24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a9be5b-2e52-431b-9709-ee1c3e639b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "end of initialization\n"
     ]
    }
   ],
   "source": [
    "# initialization of clusters\n",
    "\n",
    "# upload the proxyfile to the Dask workers to make them able to access data on the grid \n",
    "\n",
    "from distributed.diagnostics.plugin import UploadFile\n",
    "def set_proxy(dask_worker):\n",
    "    import os\n",
    "    import shutil\n",
    "    working_dir = dask_worker.local_directory\n",
    "    proxy_name = 'x509up_u0'\n",
    "    os.environ['X509_USER_PROXY'] = working_dir + '/' + proxy_name\n",
    "    os.environ['X509_CERT_DIR']=\"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "    shutil.copyfile(working_dir + '/' + proxy_name, working_dir + '/../../../proxy')    \n",
    "    os.environ['EXTRA_CLING_ARGS'] = \"-O2\"\n",
    "    return os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\")\n",
    "\n",
    "text_file = open(\"utils/postselection_tagger.h\", \"r\")\n",
    "data = text_file.read()\n",
    "def my_initialization_function():\n",
    "    print(ROOT.gInterpreter.ProcessLine(\".O\"))\n",
    "    ROOT.gInterpreter.Declare('{}'.format(data))\n",
    "    print(\"end of initialization\")\n",
    "\n",
    "# set up everything properly\n",
    "if distributed == True:\n",
    "    RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame\n",
    "    client = Client(address=\"tcp://127.0.0.1:\"+str(sched_port))\n",
    "    client.restart()\n",
    "    client.register_worker_plugin(UploadFile(\"/tmp/x509up_u0\"))\n",
    "    client.run(set_proxy)\n",
    "    ROOT.RDF.Experimental.Distributed.initialize(my_initialization_function)\n",
    "else:\n",
    "    RDataFrame = ROOT.RDataFrame\n",
    "    my_initialization_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963af90a-de0c-44a7-9b64-3927c34eeffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### utils ###################\n",
    "def cut_string(cut):\n",
    "    return cut.replace(\" \", \"\").replace(\"&&\",\"_\").replace(\">\",\"_g_\").replace(\".\",\"_\").replace(\"==\",\"_e_\")\n",
    "\n",
    "############### trigger selection #####################\n",
    "def trigger_filter(df):\n",
    "    df_trig = df.Filter(\"HLT_Mu50 || HLT_IsoMu24 || HLT_OldMu100 || HLT_TkMu100 || HLT_Ele32_WPTight_Gsf\", \"trigger\")\n",
    "    return df_trig\n",
    "################### preselection ###############\n",
    "def goodObjects(df, btagAlg, year, EE):\n",
    "    df_tmp = df.Define(\"GoodMu_idx\", \"GetGoodMuon( Muon_pt,  Muon_eta,  Muon_tightId)\")\\\n",
    "               .Define(\"nVetoEl\", \"nVetoElectron(Electron_pt, Electron_cutBased, Electron_eta)\")\\\n",
    "               .Define(\"GoodJet_idx\", \"GetGoodJet(Jet_pt, Jet_jetId, FatJet_eta)\")\\\n",
    "               .Define(\"GoodFatJet_idx\", \"GetGoodJet(FatJet_pt, FatJet_jetId, FatJet_eta)\")\\\n",
    "               .Define(\"JetBTag_idx\", \"GetJetBTag(GoodJet_idx, \"+bTagAlg+\",\"+str(year)+\",\"+str(EE)+\")\")\\\n",
    "               .Define(\"nGoodJet\", \"static_cast<int>(GoodJet_idx.size());\")\\\n",
    "               .Define(\"nJetBtag\", \"static_cast<int>(JetBTag_idx.size());\")\n",
    "    return df_tmp\n",
    "############### top selection ########################\n",
    "def LeptonTopTag(df, isMC):\n",
    "    # df_ = df.Filter(\"atLeast1GoodLep(GoodMu_idx, GoodEl_idx)\").Filter(\"nJetBtag>0\")\\\n",
    "    df_ = df.Filter(\"GoodMu_idx.size()==1\").Filter(\"nJetBtag>0\")\\\n",
    "            .Define(\"Muon_px\", \"Muon_pt[GoodMu_idx[0]]*sin(Muon_phi[GoodMu_idx[0]])\").Define(\"Muon_py\", \"Muon_pt[GoodMu_idx[0]]*cos(Muon_phi[GoodMu_idx[0]])\")\\\n",
    "            .Define(\"MET_px\", \"MET_pt*sin(MET_phi)\").Define(\"MET_py\", \"MET_pt*cos(MET_phi)\")\\\n",
    "            .Define(\"W_pt\", \"sqrt(pow(Muon_px+MET_px, 2)+pow(Muon_py+MET_py, 2))\")\\\n",
    "            .Define(\"GoodTopMuBJet_idx\",\"countTopMu(GoodMu_idx, Muon_eta, Muon_phi, JetBTag_idx, Jet_eta, Jet_phi)\")\\\n",
    "            .Define(\"GoodTopMuBJet_dr\", \"drBMu(GoodMu_idx, Muon_eta, Muon_phi, JetBTag_idx, Jet_eta, Jet_phi)\")\\\n",
    "            .Define(\"NGoodTopMu\", \"static_cast<int>(GoodTopMuBJet_idx.size());\")\\\n",
    "            .Define(\"BJetTopMu\", \"nearest(GoodTopMuBJet_idx, GoodTopMuBJet_dr)\")\n",
    "            # .Define(\"GoodTopLepBJet_idx\",\"countTopLep(GoodMu_idx, Muon_eta, Muon_phi, GoodEl_idx, Electron_eta, Electron_phi, JetBTag_idx, Jet_eta, Jet_phi)\")\\\n",
    "            # .Define(\"GoodTopLepBJet_dr\", \"drBLep(GoodMu_idx, Muon_eta, Muon_phi, GoodEl_idx, Electron_eta, Electron_phi, JetBTag_idx, Jet_eta, Jet_phi)\")\\\n",
    "            # .Define(\"NGoodTopLep\", \"static_cast<int>(GoodTopLepBJet_idx.size());\")\\\n",
    "            # .Define(\"BJetTopLep\", \"nearest(GoodTopLepBJet_idx, GoodTopLepBJet_dr)\")\n",
    "    # if isMC:\n",
    "    #     df_ = df_.Define(\"BJetTopLep_MCtag\", \"BJetTopLep_MCtag(Jet_matched, Jet_pdgId, Jet_topMother, BJetTopLep)\")\n",
    "    return df_\n",
    "\n",
    "def select_top(df, isMC):\n",
    "\n",
    "    for top in topTypes:\n",
    "        if top == \"Resolved\": idxFatJet = \"{}\"\n",
    "        else: idxFatJet = \"Top\"+top+\"_idxFatJet\"\n",
    "        df = df.Define(\"ClusterT1Top\"+top, \"findTopClustersType1(Top\"+top+\"_TopScore, \"+idxFatJet+\", Top\"+top+\"_idxJet0, Top\"+top+\"_idxJet1, Top\"+top+\"_idxJet2,BJetTopMu)\")\\\n",
    "                            .Define(\"ClusterT2Top\"+top, \"findTopClustersType2(Top\"+top+\"_TopScore, \"+idxFatJet+\", Top\"+top+\"_idxJet0, Top\"+top+\"_idxJet1, Top\"+top+\"_idxJet2,BJetTopMu)\")\\\n",
    "                            .Define(\"nClusterT1Top\"+top, \"static_cast<int> (ClusterT1Top\"+top+\".size());\")\\\n",
    "                            .Define(\"nClusterT2Top\"+top, \"static_cast<int> (ClusterT2Top\"+top+\".size());\")\n",
    "        for wp in [\"WPloose\",\"WPmedium\",\"WPtight\"]:\n",
    "            df = df.Define(\"ClusterT1Top\"+top+\"Reco_\"+wp, \"RecoTaggedTopCluster\"+top+\"(ClusterT1Top\"+top+\", Top\"+top+\"_TopScore, \"+Top_threshold[top][wp]+\")\")\\\n",
    "                   .Define(\"ClusterT2Top\"+top+\"Reco_\"+wp, \"RecoTaggedTopCluster\"+top+\"(ClusterT2Top\"+top+\", Top\"+top+\"_TopScore, \"+Top_threshold[top][wp]+\")\")\n",
    "            df = df.Define(\"nTopClusterT1\"+top+\"Reco_\"+wp, \"Sum(ClusterT1Top\"+top+\"Reco_\"+wp+\">0)\")\\\n",
    "                   .Define(\"nTopClusterT2\"+top+\"Reco_\"+wp, \"Sum(ClusterT2Top\"+top+\"Reco_\"+wp+\">0)\")\n",
    "    if (isMC):\n",
    "        for top in topTypes:\n",
    "            if top == \"Resolved\": idxFatJet = \"{}\"\n",
    "            else: idxFatJet = \"Top\"+top+\"_idxFatJet\"\n",
    "            for tr in TruthType[\"Top\"+top].keys():\n",
    "                if tr != \"standard\": df = df.Define(TruthType[\"Top\"+top+\"\"][tr], \"CalculateTop\"+tr+\"(Top\"+top+\"_idxJet0, Top\"+top+\"_idxJet1, Top\"+top+\"_idxJet2, \"+idxFatJet+\", Jet_matched, Jet_topMother, Jet_pdgId, FatJet_matched, FatJet_topMother, FatJet_pdgId)\")\n",
    "                \n",
    "                df = df.Define(\"ClusterT1Top\"+top+\"MCTag\"+tr, \"MCTaggedTopCluster(ClusterT1Top\"+top+\", \"+TruthType[\"Top\"+top+\"\"][tr]+\")\")\\\n",
    "                        .Define(\"ClusterT2Top\"+top+\"MCTag\"+tr, \"MCTaggedTopCluster(ClusterT2Top\"+top+\", \"+TruthType[\"Top\"+top+\"\"][tr]+\")\")\\\n",
    "                        .Define(\"nTopClusterT1\"+top+\"MCTag\"+tr, \"Sum(ClusterT1Top\"+top+\"MCTag\"+tr+\">0)\")\\\n",
    "                        .Define(\"nTopClusterT2\"+top+\"MCTag\"+tr, \"Sum(ClusterT2Top\"+top+\"MCTag\"+tr+\">0)\")\\\n",
    "                        .Define(\"TopClusterT1\"+top+\"MCTagTrueFirstCluster\"+tr, \"ClusterT1Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT1Top\"+top+\"MCTag\"+tr+\"[0]!=-1\")\\\n",
    "                        .Define(\"TopClusterT2\"+top+\"MCTagTrueFirstCluster\"+tr, \"ClusterT2Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT2Top\"+top+\"MCTag\"+tr+\"[0]!=-1\")\\\n",
    "                        .Define(\"TopClusterT1\"+top+\"MCTagFalseFirstCluster\"+tr, \"!ClusterT1Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT1Top\"+top+\"MCTag\"+tr+\"[0]!=-1\")\\\n",
    "                        .Define(\"TopClusterT2\"+top+\"MCTagFalseFirstCluster\"+tr, \"!ClusterT2Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT2Top\"+top+\"MCTag\"+tr+\"[0]!=-1\")\n",
    "                for wp in [\"WPloose\",\"WPmedium\",\"WPtight\"]:\n",
    "                    df = df.Define(\"nTopClusterT1\"+top+\"TrueReco\"+tr+\"_\"+wp, \"Sum(ClusterT1Top\"+top+\"MCTag\"+tr+\">0 && ClusterT1Top\"+top+\"Reco_\"+wp+\">0)\")\\\n",
    "                            .Define(\"nTopClusterT2\"+top+\"TrueReco\"+tr+\"_\"+wp, \"Sum(ClusterT2Top\"+top+\"MCTag\"+tr+\">0 && ClusterT2Top\"+top+\"Reco_\"+wp+\">0)\")\\\n",
    "                            .Define(\"TopClusterT1\"+top+\"TrueRecoFirstCluster\"+tr+\"_\"+wp, \"ClusterT1Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT1Top\"+top+\"Reco_\"+wp+\"[0] && ClusterT1Top\"+top+\"Reco_\"+wp+\"[0]!=-1\")\\\n",
    "                            .Define(\"TopClusterT2\"+top+\"TrueRecoFirstCluster\"+tr+\"_\"+wp, \"ClusterT2Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT2Top\"+top+\"Reco_\"+wp+\"[0] && ClusterT2Top\"+top+\"Reco_\"+wp+\"[0]!=-1\")\\\n",
    "                            .Define(\"TopClusterT1\"+top+\"FalseRecoFirstCluster\"+tr+\"_\"+wp, \"!ClusterT1Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT1Top\"+top+\"Reco_\"+wp+\"[0] && ClusterT1Top\"+top+\"Reco_\"+wp+\"[0] !=-1\")\\\n",
    "                            .Define(\"TopClusterT2\"+top+\"FalseRecoFirstCluster\"+tr+\"_\"+wp, \"!ClusterT2Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT2Top\"+top+\"Reco_\"+wp+\"[0] && ClusterT2Top\"+top+\"Reco_\"+wp+\"[0] !=-1\")\n",
    "        df = df.Define(\"TopMixed_Score_truthstandard\", \"TopMixed_TopScore[TopMixed_truth==1]\")\\\n",
    "               .Define(\"TopMixed_Score_truth1\", \"TopMixed_TopScore[TopMixed_truth1==1 && TopMixed_truth==0 && TopMixed_truth2==0]\")\\\n",
    "               .Define(\"TopMixed_Score_truth2\", \"TopMixed_TopScore[TopMixed_truth2==1 && TopMixed_truth==0]\")\\\n",
    "               .Define(\"TopMixed_Score_truth1_incl\", \"TopMixed_TopScore[TopMixed_truth1==1]\")\\\n",
    "               .Define(\"TopMixed_Score_truth2_incl\", \"TopMixed_TopScore[TopMixed_truth2==1]\")\\\n",
    "               .Define(\"TopResolved_Score_truthstandard\", \"TopResolved_TopScore[TopResolved_truth==1]\")\\\n",
    "               .Define(\"TopResolved_Score_truth1\", \"TopResolved_TopScore[TopResolved_truth1==1 && TopResolved_truth==0 && TopResolved_truth2==0]\")\\\n",
    "               .Define(\"TopResolved_Score_truth2\", \"TopResolved_TopScore[TopResolved_truth2==1 && TopResolved_truth==0]\")\\\n",
    "               .Define(\"TopResolved_Score_truth1_incl\", \"TopResolved_TopScore[TopResolved_truth1==1]\")\\\n",
    "               .Define(\"TopResolved_Score_truth2_incl\", \"TopResolved_TopScore[TopResolved_truth2==1]\")\n",
    "\n",
    "    df_topselected = df.Define(\"Top_idx_Resolved\",\"ClusterT1TopResolved.size()>0 ? ClusterT1TopResolved[0][0]: -1\")\\\n",
    "                       .Define(\"Top_idx_Mixed\",\"ClusterT1TopMixed.size() ? ClusterT1TopMixed[0][0] : -1\")\n",
    "    df_topselected = df_topselected.Define(\"TopMerged_bestTopScore\", \"FatJet_particleNet_TvsQCD[ArgMax(FatJet_particleNet_TvsQCD)]\")\\\n",
    "                                   .Define(\"TopMerged_bestTopPt\", \"FatJet_pt[ArgMax(FatJet_particleNet_TvsQCD)]\")\\\n",
    "                                   .Define(\"TopMerged_bestTopEta\", \"FatJet_eta[ArgMax(FatJet_particleNet_TvsQCD)]\")\\\n",
    "                                   .Define(\"TopMerged_bestTopPhi\", \"FatJet_phi[ArgMax(FatJet_particleNet_TvsQCD)]\")\\\n",
    "                                   .Define(\"TopMerged_bestTopMass\", \"FatJet_mass[ArgMax(FatJet_particleNet_TvsQCD)]\")\\\n",
    "                                   .Define(\"TopMerged_bestTopTruth\", \"FatJet_matched[ArgMax(FatJet_particleNet_TvsQCD)] == 3\")\\\n",
    "                                   .Define(\"TopMerged_truth_exists\", \"Any(FatJet_matched==3)\")\n",
    "    \n",
    "    df_topvariables = df_topselected.Define(\"Top_pt_Resolved\", \"Top_idx_Resolved !=-1 ? TopResolved_pt[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_eta_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_eta[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_phi_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_phi[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_mass_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_mass[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_score_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_TopScore[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_idxJet0_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_idxJet0[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_idxJet1_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_idxJet1[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_idxJet2_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_idxJet2[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_pt_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_pt[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_eta_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_eta[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_phi_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_phi[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_mass_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_mass[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_score_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_TopScore[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_idxJet0_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_idxJet0[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_idxJet1_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_idxJet1[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_idxJet2_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_idxJet2[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_idxFatJet_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_idxFatJet[Top_idx_Mixed]: -999\")\\\n",
    "    # SELECTION of the best Top among categories\n",
    "    \n",
    "    # return indices of the FatJet with particleNet score over the thresholds \n",
    "    for wp in [\"WPloose\",\"WPmedium\",\"WPtight\"]:\n",
    "        df_topvariables = df_topvariables.Define(\"GoodTopMer_idx_\"+wp, \"select_TopMer(FatJet_particleNet_TvsQCD, GoodFatJet_idx, \"+TopMerged_threshold[wp]+\")\")\n",
    "        # return indices of the TopMixed over the threshold with any object in common\n",
    "        df_topvariables = df_topvariables.Define(\"GoodTopMix_idx_\"+wp, \"select_TopMix(TopMixed_TopScore, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, GoodJet_idx, GoodFatJet_idx, BJetTopMu, \"+Top_threshold[\"Mixed\"][wp]+\")\")\n",
    "        # return indices of the TopResolved over the threshold with any object in common\n",
    "        df_topvariables = df_topvariables.Define(\"GoodTopRes_idx_\"+wp, \"select_TopRes(TopResolved_TopScore, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, GoodJet_idx, BJetTopMu, \"+Top_threshold[\"Resolved\"][wp]+\")\")\n",
    "    \n",
    "        df_topvariables = df_topvariables.Define(\"nGoodTopResolved_\"+wp, \"nTop(GoodTopRes_idx_\"+wp+\")\")\\\n",
    "                                .Define(\"nGoodTopMixed_\"+wp, \"nTop(GoodTopMix_idx_\"+wp+\")\")\\\n",
    "                                .Define(\"nGoodTopMerged_\"+wp, \"nTop(GoodTopMer_idx_\"+wp+\")\")\n",
    "    \n",
    "    \n",
    "        # return:  1- Event Resolved, 2- Event Mixed, 3- Event Merged, 4- Event Nothing, ...\n",
    "        df_topvariables = df_topvariables.Define(\"EventTopCategory_\"+wp, \"select_TopCategory(GoodTopMer_idx_\"+wp+\", GoodTopMix_idx_\"+wp+\", GoodTopRes_idx_\"+wp+\")\")\n",
    "        # if isMC:\n",
    "        #     df_topcategory = df_topcategory.Define(\"EventTopCategoryWithTruth\", \"select_TopCategoryWithTruth(EventTopCategory, FatJet_matched, GoodTopMer_idx, TopMixed_truth, GoodTopMix_idx, TopResolved_truth, GoodTopRes_idx)\")\n",
    "    \n",
    "        df_topvariables = df_topvariables.Define(\"Top_idx_\"+wp,\n",
    "                                               \"select_bestTop(EventTopCategory_\"+wp+\", FatJet_particleNet_TvsQCD, TopMixed_TopScore, TopResolved_TopScore)\")\n",
    "        # return best top idx wrt category --> the idx is referred to the list of candidates fixed by the EventTopCategory\n",
    "        df_topvariables = df_topvariables.Define(\"Top_pt_\"+wp, \"select_TopVar(EventTopCategory_\"+wp+\", Top_idx_\"+wp+\", FatJet_pt, TopMixed_pt, TopResolved_pt)\")\\\n",
    "                            .Define(\"Top_eta_\"+wp, \"select_TopVar(EventTopCategory_\"+wp+\", Top_idx_\"+wp+\", FatJet_eta, TopMixed_eta, TopResolved_eta)\")\\\n",
    "                            .Define(\"Top_phi_\"+wp, \"select_TopVar(EventTopCategory_\"+wp+\", Top_idx_\"+wp+\", FatJet_phi, TopMixed_phi, TopResolved_phi)\")\\\n",
    "                            .Define(\"Top_mass_\"+wp, \"select_TopVar(EventTopCategory_\"+wp+\", Top_idx_\"+wp+\", FatJet_mass, TopMixed_mass, TopResolved_mass)\")\\\n",
    "                            .Define(\"Top_score_\"+wp, \"select_TopVar(EventTopCategory_\"+wp+\", Top_idx_\"+wp+\", FatJet_particleNet_TvsQCD, TopMixed_TopScore, TopResolved_TopScore)\")\\\n",
    "\n",
    "\n",
    "    return df_topvariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69f0bf61-b2ab-422e-8baf-9abba2b984cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookhisto(df, regions_def, var, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var:\n",
    "            if v._MConly and not sampleflag: \n",
    "                continue\n",
    "            else:\n",
    "                if regions_def[reg] == \"\":\n",
    "                    if v._noUnOvFlowbin:\n",
    "                        h_[reg][v._name]= df.Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                    else:\n",
    "                        h_[reg][v._name]= df.Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                else:\n",
    "                    if v._noUnOvFlowbin:\n",
    "                        h_[reg][v._name]= df.Filter(regions_def[reg]).Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                    else:\n",
    "                        h_[reg][v._name]= df.Filter(regions_def[reg]).Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                    \n",
    "    return h_\n",
    "\n",
    "def bookhisto2D(df, regions_def, var2d, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var2d:\n",
    "            if regions_def[reg]==\"\":\n",
    "                h_[reg][v._name] = df.Redefine(v._xname, \"UnOvBin(\"+v._xname+\",\"+str(v._nxbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\")\\\n",
    "                                     .Redefine(v._yname, \"UnOvBin(\"+v._yname+\",\"+str(v._nybins)+\",\"+str(v._ymin)+\",\"+str(v._ymax)+\")\")\\\n",
    "                                     .Histo2D((v._xname+\"Vs\"+v._yname+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax), v._xname, v._yname)\n",
    "            else:\n",
    "                h_[reg][v._name] = df.Filter(regions_def[reg])\\\n",
    "                                     .Redefine(v._xname, \"UnOvBin(\"+v._xname+\",\"+str(v._nxbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\")\\\n",
    "                                     .Redefine(v._yname, \"UnOvBin(\"+v._yname+\",\"+str(v._nybins)+\",\"+str(v._ymin)+\",\"+str(v._ymax)+\")\")\\\n",
    "                                     .Histo2D((v._xname+\"Vs\"+v._yname+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax), v._xname, v._yname)\n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37967949-f502-4c77-9ceb-dd04f8062cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def savehisto(d, h, regions_def, var, s_cut):\n",
    "    histo = {reg: {v._name: ROOT.TH1D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax) for v in var} for reg in regions_def.keys()}\n",
    "    isMC=True\n",
    "    if \"Data\" in d.label: isMC = False\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    \n",
    "    for s in s_list:\n",
    "        outfile = ROOT.TFile.Open(repohisto+s.label+'.root', \"RECREATE\")\n",
    "        for reg in regions_def.keys():\n",
    "            for v in var:\n",
    "                if v._MConly and not isMC:\n",
    "                    continue\n",
    "                else:\n",
    "                    # print(reg, v)\n",
    "                    histo[reg][v._name] = h[d.label][s.label][reg][v._name].GetValue()      \n",
    "                    outfile.cd()\n",
    "                    histo[reg][v._name].Write()\n",
    "        outfile.Close()\n",
    "\n",
    "# i plot2d per il momento non ci servono, si deve trovare un modo più intelligente di farli\n",
    "def savehisto2d(d, h, regions_def, var2d, s_cut):\n",
    "    histo = {reg: {v._name: ROOT.TH2D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax,) for v in var2d} for reg in regions_def.keys()}\n",
    "        \n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    \n",
    "    for s in s_list:\n",
    "        outfile = ROOT.TFile.Open(repohisto+s.label+'_2D.root', \"RECREATE\")\n",
    "        for reg in regions_def.keys():\n",
    "            for v in histo[reg].keys():\n",
    "                histo[reg][v] = h[d.label][s.label][reg][v].GetValue()      \n",
    "                outfile.cd()\n",
    "                histo[reg][v].Write()\n",
    "        outfile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c96d28-45f4-4877-8d00-680e095a3016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#samples['DataHTF_2022']['DataHTF_2022']['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab036200-65f9-44a0-abbb-8a04003fc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples[d.label][d.components[0]]['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b72ddfd-d641-437d-a78c-8568a0a17a30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loop on datasets:  ['WJetsHT2500toInf_2018']\n",
      "Local time : 2024-06-21 12:26:21.779792\n",
      "Initializing DataFrame for WJetsHT2500toInf_2018 chain len =  1\n",
      "['root://cms-xrd-global.cern.ch//store/user/acagnott/DM_Run3_v0/WJetsToLNu_HT-2500ToInf_TuneCP5_13TeV-madgraphMLM-pythia8/WJetsHT2500toInf_2018/231222_110712/0000/tree_hadd_23.root\\n']\n",
      "All histos booked !\n",
      "WJetsHT2500toInf_2018 histos saved\n",
      "Sanpshot done!\n",
      "Job finished in:  0:11:50.367933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TClass::Init>: no dictionary for class edm::Hash<1> is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessHistory is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessConfiguration is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ParameterSetBlob is available\n",
      "Warning in <TClass::Init>: no dictionary for class pair<edm::Hash<1>,edm::ParameterSetBlob> is available\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "print(\"starting loop on datasets: \",[d.label for d in datasets])\n",
    "print(\"Local time :\", t0)\n",
    "# print(\"requirements: \"+cut)\n",
    "\n",
    "h = {}\n",
    "h_2D = {}\n",
    "\n",
    "workers = {}\n",
    "cpu_percentages = {}\n",
    "\n",
    "\n",
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "\n",
    "    if 'Data' in d.label : sampleflag = 0\n",
    "    else: sampleflag = 1\n",
    "    c_ = cut\n",
    "    h[d.label] = {}\n",
    "    h_2D[d.label] = {}\n",
    "    for s in s_list:\n",
    "        \n",
    "#         if distributed:\n",
    "#             worker_cpu_usage = get_worker_cpu_usage(client)\n",
    "#             # print(\"Utilizzo della CPU di ogni worker:\", worker_cpu_usage)\n",
    "\n",
    "#             workers[\"start\"] = list(worker_cpu_usage.keys())\n",
    "#             cpu_percentages[\"start\"] = list(worker_cpu_usage.values())\n",
    "        \n",
    "        #-------------------------------------------------------------------------\n",
    "        ############# Fixing variables for 2018-2022 #############################\n",
    "        #-------------------------------------------------------------------------\n",
    "        if s.year == 2018:\n",
    "            bTagAlg = \"Jet_btagDeepB\"\n",
    "        elif s.year == 2022:\n",
    "            bTagAlg = \"Jet_btagPNetB\"\n",
    "        if hasattr(s,\"EE\"):\n",
    "            EE = s.EE\n",
    "        else:\n",
    "            EE = 0\n",
    "        #-------------------------------------------------------------------------\n",
    "        #########################  DF initialization #############################\n",
    "        #-------------------------------------------------------------------------\n",
    "        \n",
    "        print(\"Initializing DataFrame for \"+ s.label +\" chain len = \", len(chain[d.label][s.label]))\n",
    "        if len(chain[d.label][s.label])==1: print(chain[d.label][s.label])\n",
    "        if distributed ==True:\n",
    "            df = RDataFrame(\"Events\", chain[d.label][s.label], npartitions=nmaxpartition, \n",
    "                            daskclient=client, monitor_label = \"main\" )\n",
    "        else:\n",
    "            df = RDataFrame(\"Events\", chain[d.label][s.label])\n",
    "        \n",
    "        # if 'TT' in s.label and not 'Mtt' in s.label:\n",
    "        #     df = df.Filter(\"tt_mtt_doublecounting(GenPart_pdgId, GenPart_pt, GenPart_eta, GenPart_phi, GenPart_mass)\", \"TT double counting\")\n",
    "        if sampleflag:\n",
    "            df = df.Define(\"GenPartTop_pt\", \"genpartTopPt(GenPart_pt, GenPart_pdgId, GenPart_genPartIdxMother, GenPart_genPartIdxMother_prompt)\")\n",
    "            branches.update([\"GenPartTop_pt\"])\n",
    "        \n",
    "        df_ismc         = df.Define(\"isMC\", \"isMC(\"+str(sampleflag)+\")\")\n",
    "        df_year         = df_ismc.Define(\"year\", str(s.year))\n",
    "        df_hemveto      = df_year.Define(\"HEMVeto\", \"hemveto(Jet_eta, Jet_phi, Electron_eta, Electron_phi)\")\n",
    "        df_hemveto      = df_hemveto.Filter(\"(isMC || (year != 2018) || (HEMVeto || run<319077.))\")\n",
    "        df_hlt          = trigger_filter(df_hemveto)\n",
    "        df_wnom         = df_hlt.Define('w_nominal', '1').Redefine('w_nominal', 'w_nominalhemveto(w_nominal, HEMVeto, year)')\n",
    "        df_presel       = goodObjects(df_wnom, bTagAlg, s.year, EE)\n",
    "        df_LepTop       = LeptonTopTag(df_presel, sampleflag)\n",
    "        df_TopSel       = select_top(df_LepTop, sampleflag)\n",
    "        \n",
    "        df_snap = df_TopSel\n",
    "        df_plot = df_TopSel\n",
    "        \n",
    "        if do_snapshot:\n",
    "            opts = ROOT.RDF.RSnapshotOptions()\n",
    "            opts.fLazy = True\n",
    "            if distributed: fold = \"./\"\n",
    "            else: fold = folder\n",
    "            snapshot_df = df_snap.Snapshot(\"events_nominal\", fold+\"snapTagger_\"+s.label+\".root\", branches, opts)\n",
    "            # print(\"./\"+s.label+\".root\")\n",
    "        if do_histos:\n",
    "            s_cut = cut_string(cut)\n",
    "            if len(var) != 0 :\n",
    "                h[d.label][s.label] = bookhisto(df_plot, regions_def, var, s_cut)\n",
    "            if len(var2d) != 0 :\n",
    "                h_2D[d.label][s.label] = bookhisto2D(df_plot, regions_def, var2d, s_cut)\n",
    "# if not distributed:\n",
    "#     df_presel.Report().Print()\n",
    "# if distributed:\n",
    "#     worker_cpu_usage = get_worker_cpu_usage(client)\n",
    "#     # print(\"Utilizzo della CPU di ogni worker:\", worker_cpu_usage)\n",
    "\n",
    "#     workers[\"after_book_hist\"] = list(worker_cpu_usage.keys())\n",
    "#     cpu_percentages[\"after_book_hist\"] = list(worker_cpu_usage.values())\n",
    "if do_histos:\n",
    "    print(\"All histos booked !\")\n",
    "    for d in datasets:\n",
    "        if len(var):\n",
    "            savehisto(d, h, regions_def, var, s_cut)\n",
    "        if len(var2d) != 0 :\n",
    "            savehisto2d(d, h_2D, regions_def, var2d, s_cut)\n",
    "        print(d.label + \" histos saved\")\n",
    "# if distributed:\n",
    "#     worker_cpu_usage = get_worker_cpu_usage(client)\n",
    "#     # print(\"Utilizzo della CPU di ogni worker:\", worker_cpu_usage)\n",
    "\n",
    "#     workers[\"end\"] = list(worker_cpu_usage.keys())\n",
    "#     cpu_percentages[\"end\"] = list(worker_cpu_usage.values())\n",
    "    \n",
    "if do_snapshot:\n",
    "    snapshot_df.GetValue()\n",
    "    if distributed: \n",
    "        client.run(transfer_to_tier)\n",
    "        print(\"Snapshots saved and trasfered to tier\")\n",
    "    print(\"Sanpshot done!\")\n",
    "t1 = datetime.now()\n",
    "print(\"Job finished in: \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9573b747-9a5a-4501-b084-481b10d418ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ./results/TopTagger_JetMET_selectioncomparison/plots/WJetsHT2500toInf_2018.root Title: \n",
      "All 136.0\n",
      "Presel 120.0\n",
      "PreselWpt 76.0\n",
      "1TopLep 58.0\n",
      "1TopLepWpt 40.0\n",
      "1TopLep_1TopHadrRes_WPloose 0.0\n",
      "1TopLep_1TopHadrMix_WPloose 24.0\n",
      "1TopLep_1TopHadrMer_WPloose 5.0\n",
      "1TopLep_1TopHadrAll_WPloose 29.0\n",
      "1TopLep_1TopHadrResWpt_WPloose 0.0\n",
      "1TopLep_1TopHadrMixWpt_WPloose 15.0\n",
      "1TopLep_1TopHadrMerWpt_WPloose 4.0\n",
      "1TopLep_1TopHadrAllWpt_WPloose 19.0\n",
      "1TopLep_1TopHadrRes_WPmedium 0.0\n",
      "1TopLep_1TopHadrMix_WPmedium 2.0\n",
      "1TopLep_1TopHadrMer_WPmedium 4.0\n",
      "1TopLep_1TopHadrAll_WPmedium 6.0\n",
      "1TopLep_1TopHadrResWpt_WPmedium 0.0\n",
      "1TopLep_1TopHadrMixWpt_WPmedium 1.0\n",
      "1TopLep_1TopHadrMerWpt_WPmedium 3.0\n",
      "1TopLep_1TopHadrAllWpt_WPmedium 4.0\n",
      "1TopLep_1TopHadrRes_WPtight 0.0\n",
      "1TopLep_1TopHadrMix_WPtight 0.0\n",
      "1TopLep_1TopHadrMer_WPtight 3.0\n",
      "1TopLep_1TopHadrAll_WPtight 3.0\n",
      "1TopLep_1TopHadrResWpt_WPtight 0.0\n",
      "1TopLep_1TopHadrMixWpt_WPtight 0.0\n",
      "1TopLep_1TopHadrMerWpt_WPtight 2.0\n",
      "1TopLep_1TopHadrAllWpt_WPtight 2.0\n"
     ]
    }
   ],
   "source": [
    "file = ROOT.TFile.Open(repohisto+s.label+\".root\")\n",
    "print(file)\n",
    "# for a in file.GetListOfKeys(): print(a)\n",
    "for reg in regions_def.keys():\n",
    "    # for v in var:\n",
    "    hist = file.Get(var[0]._name+\"_\"+reg+\"_\")\n",
    "    print(reg, hist.Integral())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3606a5b-e3ce-491d-a86a-6c02431ca435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions_def.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "269963fc-638b-4a19-8c8d-af26bad6e37f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# region = \"NoCut\"\n",
    "# var = \"MET_pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8daa57df-4a5e-46d7-b758-84edcebca76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if do_snapshot and distributed:\n",
    "    print(os.popen(\"davix-ls davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{}/ -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name, remote_subfolder_name)).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b07d0-f9c7-4a64-942b-33d356ef4e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cc7ee50-14e3-409c-a954-0575ca5500f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = []\n",
    "hCPU = []\n",
    "for phase in workers.keys():\n",
    "    hCPU.append(ROOT.TH1F(\"hCPU\"+phase,\"workers CPU usage \"+phase+\";workers;CPU usage (%)\", len(workers[phase]), 0.5, len(workers[phase])+0.5))\n",
    "    for i in range(len(workers[phase])):\n",
    "        hCPU[-1].SetBinContent(i+1, cpu_percentages[phase][i])\n",
    "    c1.append(ROOT.TCanvas())\n",
    "    c1[-1].Draw()\n",
    "    hCPU[-1].Draw(\"HIST\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity kernel",
   "language": "python",
   "name": "singularity-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
