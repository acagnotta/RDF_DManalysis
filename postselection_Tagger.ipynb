{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8020d75-5631-4723-b13b-9aec28a8d977",
   "metadata": {},
   "source": [
    "**Top Tagger Framework**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd081e3",
   "metadata": {},
   "source": [
    "**Framework version January 2024**\n",
    "- Log :\n",
    "    - understood bug in data (the problem was in CRAB not here)\n",
    "    - added Snapshot \n",
    "- Planned update :\n",
    "    - standalone code, prepare a couple of input parameters and make the code working with a single command\n",
    "    \n",
    "    \n",
    "__________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98655626-5252-41be-b111-76e638cecddf",
   "metadata": {},
   "source": [
    "**Code**\n",
    "\n",
    "Folder definition on Tier:\n",
    "- in the main folder */acagnott/* added folder 'remote_folder_name';\n",
    "- in \"Snapshots\" added the folder 'remote_subfolder_name';\n",
    "- in the subfolder through dask the snapshot will be copied with name \"snap_\"+label+\"_*.root\"\n",
    "\n",
    "\n",
    "Es: se lancio \"DataMETA_2018\", gli snapshot vengono salvati in ../acagnott/Snapshot/20231229/snap_DataMET_2018_*.root\n",
    "se viene lanciato \"QCD_2018\" viene creata la cartella /acagnott/Snapshot/20231229/snap_QCDHT_100to200_2018_*.root e così via per ogni components\n",
    "\n",
    "---> Viene usato solo il giorno in modo che tutti i sample lanciati lo stesso giorno verranno salvati nella stessa cartella con nomi diversi, visto che vengono lanciati in momenti diversi della giornata lo stesso tipo di job. Forse va modificato il formato se i singoli job iniziano a durare più di un giorno dato che viene comunque lanciato un sampel per volta (in tal caso verrebbero salvati in cartelle diverse. Si potrebbe pensare di mettere la data a mano, cioé invece di usare datetime.now() si potrebbe inserire la data manualmente per fare in modo di mettere tutti i file nella stessa folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf1ce47-e2f0-48f4-8e4b-b0fa9f1bacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "sched_port = 22358 #Dask port\n",
    "nmaxpartition = 100\n",
    "distributed = False#True#\n",
    "do_histos = True\n",
    "hist_folder = \"TopTagger\"\n",
    "do_snapshot = True\n",
    "remote_subfolder_name = datetime.now().strftime(\"%Y%m%d\") #20231229\n",
    "\n",
    "in_dataset = [\n",
    "    # \"DataMETA_2018\", \n",
    "    # \"DataMETB_2018\", \n",
    "    # \"DataMETC_2018\",\n",
    "    # \"DataMETD_2018\",\n",
    "    # \"DataSingleMuA_2018\", \n",
    "    # \"DataSingleMuB_2018\", \n",
    "    # \"DataSingleMuC_2018\",\n",
    "    # \"DataSingleMuD_2018\",\n",
    "    \n",
    "    # \"TprimeToTZ_700_2018\",\n",
    "    # \"TprimeToTZ_1000_2018\", \n",
    "    # \"TprimeToTZ_1800_2018\",\n",
    "    \n",
    "    # \"QCD_2018\",\n",
    "    # \"TT_2018\",\n",
    "    # \"ZJetsToNuNu_2018\",\n",
    "    # \"WJets_2018\",\n",
    "    \n",
    "    # \"TT_semilep_2018\",\n",
    "    # \"TT_hadr_2018\",\n",
    "    # \"TT_Mtt700to1000_2018\",\n",
    "    # \"TT_Mtt1000toInf_2018\",\n",
    "    # \"QCDHT_100to200_2018\", \"QCDHT_200to300_2018\",\"QCDHT_300to500_2018\", \"QCDHT_500to700_2018\", \"QCDHT_700to1000_2018\",\"QCDHT_1000to1500_2018\", \"QCDHT_1500to2000_2018\",\"QCDHT_2000toInf_2018\",\"TT_hadr_2018\", \"TT_semilep_2018\", \"TT_Mtt700to1000_2018\", \"TT_Mtt1000toInf_2018\",\"ZJetsToNuNu_HT100to200_2018\", \"ZJetsToNuNu_HT200to400_2018\", \"ZJetsToNuNu_HT400to600_2018\", \"ZJetsToNuNu_HT600to800_2018\", \"ZJetsToNuNu_HT800to1200_2018\", \"ZJetsToNuNu_HT1200to2500_2018\", \"ZJetsToNuNu_HT2500toInf_2018\", \"WJetsHT100to200_2018\", \"WJetsHT200to400_2018\", \"WJetsHT400to600_2018\", \"WJetsHT600to800_2018\", \"WJetsHT800to1200_2018\", \"WJetsHT1200to2500_2018\", \"WJetsHT2500toInf_2018\"\n",
    "    # \"QCDHT_100to200_2018\", \n",
    "    # \"QCDHT_200to300_2018\",\n",
    "    # \"QCDHT_300to500_2018\", \n",
    "    # \"QCDHT_500to700_2018\", \n",
    "    # \"QCDHT_700to1000_2018\", \n",
    "    # \"QCDHT_1000to1500_2018\", \n",
    "    # \"QCDHT_1500to2000_2018\",\n",
    "    # \"QCDHT_2000toInf_2018\"\n",
    "    # \"WJetsHT100to200_2018\", \n",
    "    # \"WJetsHT200to400_2018\", \n",
    "    # \"WJetsHT400to600_2018\", \n",
    "    # \"WJetsHT600to800_2018\", \n",
    "    # \"WJetsHT800to1200_2018\",\n",
    "    # \"WJetsHT1200to2500_2018\", \n",
    "    \"WJetsHT2500toInf_2018\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e16aaf3-78d5-4a3c-851c-21f771dcf586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.27/01\n",
      "/tmp/x509up_u0 /cvmfs/grid.cern.ch/etc/grid-security/certificates/\n",
      "You are producing histograms\n",
      "You are producing snapshot\n",
      "You are saving snapshots in local\n",
      "folder name : ./results/TopTagger/\n",
      "local folder histos: ./results/TopTagger/\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import os\n",
    "from utils.samples import *\n",
    "from utils.variables_tagger import *\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from dask.distributed import Client\n",
    "import psutil\n",
    "from psutil import cpu_percent\n",
    "ROOT.RDF.Experimental.Distributed.open_files_locally = False\n",
    "\n",
    "os.environ['X509_CERT_DIR'] = \"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "os.environ['X509_USER_PROXY'] = \"/tmp/x509up_u0\"\n",
    "print(os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\"))\n",
    "\n",
    "\n",
    "if distributed:\n",
    "    nfiles_max = 10000\n",
    "else:\n",
    "    nfiles_max = 1  #######\n",
    "\n",
    "# Cosa aggiungere, modificare la cartella sul tier in questo modo ../acagnott/Snapshot_rdf/*dataset_name*/*data di processamente con orario*/\n",
    "# insomma come fa crab\n",
    "\n",
    "\n",
    "if do_histos: print(\"You are producing histograms\")\n",
    "if do_snapshot: print(\"You are producing snapshot\")\n",
    "\n",
    "remote_folder_name = \"Snapshots\"\n",
    "#output histos folder\n",
    "folder = \"./results/\"+hist_folder+\"/\"\n",
    "# eos_folder = \"/eos/home-a/acagnott/DarkMatter/nosynch/\"+hist_folder\n",
    "\n",
    "if do_snapshot and remote_subfolder_name == datetime.now().strftime(\"%Y%m%d\") and distributed: \n",
    "    print(\"You are naming the tier subfolder using the current day \\n\")\n",
    "    print(\"Snapshots folder name : ~/acagnott/{}/{}\".format(remote_folder_name, remote_subfolder_name))\n",
    "elif do_snapshot and distributed:\n",
    "    print(\"You are naming the tier subfolder manually\")\n",
    "    print(\"Snapshots folder name : ~/acagnott/{}/{}\".format(remote_folder_name, remote_subfolder_name))\n",
    "elif do_snapshot:\n",
    "    print(\"You are saving snapshots in local\")\n",
    "    print(\"folder name : \" + folder)\n",
    "\n",
    "\n",
    "if do_histos : \n",
    "    print(\"local folder histos: {}\".format(folder))\n",
    "    # print(\"At the end of the job the histos will be transfer to eos storage\")\n",
    "    # print(\"eos folder : {}\".format(eos_folder))\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "repohisto = folder+\"plots/\"\n",
    "if not os.path.exists(repohisto):\n",
    "    os.mkdir(repohisto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe0ba0b-23a0-4b47-96f6-4dc0917ec6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating folders on Tier\n",
    "if do_snapshot and distributed:\n",
    "    tier_main_folder = \"davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/\"\n",
    "    os.popen(\"davix-mkdir davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{} -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name))\n",
    "    os.popen(\"davix-mkdir davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{} -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name, remote_subfolder_name))\n",
    "    \n",
    "# transfer function for dask worker\n",
    "def transfer_to_tier(dask_worker):\n",
    "    import os\n",
    "    os.popen('for filename in snapTagger_*.root; do davix-put $filename davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{}/$filename -E ./proxy --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/; done'.format(remote_folder_name, remote_subfolder_name))\n",
    "    return True #, os.popen(\"for filename in ./test_*.root; do echo $filename; done\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac5975c9-41cf-483d-9c51-a36a0ed97f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worker_cpu_usage(client):\n",
    "    workers = list(client.scheduler_info()[\"workers\"].keys())\n",
    "    cpu_usage = {}\n",
    "\n",
    "    for worker in workers:\n",
    "        # Ottieni l'utilizzo della CPU per ogni worker\n",
    "        cpu_percent = client.run(psutil.cpu_percent)\n",
    "\n",
    "        # Calcola la media dell'utilizzo della CPU per ogni worker\n",
    "        avg_cpu_percent = sum(cpu_percent.values()) / len(cpu_percent)\n",
    "        \n",
    "        # Memorizza i risultati nell'oggetto cpu_usage\n",
    "        cpu_usage[worker] = avg_cpu_percent\n",
    "\n",
    "    return cpu_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3271c9-56c5-4ee1-a501-f85177699ab8",
   "metadata": {},
   "source": [
    "- Import of utils from variables.py\n",
    "Cut (if any), Regions, Variables\n",
    "\n",
    "- syncro between in_dataset and sample_dict (from sample.py) to syncronize labels and ather featurs of the dataset (as sigma if needed)\n",
    "- import of samples_dict.json to load files list (path to reach them on tier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f446ef-90d9-46e8-ad16-8ded4fa2fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions to book: \n",
      "  All\n",
      "  Presel\n",
      "  1TopLep\n",
      "  0TopLep\n",
      "  atLeast2TopLep\n",
      "  1TopLepMCtag\n",
      "Variables for histograms :\n",
      "['MET_pt', 'MET_phi', 'PuppiMET_pt', 'nGoodJet', 'nJetBtag', 'Top_mass_Resolved', 'Top_pt_Resolved', 'Top_score_Resolved', 'Top_mass_Mixed', 'Top_pt_Mixed', 'Top_score_Mixed', 'nClusterT1TopMixed', 'nClusterT2TopMixed', 'nClusterT1TopResolved', 'nClusterT2TopResolved', 'nTopClusterT1MixedReco', 'nTopClusterT2MixedReco', 'nTopClusterT1ResolvedReco', 'nTopClusterT2ResolvedReco', 'TopMixed_TopScore', 'TopMixed_Score_truthstandard', 'TopMixed_Score_truth1_incl', 'TopMixed_Score_truth2_incl', 'TopMixed_Score_truth1', 'TopMixed_Score_truth2', 'TopResolved_TopScore', 'TopResolved_Score_truthstandard', 'TopResolved_Score_truth1', 'TopResolved_Score_truth2', 'TopResolved_Score_truth1_incl', 'TopResolved_Score_truth2_incl', 'nTopClusterT1ResolvedMCTagstandard', 'nTopClusterT2ResolvedMCTagstandard', 'nTopClusterT2ResolvedTrueRecostandard', 'nTopClusterT1ResolvedTrueRecostandard', 'TopClusterT1ResolvedTrueRecoFirstClusterstandard', 'TopClusterT2ResolvedTrueRecoFirstClusterstandard', 'TopClusterT1ResolvedFalseRecoFirstClusterstandard', 'TopClusterT2ResolvedFalseRecoFirstClusterstandard', 'TopClusterT1ResolvedMCTagTrueFirstClusterstandard', 'TopClusterT2ResolvedMCTagTrueFirstClusterstandard', 'TopClusterT1ResolvedMCTagFalseFirstClusterstandard', 'TopClusterT2ResolvedMCTagFalseFirstClusterstandard', 'nTopClusterT1ResolvedMCTagtruth1', 'nTopClusterT2ResolvedMCTagtruth1', 'nTopClusterT2ResolvedTrueRecotruth1', 'nTopClusterT1ResolvedTrueRecotruth1', 'TopClusterT1ResolvedTrueRecoFirstClustertruth1', 'TopClusterT2ResolvedTrueRecoFirstClustertruth1', 'TopClusterT1ResolvedFalseRecoFirstClustertruth1', 'TopClusterT2ResolvedFalseRecoFirstClustertruth1', 'TopClusterT1ResolvedMCTagTrueFirstClustertruth1', 'TopClusterT2ResolvedMCTagTrueFirstClustertruth1', 'TopClusterT1ResolvedMCTagFalseFirstClustertruth1', 'TopClusterT2ResolvedMCTagFalseFirstClustertruth1', 'nTopClusterT1ResolvedMCTagtruth2', 'nTopClusterT2ResolvedMCTagtruth2', 'nTopClusterT2ResolvedTrueRecotruth2', 'nTopClusterT1ResolvedTrueRecotruth2', 'TopClusterT1ResolvedTrueRecoFirstClustertruth2', 'TopClusterT2ResolvedTrueRecoFirstClustertruth2', 'TopClusterT1ResolvedFalseRecoFirstClustertruth2', 'TopClusterT2ResolvedFalseRecoFirstClustertruth2', 'TopClusterT1ResolvedMCTagTrueFirstClustertruth2', 'TopClusterT2ResolvedMCTagTrueFirstClustertruth2', 'TopClusterT1ResolvedMCTagFalseFirstClustertruth2', 'TopClusterT2ResolvedMCTagFalseFirstClustertruth2', 'nTopClusterT1MixedMCTagstandard', 'nTopClusterT2MixedMCTagstandard', 'nTopClusterT2MixedTrueRecostandard', 'nTopClusterT1MixedTrueRecostandard', 'TopClusterT1MixedTrueRecoFirstClusterstandard', 'TopClusterT2MixedTrueRecoFirstClusterstandard', 'TopClusterT1MixedFalseRecoFirstClusterstandard', 'TopClusterT2MixedFalseRecoFirstClusterstandard', 'TopClusterT1MixedMCTagTrueFirstClusterstandard', 'TopClusterT2MixedMCTagTrueFirstClusterstandard', 'TopClusterT1MixedMCTagFalseFirstClusterstandard', 'TopClusterT2MixedMCTagFalseFirstClusterstandard', 'nTopClusterT1MixedMCTagtruth1', 'nTopClusterT2MixedMCTagtruth1', 'nTopClusterT2MixedTrueRecotruth1', 'nTopClusterT1MixedTrueRecotruth1', 'TopClusterT1MixedTrueRecoFirstClustertruth1', 'TopClusterT2MixedTrueRecoFirstClustertruth1', 'TopClusterT1MixedFalseRecoFirstClustertruth1', 'TopClusterT2MixedFalseRecoFirstClustertruth1', 'TopClusterT1MixedMCTagTrueFirstClustertruth1', 'TopClusterT2MixedMCTagTrueFirstClustertruth1', 'TopClusterT1MixedMCTagFalseFirstClustertruth1', 'TopClusterT2MixedMCTagFalseFirstClustertruth1', 'nTopClusterT1MixedMCTagtruth2', 'nTopClusterT2MixedMCTagtruth2', 'nTopClusterT2MixedTrueRecotruth2', 'nTopClusterT1MixedTrueRecotruth2', 'TopClusterT1MixedTrueRecoFirstClustertruth2', 'TopClusterT2MixedTrueRecoFirstClustertruth2', 'TopClusterT1MixedFalseRecoFirstClustertruth2', 'TopClusterT2MixedFalseRecoFirstClustertruth2', 'TopClusterT1MixedMCTagTrueFirstClustertruth2', 'TopClusterT2MixedMCTagTrueFirstClustertruth2', 'TopClusterT1MixedMCTagFalseFirstClustertruth2', 'TopClusterT2MixedMCTagFalseFirstClustertruth2']\n",
      "Variables for histograms 2D :\n",
      "[]\n",
      "{'TopResolved': {'standard': 'TopResolved_truth', 'truth1': 'TopResolved_truth1', 'truth2': 'TopResolved_truth2'}, 'TopMixed': {'standard': 'TopMixed_truth', 'truth1': 'TopMixed_truth1', 'truth2': 'TopMixed_truth2'}}\n",
      "Datasets to process :  ['WJetsHT2500toInf_2018']\n",
      "Dataset : WJetsHT2500toInf_2018\n",
      "# of files to process :  1\n",
      "files strings :\n",
      "  root://cms-xrd-global.cern.ch//store/user/acagnott/DM_Run3_v0/WJetsToLNu_HT-2500ToInf_TuneCP5_13TeV-madgraphMLM-pythia8/WJetsHT2500toInf_2018/231222_110712/0000/tree_hadd_23.root\n",
      "\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  3126.0\n"
     ]
    }
   ],
   "source": [
    "# Common cuts over the different regions\n",
    "cut = requirements # ---> see variables.py\n",
    "\n",
    "# Regions definition\n",
    "regions_def = regions # ---> see variables.py\n",
    "print(\"Regions to book: \")\n",
    "for r in regions_def.keys():\n",
    "    print(\"  \"+r)\n",
    "    \n",
    "# Samples dictionary --> to load the files of each samples    \n",
    "sample_file = open(\"utils/dict_samples.json\", \"rb\")\n",
    "samples = json.load(sample_file)\n",
    "sample_file.close()\n",
    "\n",
    "# Variables definition \n",
    "var = vars  # ---> variables.py\n",
    "var2d = vars2D \n",
    "\n",
    "print(\"Variables for histograms :\")\n",
    "print([v._name for v in var])\n",
    "print(\"Variables for histograms 2D :\")\n",
    "print([v._name for v in var2d])\n",
    "\n",
    "# Alternative definition of truth\n",
    "\n",
    "print(TruthType)\n",
    "\n",
    "# Snapshots branches\n",
    "\n",
    "branches = {\"isMC\",\"year\",\"HEMVeto\",\"HT_eventHT\", \"MET_pt\", \"MET_phi\", \"w_nominal\", \n",
    "            \"Top_score_Resolved\", \"Top_pt_Resolved\", \"Top_eta_Resolved\", \"Top_phi_Resolved\", \"Top_mass_Resolved\", \"Top_idx_Resolved\",\n",
    "            \"Top_score_Mixed\", \"Top_pt_Mixed\", \"Top_eta_Mixed\", \"Top_phi_Mixed\", \"Top_mass_Mixed\", \"Top_idx_Mixed\",\n",
    "            \"Top_idx_Resolved\", \"Top_idx_Mixed\", \"NGoodTopLep\", \"BJetTopLep\", \"BJetTopLep_MCtag\", \"TopMerged_bestTopTruth\", \"TopMerged_bestTopScore\",\n",
    "            \"TopMerged_bestTopPt\", \"TopMerged_bestTopEta\", \"TopMerged_bestTopPhi\", \"TopMerged_bestTopMass\", \"TopMerged_truth_exists\"}\n",
    "            \n",
    "for top in topTypes:\n",
    "    branches.update([\n",
    "        \"nClusterT2Top\"+top+\"\",\n",
    "        \"nClusterT1Top\"+top+\"\",\n",
    "        \"nTopClusterT2\"+top+\"Reco\",\n",
    "        \"nTopClusterT1\"+top+\"Reco\"])\n",
    "for t in TruthType.keys():\n",
    "        top = t.replace(\"Top\",\"\")\n",
    "        for tr in TruthType[t].keys():\n",
    "            branches.update([\"nTopClusterT2\"+top+\"MCTag\"+tr,\n",
    "                             \"nTopClusterT1\"+top+\"MCTag\"+tr,\n",
    "                             \"nTopClusterT2\"+top+\"TrueReco\"+tr,\n",
    "                             \"nTopClusterT1\"+top+\"TrueReco\"+tr,\n",
    "                             \"TopClusterT2\"+top+\"TrueRecoFirstCluster\"+tr,\n",
    "                             \"TopClusterT1\"+top+\"TrueRecoFirstCluster\"+tr,\n",
    "                             \"TopClusterT2\"+top+\"FalseRecoFirstCluster\"+tr,\n",
    "                             \"TopClusterT1\"+top+\"FalseRecoFirstCluster\"+tr,\n",
    "                             \"TopClusterT2\"+top+\"MCTagTrueFirstCluster\"+tr,\n",
    "                             \"TopClusterT1\"+top+\"MCTagTrueFirstCluster\"+tr,\n",
    "                             \"TopClusterT2\"+top+\"MCTagFalseFirstCluster\"+tr,\n",
    "                             \"TopClusterT1\"+top+\"MCTagFalseFirstCluster\"+tr\n",
    "                             ])\n",
    "\n",
    "# Loading datasets choose at the beginning of the notebook\n",
    "datasets = []\n",
    "for in_d in in_dataset:\n",
    "    if not in_d in sample_dict.keys():\n",
    "        print(\"Check the in_dataset string... \", sample_dict.keys())\n",
    "    else : \n",
    "        datasets.append(sample_dict[in_d])\n",
    "print(\"Datasets to process : \", [d.label for d in datasets])\n",
    "\n",
    "# Loading the files strings to pass to the DataFrame\n",
    "chain = {}\n",
    "ntot_events = {}\n",
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        samples_list = d.components\n",
    "    else:\n",
    "        samples_list = [d]\n",
    "    chain[d.label] = {}\n",
    "    ntot_events[d.label] = {}\n",
    "    for s in samples_list:\n",
    "        if distributed: \n",
    "            nfiles = len(samples[d.label][s.label]['strings'])\n",
    "            \n",
    "            # for i, string in enumerate(samples[d.label][s.label]['strings']): \n",
    "            #     samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"root://stormgf2.pi.infn.it/\")\n",
    "            \n",
    "            # chain[d.label][s.label] = samples[d.label][s.label]['strings']\n",
    "            if nfiles_max>len(samples[d.label][s.label]['strings']): \n",
    "                chain[d.label][s.label] = samples[d.label][s.label]['strings']\n",
    "            else: \n",
    "                chain[d.label][s.label] = samples[d.label][s.label]['strings'][:nfiles_max]\n",
    "        else: \n",
    "            nfiles = nfiles_max\n",
    "            chain[d.label][s.label] = samples[d.label][s.label]['strings'][:nfiles]\n",
    "        if not \"Data\" in s.label: ntot_events[d.label][s.label] = np.sum(samples[d.label][s.label]['ntot'][:nfiles])\n",
    "        else: ntot_events[d.label][s.label] = None\n",
    "        print(\"Dataset : \"+s.label)\n",
    "        print(\"# of files to process : \", nfiles)\n",
    "        if distributed and nfiles_max>2:\n",
    "            print(\"files strings :\\n  {}\\n  {}\\n  ... \\n  {}\\n  {}\".format(chain[d.label][s.label][0], chain[d.label][s.label][1], chain[d.label][s.label][-2], chain[d.label][s.label][-1]))\n",
    "        else :\n",
    "            print(\"files strings :\\n  {}\".format(chain[d.label][s.label][0]))\n",
    "        print(\"# of total events in the files to process (MC only, if Data the number is None) : \", ntot_events[d.label][s.label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54586805-5e2c-4b36-a13b-11497d5f2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invLepveto = \"\"#\"!\" # per non invertirlo basta passare stinga vuota\n",
    "# met_cut = 250\n",
    "# mdphi_cut = 0\n",
    "# HLT_filter = \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight || HLT_Ele32_WPTight_Gsf || HLT_Ele115_CaloIdVT_GsfTrkIdT || HLT_Photon200 || HLT_IsoMu24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a9be5b-2e52-431b-9709-ee1c3e639b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "end of initialization\n"
     ]
    }
   ],
   "source": [
    "# initialization of clusters\n",
    "\n",
    "# upload the proxyfile to the Dask workers to make them able to access data on the grid \n",
    "\n",
    "from distributed.diagnostics.plugin import UploadFile\n",
    "def set_proxy(dask_worker):\n",
    "    import os\n",
    "    import shutil\n",
    "    working_dir = dask_worker.local_directory\n",
    "    proxy_name = 'x509up_u0'\n",
    "    os.environ['X509_USER_PROXY'] = working_dir + '/' + proxy_name\n",
    "    os.environ['X509_CERT_DIR']=\"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "    shutil.copyfile(working_dir + '/' + proxy_name, working_dir + '/../../../proxy')    \n",
    "    os.environ['EXTRA_CLING_ARGS'] = \"-O2\"\n",
    "    return os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\")\n",
    "\n",
    "text_file = open(\"utils/postselection_tagger.h\", \"r\")\n",
    "data = text_file.read()\n",
    "def my_initialization_function():\n",
    "    print(ROOT.gInterpreter.ProcessLine(\".O\"))\n",
    "    ROOT.gInterpreter.Declare('{}'.format(data))\n",
    "    print(\"end of initialization\")\n",
    "\n",
    "# set up everything properly\n",
    "if distributed == True:\n",
    "    RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame\n",
    "    client = Client(address=\"tcp://127.0.0.1:\"+str(sched_port))\n",
    "    client.restart()\n",
    "    client.register_worker_plugin(UploadFile(\"/tmp/x509up_u0\"))\n",
    "    client.run(set_proxy)\n",
    "    ROOT.RDF.Experimental.Distributed.initialize(my_initialization_function)\n",
    "else:\n",
    "    RDataFrame = ROOT.RDataFrame\n",
    "    my_initialization_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963af90a-de0c-44a7-9b64-3927c34eeffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### utils ###################\n",
    "def cut_string(cut):\n",
    "    return cut.replace(\" \", \"\").replace(\"&&\",\"_\").replace(\">\",\"_g_\").replace(\".\",\"_\").replace(\"==\",\"_e_\")\n",
    "\n",
    "############### trigger selection #####################\n",
    "def trigger_filter(df):\n",
    "    df_trig = df.Filter(\"HLT_Mu50 || HLT_IsoMu24 || HLT_OldMu100 || HLT_TkMu100 || HLT_Ele32_WPTight_Gsf\", \"trigger\")\n",
    "    return df_trig\n",
    "################### preselection ###############\n",
    "def goodObjects(df, btagAlg, year, EE):\n",
    "    df_tmp = df.Define(\"GoodMu_idx\", \"GetGoodMuon( Muon_pt,  Muon_eta,  Muon_tightId,  Muon_miniPFRelIso_all)\")\\\n",
    "               .Define(\"GoodEl_idx\", \"GetGoodElectron( Electron_pt,  Electron_eta,  Electron_mvaFall17V2noIso_WP90,  Electron_miniPFRelIso_all)\")\\\n",
    "               .Define(\"GoodJet_idx\", \"GetGoodJet(Jet_pt, Jet_jetId)\")\\\n",
    "               .Define(\"JetBTag_idx\", \"GetJetBTag(GoodJet_idx, \"+bTagAlg+\",\"+str(year)+\",\"+str(EE)+\")\")\\\n",
    "               .Define(\"nGoodJet\", \"static_cast<int>(GoodJet_idx.size());\")\\\n",
    "               .Define(\"nJetBtag\", \"static_cast<int>(JetBTag_idx.size());\")\n",
    "    return df_tmp\n",
    "############### top selection ########################\n",
    "def LeptonTopTag(df, isMC):\n",
    "    df_ = df.Filter(\"atLeast1GoodLep(GoodMu_idx, GoodEl_idx)\").Filter(\"nJetBtag>0\")\\\n",
    "            .Define(\"GoodTopLepBJet_idx\",\"countTopLep(GoodMu_idx, Muon_eta, Muon_phi, GoodEl_idx, Electron_eta, Electron_phi, JetBTag_idx, Jet_eta, Jet_phi)\")\\\n",
    "            .Define(\"GoodTopLepBJet_dr\", \"drBLep(GoodMu_idx, Muon_eta, Muon_phi, GoodEl_idx, Electron_eta, Electron_phi, JetBTag_idx, Jet_eta, Jet_phi)\")\\\n",
    "            .Define(\"NGoodTopLep\", \"static_cast<int>(GoodTopLepBJet_idx.size());\")\\\n",
    "            .Define(\"BJetTopLep\", \"nearest(GoodTopLepBJet_idx, GoodTopLepBJet_dr)\")\n",
    "    if isMC:\n",
    "        df_ = df_.Define(\"BJetTopLep_MCtag\", \"BJetTopLep_MCtag(Jet_matched, Jet_pdgId, Jet_topMother, BJetTopLep)\")\n",
    "    return df_\n",
    "\n",
    "def select_top(df, isMC):\n",
    "\n",
    "    for top in topTypes:\n",
    "        if top == \"Resolved\": idxFatJet = \"{}\"\n",
    "        else: idxFatJet = \"Top\"+top+\"_idxFatJet\"\n",
    "        df = df.Define(\"ClusterT1Top\"+top, \"findTopClustersType1(Top\"+top+\"_TopScore, \"+idxFatJet+\", Top\"+top+\"_idxJet0, Top\"+top+\"_idxJet1, Top\"+top+\"_idxJet2,BJetTopLep)\")\\\n",
    "                            .Define(\"ClusterT2Top\"+top, \"findTopClustersType2(Top\"+top+\"_TopScore, \"+idxFatJet+\", Top\"+top+\"_idxJet0, Top\"+top+\"_idxJet1, Top\"+top+\"_idxJet2,BJetTopLep)\")\\\n",
    "                            .Define(\"nClusterT1Top\"+top, \"static_cast<int> (ClusterT1Top\"+top+\".size());\")\\\n",
    "                            .Define(\"nClusterT2Top\"+top, \"static_cast<int> (ClusterT2Top\"+top+\".size());\")\\\n",
    "                            .Define(\"ClusterT1Top\"+top+\"Reco\", \"RecoTaggedTopCluster\"+top+\"(ClusterT1Top\"+top+\", Top\"+top+\"_TopScore)\")\\\n",
    "                            .Define(\"ClusterT2Top\"+top+\"Reco\", \"RecoTaggedTopCluster\"+top+\"(ClusterT2Top\"+top+\", Top\"+top+\"_TopScore)\")\\\n",
    "                            .Define(\"nTopClusterT1\"+top+\"Reco\", \"Sum(ClusterT1Top\"+top+\"Reco>0)\")\\\n",
    "                            .Define(\"nTopClusterT2\"+top+\"Reco\", \"Sum(ClusterT2Top\"+top+\"Reco>0)\")\n",
    "    if (isMC):\n",
    "        for top in topTypes:\n",
    "            if top == \"Resolved\": idxFatJet = \"{}\"\n",
    "            else: idxFatJet = \"Top\"+top+\"_idxFatJet\"\n",
    "            for tr in TruthType[\"Top\"+top].keys():\n",
    "                if tr != \"standard\": df = df.Define(TruthType[\"Top\"+top+\"\"][tr], \"CalculateTop\"+tr+\"(Top\"+top+\"_idxJet0, Top\"+top+\"_idxJet1, Top\"+top+\"_idxJet2, \"+idxFatJet+\", Jet_matched, Jet_topMother, Jet_pdgId, FatJet_matched, FatJet_topMother, FatJet_pdgId)\")\n",
    "                \n",
    "                df = df.Define(\"ClusterT1Top\"+top+\"MCTag\"+tr, \"MCTaggedTopCluster(ClusterT1Top\"+top+\", \"+TruthType[\"Top\"+top+\"\"][tr]+\")\")\\\n",
    "                                    .Define(\"ClusterT2Top\"+top+\"MCTag\"+tr, \"MCTaggedTopCluster(ClusterT2Top\"+top+\", \"+TruthType[\"Top\"+top+\"\"][tr]+\")\")\\\n",
    "                                    .Define(\"nTopClusterT1\"+top+\"MCTag\"+tr, \"Sum(ClusterT1Top\"+top+\"MCTag\"+tr+\">0)\")\\\n",
    "                                    .Define(\"nTopClusterT2\"+top+\"MCTag\"+tr, \"Sum(ClusterT2Top\"+top+\"MCTag\"+tr+\">0)\")\\\n",
    "                                    .Define(\"nTopClusterT1\"+top+\"TrueReco\"+tr, \"Sum(ClusterT1Top\"+top+\"MCTag\"+tr+\">0 && ClusterT1Top\"+top+\"Reco>0)\")\\\n",
    "                                    .Define(\"nTopClusterT2\"+top+\"TrueReco\"+tr, \"Sum(ClusterT2Top\"+top+\"MCTag\"+tr+\">0 && ClusterT2Top\"+top+\"Reco>0)\")\\\n",
    "                                    .Define(\"TopClusterT1\"+top+\"TrueRecoFirstCluster\"+tr, \"ClusterT1Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT1Top\"+top+\"Reco[0] && ClusterT1Top\"+top+\"Reco[0]!=-1\")\\\n",
    "                                    .Define(\"TopClusterT2\"+top+\"TrueRecoFirstCluster\"+tr, \"ClusterT2Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT2Top\"+top+\"Reco[0] && ClusterT2Top\"+top+\"Reco[0]!=-1\")\\\n",
    "                                    .Define(\"TopClusterT1\"+top+\"FalseRecoFirstCluster\"+tr, \"!ClusterT1Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT1Top\"+top+\"Reco[0] && ClusterT1Top\"+top+\"Reco[0] !=-1\")\\\n",
    "                                    .Define(\"TopClusterT2\"+top+\"FalseRecoFirstCluster\"+tr, \"!ClusterT2Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT2Top\"+top+\"Reco[0] && ClusterT2Top\"+top+\"Reco[0] !=-1\")\\\n",
    "                                    .Define(\"TopClusterT1\"+top+\"MCTagTrueFirstCluster\"+tr, \"ClusterT1Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT1Top\"+top+\"MCTag\"+tr+\"[0]!=-1\")\\\n",
    "                                    .Define(\"TopClusterT2\"+top+\"MCTagTrueFirstCluster\"+tr, \"ClusterT2Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT2Top\"+top+\"MCTag\"+tr+\"[0]!=-1\")\\\n",
    "                                    .Define(\"TopClusterT1\"+top+\"MCTagFalseFirstCluster\"+tr, \"!ClusterT1Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT1Top\"+top+\"MCTag\"+tr+\"[0]!=-1\")\\\n",
    "                                    .Define(\"TopClusterT2\"+top+\"MCTagFalseFirstCluster\"+tr, \"!ClusterT2Top\"+top+\"MCTag\"+tr+\"[0] && ClusterT2Top\"+top+\"MCTag\"+tr+\"[0]!=-1\")\n",
    "        df = df.Define(\"TopMixed_Score_truthstandard\", \"TopMixed_TopScore[TopMixed_truth==1]\")\\\n",
    "               .Define(\"TopMixed_Score_truth1\", \"TopMixed_TopScore[TopMixed_truth1==1 && TopMixed_truth==0 && TopMixed_truth2==0]\")\\\n",
    "               .Define(\"TopMixed_Score_truth2\", \"TopMixed_TopScore[TopMixed_truth2==1 && TopMixed_truth==0]\")\\\n",
    "               .Define(\"TopMixed_Score_truth1_incl\", \"TopMixed_TopScore[TopMixed_truth1==1]\")\\\n",
    "               .Define(\"TopMixed_Score_truth2_incl\", \"TopMixed_TopScore[TopMixed_truth2==1]\")\\\n",
    "               .Define(\"TopResolved_Score_truthstandard\", \"TopResolved_TopScore[TopResolved_truth==1]\")\\\n",
    "               .Define(\"TopResolved_Score_truth1\", \"TopResolved_TopScore[TopResolved_truth1==1 && TopResolved_truth==0 && TopResolved_truth2==0]\")\\\n",
    "               .Define(\"TopResolved_Score_truth2\", \"TopResolved_TopScore[TopResolved_truth2==1 && TopResolved_truth==0]\")\\\n",
    "               .Define(\"TopResolved_Score_truth1_incl\", \"TopResolved_TopScore[TopResolved_truth1==1]\")\\\n",
    "               .Define(\"TopResolved_Score_truth2_incl\", \"TopResolved_TopScore[TopResolved_truth2==1]\")\n",
    "\n",
    "    df_topselected = df.Define(\"Top_idx_Resolved\",\"ClusterT1TopResolved.size()>0 ? ClusterT1TopResolved[0][0]: -1\")\\\n",
    "                       .Define(\"Top_idx_Mixed\",\"ClusterT1TopMixed.size() ? ClusterT1TopMixed[0][0] : -1\")\n",
    "    df_topselected = df_topselected.Define(\"TopMerged_bestTopScore\", \"FatJet_particleNet_TvsQCD[ArgMax(FatJet_particleNet_TvsQCD)]\")\\\n",
    "                                   .Define(\"TopMerged_bestTopPt\", \"FatJet_pt[ArgMax(FatJet_particleNet_TvsQCD)]\")\\\n",
    "                                   .Define(\"TopMerged_bestTopEta\", \"FatJet_eta[ArgMax(FatJet_particleNet_TvsQCD)]\")\\\n",
    "                                   .Define(\"TopMerged_bestTopPhi\", \"FatJet_phi[ArgMax(FatJet_particleNet_TvsQCD)]\")\\\n",
    "                                   .Define(\"TopMerged_bestTopMass\", \"FatJet_mass[ArgMax(FatJet_particleNet_TvsQCD)]\")\\\n",
    "                                   .Define(\"TopMerged_bestTopTruth\", \"FatJet_matched[ArgMax(FatJet_particleNet_TvsQCD)] == 3\")\\\n",
    "                                   .Define(\"TopMerged_truth_exists\", \"Any(FatJet_matched==3)\")\n",
    "    \n",
    "    # return best top idx wrt category --> the idx is referred to the list of candidates fixed by the EventTopCategory\n",
    "    df_topvariables = df_topselected.Define(\"Top_pt_Resolved\", \"Top_idx_Resolved !=-1 ? TopResolved_pt[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_eta_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_eta[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_phi_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_phi[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_mass_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_mass[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_score_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_TopScore[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_idxJet0_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_idxJet0[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_idxJet1_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_idxJet1[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_idxJet2_Resolved\", \"Top_idx_Resolved!=-1 ? TopResolved_idxJet2[Top_idx_Resolved]: -999\")\\\n",
    "                        .Define(\"Top_pt_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_pt[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_eta_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_eta[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_phi_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_phi[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_mass_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_mass[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_score_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_TopScore[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_idxJet0_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_idxJet0[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_idxJet1_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_idxJet1[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_idxJet2_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_idxJet2[Top_idx_Mixed]: -999\")\\\n",
    "                        .Define(\"Top_idxFatJet_Mixed\", \"Top_idx_Mixed!=-1 ? TopMixed_idxFatJet[Top_idx_Mixed]: -999\")\\\n",
    "                        # .Define(\"Top_isolationPtJetsdR04\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 0.4, 1)\")\\\n",
    "                        # .Define(\"Top_isolationPtJetsdR06\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 0.6, 1)\")\\\n",
    "                        # .Define(\"Top_isolationPtJetsdR08\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 0.8, 1)\")\\\n",
    "                        # .Define(\"Top_isolationPtJetsdR12\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 1.2, 1)\")\\\n",
    "                        # .Define(\"Top_isolationNJetsdR04\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 0.4, 0)\")\\\n",
    "                        # .Define(\"Top_isolationNJetsdR06\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 0.6, 0)\")\\\n",
    "                        # .Define(\"Top_isolationNJetsdR08\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 0.8, 0)\")\\\n",
    "                        # .Define(\"Top_isolationNJetsdR12\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 1.2, 0)\")\n",
    "\n",
    "    return df_topvariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69f0bf61-b2ab-422e-8baf-9abba2b984cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookhisto(df, regions_def, var, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var:\n",
    "            if v._MConly and not sampleflag: \n",
    "                continue\n",
    "            else:\n",
    "                if regions_def[reg] == \"\":\n",
    "                    if v._noUnOvFlowbin:\n",
    "                        h_[reg][v._name]= df.Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                    else:\n",
    "                        h_[reg][v._name]= df.Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                else:\n",
    "                    if v._noUnOvFlowbin:\n",
    "                        h_[reg][v._name]= df.Filter(regions_def[reg]).Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                    else:\n",
    "                        h_[reg][v._name]= df.Filter(regions_def[reg]).Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                    \n",
    "    return h_\n",
    "\n",
    "def bookhisto2D(df, regions_def, var2d, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var2d:\n",
    "            if regions_def[reg]==\"\":\n",
    "                h_[reg][v._name] = df.Redefine(v._xname, \"UnOvBin(\"+v._xname+\",\"+str(v._nxbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\")\\\n",
    "                                     .Redefine(v._yname, \"UnOvBin(\"+v._yname+\",\"+str(v._nybins)+\",\"+str(v._ymin)+\",\"+str(v._ymax)+\")\")\\\n",
    "                                     .Histo2D((v._xname+\"Vs\"+v._yname+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax), v._xname, v._yname)\n",
    "            else:\n",
    "                h_[reg][v._name] = df.Filter(regions_def[reg])\\\n",
    "                                     .Redefine(v._xname, \"UnOvBin(\"+v._xname+\",\"+str(v._nxbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\")\\\n",
    "                                     .Redefine(v._yname, \"UnOvBin(\"+v._yname+\",\"+str(v._nybins)+\",\"+str(v._ymin)+\",\"+str(v._ymax)+\")\")\\\n",
    "                                     .Histo2D((v._xname+\"Vs\"+v._yname+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax), v._xname, v._yname)\n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37967949-f502-4c77-9ceb-dd04f8062cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def savehisto(d, h, regions_def, var, s_cut):\n",
    "    histo = {reg: {v._name: ROOT.TH1D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax) for v in var} for reg in regions_def.keys()}\n",
    "    isMC=True\n",
    "    if \"Data\" in d.label: isMC = False\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    \n",
    "    for s in s_list:\n",
    "        outfile = ROOT.TFile.Open(repohisto+s.label+'.root', \"RECREATE\")\n",
    "        for reg in regions_def.keys():\n",
    "            for v in var:\n",
    "                if v._MConly and not isMC:\n",
    "                    continue\n",
    "                else:\n",
    "                    # print(reg, v)\n",
    "                    histo[reg][v._name] = h[d.label][s.label][reg][v._name].GetValue()      \n",
    "                    outfile.cd()\n",
    "                    histo[reg][v._name].Write()\n",
    "        outfile.Close()\n",
    "\n",
    "# i plot2d per il momento non ci servono, si deve trovare un modo più intelligente di farli\n",
    "def savehisto2d(d, h, regions_def, var2d, s_cut):\n",
    "    histo = {reg: {v._name: ROOT.TH2D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax,) for v in var2d} for reg in regions_def.keys()}\n",
    "        \n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    \n",
    "    for s in s_list:\n",
    "        outfile = ROOT.TFile.Open(repohisto+s.label+'_2D.root', \"RECREATE\")\n",
    "        for reg in regions_def.keys():\n",
    "            for v in histo[reg].keys():\n",
    "                histo[reg][v] = h[d.label][s.label][reg][v].GetValue()      \n",
    "                outfile.cd()\n",
    "                histo[reg][v].Write()\n",
    "        outfile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c96d28-45f4-4877-8d00-680e095a3016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#samples['DataHTF_2022']['DataHTF_2022']['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab036200-65f9-44a0-abbb-8a04003fc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples[d.label][d.components[0]]['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b72ddfd-d641-437d-a78c-8568a0a17a30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loop on datasets:  ['WJetsHT2500toInf_2018']\n",
      "Local time : 2024-05-01 20:27:33.439016\n",
      "Initializing DataFrame for WJetsHT2500toInf_2018 chain len =  1\n",
      "['root://cms-xrd-global.cern.ch//store/user/acagnott/DM_Run3_v0/WJetsToLNu_HT-2500ToInf_TuneCP5_13TeV-madgraphMLM-pythia8/WJetsHT2500toInf_2018/231222_110712/0000/tree_hadd_23.root\\n']\n",
      "All histos booked !\n",
      "WJetsHT2500toInf_2018 histos saved\n",
      "Sanpshot done!\n",
      "Job finished in:  0:00:19.120814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TClass::Init>: no dictionary for class edm::Hash<1> is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessHistory is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessConfiguration is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ParameterSetBlob is available\n",
      "Warning in <TClass::Init>: no dictionary for class pair<edm::Hash<1>,edm::ParameterSetBlob> is available\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "print(\"starting loop on datasets: \",[d.label for d in datasets])\n",
    "print(\"Local time :\", t0)\n",
    "# print(\"requirements: \"+cut)\n",
    "\n",
    "h = {}\n",
    "h_2D = {}\n",
    "\n",
    "workers = {}\n",
    "cpu_percentages = {}\n",
    "\n",
    "\n",
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "\n",
    "    if 'Data' in d.label : sampleflag = 0\n",
    "    else: sampleflag = 1\n",
    "    c_ = cut\n",
    "    h[d.label] = {}\n",
    "    h_2D[d.label] = {}\n",
    "    for s in s_list:\n",
    "        \n",
    "#         if distributed:\n",
    "#             worker_cpu_usage = get_worker_cpu_usage(client)\n",
    "#             # print(\"Utilizzo della CPU di ogni worker:\", worker_cpu_usage)\n",
    "\n",
    "#             workers[\"start\"] = list(worker_cpu_usage.keys())\n",
    "#             cpu_percentages[\"start\"] = list(worker_cpu_usage.values())\n",
    "        \n",
    "        #-------------------------------------------------------------------------\n",
    "        ############# Fixing variables for 2018-2022 #############################\n",
    "        #-------------------------------------------------------------------------\n",
    "        if s.year == 2018:\n",
    "            bTagAlg = \"Jet_btagDeepB\"\n",
    "        elif s.year == 2022:\n",
    "            bTagAlg = \"Jet_btagPNetB\"\n",
    "        if hasattr(s,\"EE\"):\n",
    "            EE = s.EE\n",
    "        else:\n",
    "            EE = 0\n",
    "        #-------------------------------------------------------------------------\n",
    "        #########################  DF initialization #############################\n",
    "        #-------------------------------------------------------------------------\n",
    "        \n",
    "        print(\"Initializing DataFrame for \"+ s.label +\" chain len = \", len(chain[d.label][s.label]))\n",
    "        if len(chain[d.label][s.label])==1: print(chain[d.label][s.label])\n",
    "        if distributed ==True:\n",
    "            df = RDataFrame(\"Events\", chain[d.label][s.label], npartitions=nmaxpartition, \n",
    "                            daskclient=client, monitor_label = \"main\" )\n",
    "        else:\n",
    "            df = RDataFrame(\"Events\", chain[d.label][s.label])\n",
    "        \n",
    "        if 'TT' in s.label and not 'Mtt' in s.label:\n",
    "            df = df.Filter(\"tt_mtt_doublecounting(GenPart_pdgId, GenPart_pt, GenPart_eta, GenPart_phi, GenPart_mass)\", \"TT double counting\")\n",
    "        \n",
    "        df_ismc         = df.Define(\"isMC\", \"isMC(\"+str(sampleflag)+\")\")\n",
    "        df_year         = df_ismc.Define(\"year\", str(s.year))\n",
    "        df_hemveto      = df_year.Define(\"HEMVeto\", \"hemveto(Jet_eta, Jet_phi, Electron_eta, Electron_phi)\")\n",
    "        df_hemveto      = df_hemveto.Filter(\"(isMC || (year != 2018) || (HEMVeto || run<319077.))\")\n",
    "        df_hlt          = trigger_filter(df_hemveto)        \n",
    "        df_wnom         = df_hlt.Define('w_nominal', '1').Redefine('w_nominal', 'w_nominalhemveto(w_nominal, HEMVeto, year)')\n",
    "        df_presel       = goodObjects(df_wnom, bTagAlg, s.year, EE)\n",
    "        df_LepTop       = LeptonTopTag(df_presel, sampleflag)\n",
    "        df_TopSel       = select_top(df_LepTop, sampleflag)\n",
    "        \n",
    "        df_snap = df_TopSel\n",
    "        df_plot = df_TopSel\n",
    "        \n",
    "        if do_snapshot:\n",
    "            opts = ROOT.RDF.RSnapshotOptions()\n",
    "            opts.fLazy = True\n",
    "            if distributed: fold = \"./\"\n",
    "            else: fold = folder\n",
    "            snapshot_df = df_snap.Snapshot(\"events_nominal\", fold+\"snapTagger_\"+s.label+\".root\", branches, opts)\n",
    "            # print(\"./\"+s.label+\".root\")\n",
    "        if do_histos:\n",
    "            s_cut = cut_string(cut)\n",
    "            if len(var) != 0 :\n",
    "                h[d.label][s.label] = bookhisto(df_plot, regions_def, var, s_cut)\n",
    "            if len(var2d) != 0 :\n",
    "                h_2D[d.label][s.label] = bookhisto2D(df_plot, regions_def, var2d, s_cut)\n",
    "# if not distributed:\n",
    "#     df_presel.Report().Print()\n",
    "# if distributed:\n",
    "#     worker_cpu_usage = get_worker_cpu_usage(client)\n",
    "#     # print(\"Utilizzo della CPU di ogni worker:\", worker_cpu_usage)\n",
    "\n",
    "#     workers[\"after_book_hist\"] = list(worker_cpu_usage.keys())\n",
    "#     cpu_percentages[\"after_book_hist\"] = list(worker_cpu_usage.values())\n",
    "if do_histos:\n",
    "    print(\"All histos booked !\")\n",
    "    for d in datasets:\n",
    "        if len(var):\n",
    "            savehisto(d, h, regions_def, var, s_cut)\n",
    "        if len(var2d) != 0 :\n",
    "            savehisto2d(d, h_2D, regions_def, var2d, s_cut)\n",
    "        print(d.label + \" histos saved\")\n",
    "# if distributed:\n",
    "#     worker_cpu_usage = get_worker_cpu_usage(client)\n",
    "#     # print(\"Utilizzo della CPU di ogni worker:\", worker_cpu_usage)\n",
    "\n",
    "#     workers[\"end\"] = list(worker_cpu_usage.keys())\n",
    "#     cpu_percentages[\"end\"] = list(worker_cpu_usage.values())\n",
    "    \n",
    "if do_snapshot:\n",
    "    snapshot_df.GetValue()\n",
    "    if distributed: \n",
    "        client.run(transfer_to_tier)\n",
    "        print(\"Snapshots saved and trasfered to tier\")\n",
    "    print(\"Sanpshot done!\")\n",
    "t1 = datetime.now()\n",
    "print(\"Job finished in: \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9573b747-9a5a-4501-b084-481b10d418ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ./results/TopTagger/plots/WJetsHT2500toInf_2018.root Title: \n",
      "All 244.0\n",
      "Presel 244.0\n",
      "1TopLep 112.0\n",
      "0TopLep 120.0\n",
      "atLeast2TopLep 12.0\n",
      "1TopLepMCtag 0.0\n"
     ]
    }
   ],
   "source": [
    "file = ROOT.TFile.Open(repohisto+s.label+\".root\")\n",
    "print(file)\n",
    "# for a in file.GetListOfKeys(): print(a)\n",
    "for reg in regions_def.keys():\n",
    "    # for v in var:\n",
    "    hist = file.Get(var[0]._name+\"_\"+reg+\"_\")\n",
    "    print(reg, hist.Integral())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3606a5b-e3ce-491d-a86a-6c02431ca435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions_def.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "269963fc-638b-4a19-8c8d-af26bad6e37f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# region = \"NoCut\"\n",
    "# var = \"MET_pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8daa57df-4a5e-46d7-b758-84edcebca76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if do_snapshot and distributed:\n",
    "    print(os.popen(\"davix-ls davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{}/ -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name, remote_subfolder_name)).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b07d0-f9c7-4a64-942b-33d356ef4e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cc7ee50-14e3-409c-a954-0575ca5500f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = []\n",
    "hCPU = []\n",
    "for phase in workers.keys():\n",
    "    hCPU.append(ROOT.TH1F(\"hCPU\"+phase,\"workers CPU usage \"+phase+\";workers;CPU usage (%)\", len(workers[phase]), 0.5, len(workers[phase])+0.5))\n",
    "    for i in range(len(workers[phase])):\n",
    "        hCPU[-1].SetBinContent(i+1, cpu_percentages[phase][i])\n",
    "    c1.append(ROOT.TCanvas())\n",
    "    c1[-1].Draw()\n",
    "    hCPU[-1].Draw(\"HIST\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity kernel",
   "language": "python",
   "name": "singularity-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
