{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4213965a-36f5-4afb-aa9a-f4168cbb483b",
   "metadata": {},
   "source": [
    "**Run 3 analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd081e3",
   "metadata": {},
   "source": [
    "**Framework version January 2024**\n",
    "- Log :\n",
    "    - Added systematics\n",
    "    - rescale plot wrt Nexpected \n",
    "- Planned update :\n",
    "    - simplify the code \n",
    "    - standalone code, prepare a couple of input parameters and make the code working with a single command\n",
    "    \n",
    "    \n",
    "__________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98655626-5252-41be-b111-76e638cecddf",
   "metadata": {},
   "source": [
    "**Code**\n",
    "\n",
    "Folder definition on Tier:\n",
    "- in the main folder */acagnott/* added folder 'remote_folder_name';\n",
    "- in \"Snapshots\" added the folder 'remote_subfolder_name';\n",
    "- in the subfolder through dask the snapshot will be copied with name \"snap_\"+label+\"_*.root\"\n",
    "\n",
    "\n",
    "Es: se lancio \"DataMETA_2018\", gli snapshot vengono salvati in ../acagnott/Snapshot/20231229/snap_DataMET_2018_*.root\n",
    "se viene lanciato \"QCD_2018\" viene creata la cartella /acagnott/Snapshot/20231229/snap_QCDHT_100to200_2018_*.root e così via per ogni components\n",
    "\n",
    "---> Viene usato solo il giorno in modo che tutti i sample lanciati lo stesso giorno verranno salvati nella stessa cartella con nomi diversi, visto che vengono lanciati in momenti diversi della giornata lo stesso tipo di job. Forse va modificato il formato se i singoli job iniziano a durare più di un giorno dato che viene comunque lanciato un sampel per volta (in tal caso verrebbero salvati in cartelle diverse. Si potrebbe pensare di mettere la data a mano, cioé invece di usare datetime.now() si potrebbe inserire la data manualmente per fare in modo di mettere tutti i file nella stessa folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf1ce47-e2f0-48f4-8e4b-b0fa9f1bacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "sched_port = 24163#Dask port\n",
    "nmaxpartition = 1#150\n",
    "distributed = True#False#\n",
    "do_variations = True#False#\n",
    "do_histos = True\n",
    "hist_folder = \"run2022_syst\"\n",
    "do_snapshot = False\n",
    "if do_variations : do_snapshot = False\n",
    "remote_subfolder_name = datetime.now().strftime(\"%Y%m%d\") #20231229\n",
    "\n",
    "if do_variations == True:\n",
    "    variations = [\"nominal\", \"pu\", \"jer\", \"jesTotal\"]\n",
    "else :\n",
    "    variations = [\"nominal\"]\n",
    "\n",
    "in_dataset = [\n",
    "    # \"DataMETA_2018\", \"DataMETB_2018\", \"DataMETC_2018\", \"DataMETD_2018\", \"DataSingleMuA_2018\", \"DataSingleMuB_2018\", \"DataSingleMuC_2018\", \"DataSingleMuD_2018\", \"TprimeToTZ_700_2018\", \"TprimeToTZ_1000_2018\", \"TprimeToTZ_1800_2018\", \"QCD_2018\", \"TT_2018\",\"ZJetsToNuNu_2018\", \"WJets_2018\",\n",
    "    \n",
    "    # \"QCD_2022\",\n",
    "    # \"ZJetsToNuNu_2jets_2022\", \n",
    "    # \"ZJetsToNuNu_2jets_PT40to100_2J_2022\"\n",
    "    # \"ZJetsToNuNu_2jets_PT100to200_2J_2022\",\n",
    "    # \"ZJetsToNuNu_2jets_PT200to400_2J_2022\",\n",
    "    # \"ZJetsToNuNu_2jets_PT400to600_2J_2022\",\n",
    "    # \"ZJetsToNuNu_2jets_PT600_2J_2022\",\n",
    "    # \"TT_2022\",\n",
    "    # \"WJets_2jets_2022\"\n",
    "    # \"TprimeToTZ_700_2022\", \"TprimeToTZ_1000_2022\", \"TprimeToTZ_1800_2022\"\n",
    "    # \"TprimeToTZ_800_2022\", \"TprimeToTZ_900_2022\",\"TprimeToTZ_1100_2022\", \"TprimeToTZ_1200_2022\", \"TprimeToTZ_1300_2022\", \"TprimeToTZ_1400_2022\", \n",
    "    \"TprimeToTZ_1500_2022\", \"TprimeToTZ_1600_2022\", \n",
    "    \"TprimeToTZ_1700_2022\", \n",
    "    # \"DataJetMET_2022\"\n",
    "\n",
    "    # \"QCD_2022EE\",    \n",
    "    # \"ZJetsToNuNu_2jets_2022EE\",\n",
    "    # \"TT_2022EE\",\n",
    "    # \"WJets_2jets_2022EE\"\n",
    "    # \"TprimeToTZ_700_2022EE\", \"TprimeToTZ_1000_2022EE\", \"TprimeToTZ_1800_2022EE\",\n",
    "    # \"TprimeToTZ_800_2022EE\", \"TprimeToTZ_900_2022EE\",  \n",
    "    # \"TprimeToTZ_1100_2022EE\", \"TprimeToTZ_1200_2022EE\", \"TprimeToTZ_1300_2022EE\", \"TprimeToTZ_1400_2022EE\", \n",
    "    # \"TprimeToTZ_1500_2022EE\", \"TprimeToTZ_1600_2022EE\", \"TprimeToTZ_1700_2022EE\", \n",
    "    # \"DataJetMET_2022EE\"\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "branches = {\"PuppiMET_T1_pt_nominal\", \"PuppiMET_T1_phi_nominal\", \"MHT\", \n",
    "            \"Top_mass\", \"Top_pt\", \"Top_score\", \"Top_isolationPtJetsdR04\", \"Top_isolationPtJetsdR06\", \"Top_isolationPtJetsdR08\", \"Top_isolationPtJetsdR12\", \"Top_isolationNJetsdR04\", \"Top_isolationNJetsdR06\", \"Top_isolationNJetsdR08\", \"Top_isolationNJetsdR12\",\n",
    "            \"nVetoMuon\", \"nVetoElectron\", \"nJetBtagLoose\", \"nJetBtagMedium\", \n",
    "            \"nGoodJet\", \"nTightElectron\", \"nTightMuon\", \"MT\", \"MT_T\"\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e16aaf3-78d5-4a3c-851c-21f771dcf586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/x509up_u0 /cvmfs/grid.cern.ch/etc/grid-security/certificates/\n",
      "You are producing histograms\n",
      "local folder histos: ./results/run2022_syst/\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import os\n",
    "from utils.samples import *\n",
    "from utils.variables import *\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from dask.distributed import Client\n",
    "ROOT.RDF.Experimental.Distributed.open_files_locally = False\n",
    "\n",
    "os.environ['X509_CERT_DIR'] = \"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "os.environ['X509_USER_PROXY'] = \"/tmp/x509up_u0\"\n",
    "print(os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\"))\n",
    "\n",
    "\n",
    "if distributed:\n",
    "    nfiles_max = 1000\n",
    "else:\n",
    "    nfiles_max = 1  #######\n",
    "\n",
    "# Cosa aggiungere, modificare la cartella sul tier in questo modo ../acagnott/Snapshot_rdf/*dataset_name*/*data di processamente con orario*/\n",
    "# insomma come fa crab\n",
    "\n",
    "\n",
    "if do_histos: print(\"You are producing histograms\")\n",
    "if do_snapshot: print(\"You are producing snapshot\")\n",
    "\n",
    "remote_folder_name = \"Snapshots\"\n",
    "#output histos folder\n",
    "folder = \"./results/\"+hist_folder+\"/\"\n",
    "# eos_folder = \"/eos/home-a/acagnott/DarkMatter/nosynch/\"+hist_folder\n",
    "\n",
    "if do_snapshot and remote_subfolder_name == datetime.now().strftime(\"%Y%m%d\") and distributed: \n",
    "    print(\"You are naming the tier subfolder using the current day \\n\")\n",
    "    print(\"Snapshots folder name : ~/acagnott/{}/{}\".format(remote_folder_name, remote_subfolder_name))\n",
    "elif do_snapshot and distributed:\n",
    "    print(\"You are naming the tier subfolder manually\")\n",
    "    print(\"Snapshots folder name : ~/acagnott/{}/{}\".format(remote_folder_name, remote_subfolder_name))\n",
    "elif do_snapshot:\n",
    "    print(\"You are saving snapshots in local\")\n",
    "    print(\"folder name : \" + folder)\n",
    "\n",
    "\n",
    "if do_histos : \n",
    "    print(\"local folder histos: {}\".format(folder))\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "repohisto = folder+\"plots/\"\n",
    "if not os.path.exists(repohisto):\n",
    "    os.mkdir(repohisto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe0ba0b-23a0-4b47-96f6-4dc0917ec6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating folders on Tier\n",
    "if do_snapshot and distributed:\n",
    "    tier_main_folder = \"davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/\"\n",
    "    os.popen(\"davix-mkdir davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{} -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name))\n",
    "    os.popen(\"davix-mkdir davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{} -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name, remote_subfolder_name))\n",
    "    \n",
    "# transfer function for dask worker\n",
    "def transfer_to_tier(dask_worker):\n",
    "    import os\n",
    "    os.popen('for filename in snap_*.root; do davix-put $filename davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{}/$filename -E ./proxy --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/; done'.format(remote_folder_name, remote_subfolder_name))\n",
    "    return True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3271c9-56c5-4ee1-a501-f85177699ab8",
   "metadata": {},
   "source": [
    "- Import of utils from variables.py\n",
    "Cut (if any), Regions, Variables\n",
    "\n",
    "- syncro between in_dataset and sample_dict (from sample.py) to syncronize labels and ather featurs of the dataset (as sigma if needed)\n",
    "- import of samples_dict.json to load files list (path to reach them on tier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00f446ef-90d9-46e8-ad16-8ded4fa2fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions to book: \n",
      "  SR\n",
      "  SR0fjets\n",
      "  SRatleast1fjets\n",
      "  ResSR\n",
      "  ResSR0fjets\n",
      "  ResSRatleast1fjets\n",
      "  MixSR\n",
      "  MixSR0fjets\n",
      "  MixSRatleast1fjets\n",
      "  MerSR\n",
      "  MerSR0fjets\n",
      "  MerSRatleast1fjets\n",
      "  SRTop\n",
      "  SRTop0fjets\n",
      "  SRTopatleast1fjets\n",
      "  AH\n",
      "  SL\n",
      "  AH1lWR\n",
      "  AH0lZR\n",
      "Variables for histograms :\n",
      "['PuppiMET_pt', 'PuppiMET_phi', 'PuppiMET_T1_pt_nominal', 'PuppiMET_T1_phi_nominal', 'LeadingJetPt_pt', 'LeadingFatJetPt_pt', 'nTopMixed', 'nTopResolved', 'nJet', 'nJetBtagMedium', 'nJetBtagLoose', 'nFatJet', 'MinDelta_phi', 'HT_eventHT', 'MHT', 'PV_npvsGood', 'TopMixed_TopScore_nominal', 'TopResolved_TopScore_nominal', 'EventTopCategory', 'Top_mass', 'Top_pt', 'Top_score', 'MT_T', 'FatJet_particleNetWithMass_TvsQCD']\n",
      "Datasets to process :  ['TprimeToTZ_1500_2022', 'TprimeToTZ_1600_2022', 'TprimeToTZ_1700_2022', 'WJets_2jets_2022EE']\n",
      "Dataset : TprimeToTZ_1500_2022\n",
      "# of files to process :  1\n",
      "files strings :\n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/TprimeToTZ_1500_2022/20241026_174656/tree_hadd_0.root\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  88000\n",
      "Dataset : TprimeToTZ_1600_2022\n",
      "# of files to process :  2\n",
      "files strings :\n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/TprimeToTZ_1600_2022/20241026_174701/tree_hadd_0.root\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  88000\n",
      "Dataset : TprimeToTZ_1700_2022\n",
      "# of files to process :  1\n",
      "files strings :\n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/TprimeToTZ_1700_2022/20241026_174706/tree_hadd_0.root\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  88000\n",
      "Dataset : WJets_2jets0J_2022EE\n",
      "# of files to process :  2510\n",
      "files strings :\n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/WJets_2jets0J_2022EE/20240706_152803/tree_hadd_755.root\n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/WJets_2jets0J_2022EE/20240706_152803/tree_hadd_1035.root\n",
      "  ... \n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/WJets_2jets0J_2022EE/20240706_152803/tree_hadd_135.root\n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/WJets_2jets0J_2022EE/20240706_152803/tree_hadd_182.root\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  414935821\n",
      "Dataset : WJets_2jets1J_2022EE\n",
      "# of files to process :  2669\n",
      "files strings :\n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/WJets_2jets1J_2022EE/20240706_163009/tree_hadd_755.root\n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/WJets_2jets1J_2022EE/20240706_163009/tree_hadd_1035.root\n",
      "  ... \n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/WJets_2jets1J_2022EE/20240706_163009/tree_hadd_182.root\n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/WJets_2jets1J_2022EE/20240706_163009/tree_hadd_2662.root\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  252861610\n",
      "Dataset : WJets_2jets2J_2022EE\n",
      "# of files to process :  2135\n",
      "files strings :\n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/WJets_2jets2J_2022EE/20240706_174757/tree_hadd_755.root\n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/WJets_2jets2J_2022EE/20240706_174757/tree_hadd_1035.root\n",
      "  ... \n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/WJets_2jets2J_2022EE/20240706_174757/tree_hadd_135.root\n",
      "  davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/WJets_2jets2J_2022EE/20240706_174757/tree_hadd_182.root\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  106196255\n"
     ]
    }
   ],
   "source": [
    "cut = requirements # ---> see variables.py\n",
    "\n",
    "regions_def = regions # ---> see variables.py\n",
    "print(\"Regions to book: \")\n",
    "for r in regions_def.keys():\n",
    "    print(\"  \"+r)\n",
    "    \n",
    "sample_file = open(\"utils/dict_samples_2022.json\", \"rb\")\n",
    "samples = json.load(sample_file)\n",
    "sample_file.close()\n",
    "\n",
    "var = vars  # ---> variables.py\n",
    "var2d = vars2D \n",
    "\n",
    "print(\"Variables for histograms :\")\n",
    "print([v._name for v in var])\n",
    "\n",
    "datasets = []\n",
    "for in_d in in_dataset:\n",
    "    if not in_d in sample_dict.keys():\n",
    "        print(\"Check the in_dataset string... \", sample_dict.keys())\n",
    "    else : \n",
    "        datasets.append(sample_dict[in_d])\n",
    "print(\"Datasets to process : \", [d.label for d in datasets])\n",
    "\n",
    "\n",
    "chain = {}\n",
    "ntot_events = {}\n",
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        samples_list = d.components\n",
    "    else:\n",
    "        samples_list = [d]\n",
    "    chain[d.label] = {}\n",
    "    ntot_events[d.label] = {}\n",
    "    for s in samples_list:\n",
    "        if distributed: \n",
    "            nfiles = len(samples[d.label][s.label]['strings'])\n",
    "            for i, string in enumerate(samples[d.label][s.label]['strings']): \n",
    "                if distributed:\n",
    "                    samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"davs://stwebdav.pi.infn.it:8443/cms/\")\n",
    "                else:\n",
    "                    samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"root://stormgf2.pi.infn.it/\")\n",
    "            chain[d.label][s.label] = samples[d.label][s.label]['strings']\n",
    "        else: \n",
    "            nfiles = nfiles_max\n",
    "            for i, string in enumerate(samples[d.label][s.label]['strings']): \n",
    "                if distributed: samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"davs://stwebdav.pi.infn.it:8443/cms/\")\n",
    "                else: samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"root://stormgf2.pi.infn.it/\")\n",
    "            chain[d.label][s.label] = samples[d.label][s.label]['strings'][:nfiles]\n",
    "        if not \"Data\" in s.label: ntot_events[d.label][s.label] = np.sum(samples[d.label][s.label]['ntot'][:nfiles])\n",
    "        else: ntot_events[d.label][s.label] = None\n",
    "        print(\"Dataset : \"+s.label)\n",
    "        print(\"# of files to process : \", nfiles)\n",
    "        if distributed and len(chain[d.label][s.label])>2:\n",
    "            print(\"files strings :\\n  {}\\n  {}\\n  ... \\n  {}\\n  {}\".format(chain[d.label][s.label][0], chain[d.label][s.label][1], chain[d.label][s.label][-2], chain[d.label][s.label][-1]))\n",
    "        else :\n",
    "            print(\"files strings :\\n  {}\".format(chain[d.label][s.label][0]))\n",
    "        print(\"# of total events in the files to process (MC only, if Data the number is None) : \", ntot_events[d.label][s.label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a9be5b-2e52-431b-9709-ee1c3e639b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 17:26:45,348 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "# initialization of clusters\n",
    "\n",
    "# upload the proxyfile to the Dask workers to make them able to access data on the grid \n",
    "\n",
    "from distributed.diagnostics.plugin import UploadFile\n",
    "def set_proxy(dask_worker):\n",
    "    import os\n",
    "    import shutil\n",
    "    working_dir = dask_worker.local_directory\n",
    "    proxy_name = 'x509up_u0'\n",
    "    os.environ['X509_USER_PROXY'] = working_dir + '/' + proxy_name\n",
    "    os.environ['X509_CERT_DIR']=\"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "    shutil.copyfile(working_dir + '/' + proxy_name, working_dir + '/../../../proxy')    \n",
    "    os.environ['EXTRA_CLING_ARGS'] = \"-O2\"\n",
    "    return os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\")\n",
    "\n",
    "text_file = open(\"utils/postselection.h\", \"r\")\n",
    "data = text_file.read()\n",
    "def my_initialization_function():\n",
    "    print(ROOT.gInterpreter.ProcessLine(\".O\"))\n",
    "    ROOT.gInterpreter.Declare('{}'.format(data))\n",
    "    print(\"end of initialization\")\n",
    "\n",
    "# set up everything properly\n",
    "if distributed == True:\n",
    "    RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame\n",
    "    client = Client(address=\"tcp://127.0.0.1:\"+str(sched_port))\n",
    "    client.restart()\n",
    "    # client.register_worker_plugin(UploadFile(\"/tmp/x509up_u0\"))\n",
    "    client.register_plugin(UploadFile(\"/tmp/x509up_u0\"))\n",
    "    client.run(set_proxy)\n",
    "    ROOT.RDF.Experimental.Distributed.initialize(my_initialization_function)\n",
    "else:\n",
    "    RDataFrame = ROOT.RDataFrame\n",
    "    my_initialization_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "963af90a-de0c-44a7-9b64-3927c34eeffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### utils ###################\n",
    "def cut_string(cut):\n",
    "    return cut.replace(\" \", \"\").replace(\"&&\",\"_\").replace(\">\",\"_g_\").replace(\".\",\"_\").replace(\"==\",\"_e_\")\n",
    "\n",
    "################### preselection ###############\n",
    "def preselection(df, btagAlg, year, EE):\n",
    "    \n",
    "    df = df.Define(\"GoodJet_idx\", \"GetGoodJet(Jet_pt_nominal, Jet_eta, Jet_jetId)\")\n",
    "    df = df.Define(\"nGoodJet\", \"nGoodJet(GoodJet_idx)\")\n",
    "    df = df.Define(\"GoodFatJet_idx\", \"GetGoodJet(FatJet_pt_nominal, FatJet_eta, FatJet_jetId)\")\n",
    "    df = df.Define(\"nGoodFatJet\", \"GoodFatJet_idx.size()\")\n",
    "    df = df.Filter(\"nGoodJet>2 || nGoodFatJet>0 \", \"jet presel\")\n",
    "\n",
    "    df = df.Redefine(\"MinDelta_phi\", \"min_DeltaPhi(PuppiMET_T1_phi_nominal, Jet_phi, GoodJet_idx)\")\n",
    "    df = df.Define(\"nTightElectron\", \"nTightElectron(Electron_pt, Electron_eta, Electron_cutBased)\")\n",
    "    df = df.Define(\"TightElectron_idx\", \"TightElectron_idx(Electron_pt, Electron_eta, Electron_cutBased)\")\n",
    "    df = df.Define(\"nVetoElectron\", \"nVetoElectron(Electron_pt, Electron_cutBased, Electron_eta)\")\n",
    "    df = df.Define(\"nTightMuon\", \"nTightMuon(Muon_pt, Muon_eta, Muon_tightId)\")\n",
    "    df = df.Define(\"TightMuon_idx\", \"TightMuon_idx(Muon_pt, Muon_eta, Muon_tightId)\")\n",
    "    df = df.Define(\"nVetoMuon\", \"nVetoMuon(Muon_pt, Muon_eta, Muon_looseId)\")\n",
    "    df = df.Define(\"Lepton_flavour\", \"Lepton_flavour(nTightElectron, nTightMuon)\").Define(\"Lep_pt\", \"Lepton_var(Lepton_flavour, Electron_pt, TightElectron_idx, Muon_pt, TightMuon_idx)\").Define(\"Lep_phi\", \"Lepton_var(Lepton_flavour, Electron_phi, TightElectron_idx, Muon_phi, TightMuon_idx)\")\n",
    "    df = df.Define(\"MT\", \"sqrt(2 * Lep_pt * PuppiMET_T1_pt_nominal * (1 - cos(Lep_phi - PuppiMET_T1_phi_nominal)))\")\n",
    "    \n",
    "    df = df.Define(\"LeadingJetPt_idx\", \"GetLeadingPtJet(Jet_pt_nominal)\")\n",
    "    df = df.Define(\"LeadingJetPt_pt\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_pt_nominal)\")\n",
    "    df = df.Define(\"LeadingJetPt_eta\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_eta)\")\n",
    "    df = df.Define(\"LeadingJetPt_phi\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_phi)\")\n",
    "    df = df.Define(\"LeadingJetPt_mass\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_mass_nominal)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_idx\", \"GetLeadingPtJet(FatJet_pt)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_pt\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_pt_nominal)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_eta\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_eta)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_phi\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_phi)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_mass\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_mass_nominal)\")\n",
    "    df = df.Define(\"LeadingMuonPt_idx\", \"GetLeadingPtLep(Muon_pt, Muon_eta, Muon_looseId)\")\n",
    "    df = df.Define(\"LeadingMuonPt_pt\", \"GetLeadingJetVar(LeadingMuonPt_idx, Muon_pt)\")\n",
    "    df = df.Define(\"LeadingMuonPt_eta\", \"GetLeadingJetVar(LeadingMuonPt_idx, Muon_eta)\")\n",
    "    df = df.Define(\"LeadingMuonPt_phi\", \"GetLeadingJetVar(LeadingMuonPt_idx, Muon_phi)\")\n",
    "    df = df.Define(\"LeadingElectronPt_idx\", \"GetLeadingPtLep(Electron_pt, Electron_eta, Electron_cutBased)\")\n",
    "    df = df.Define(\"LeadingElectronPt_pt\", \"GetLeadingJetVar(LeadingElectronPt_idx, Electron_pt)\")\n",
    "    df = df.Define(\"LeadingElectronPt_eta\", \"GetLeadingJetVar(LeadingElectronPt_idx, Electron_eta)\")\n",
    "    df = df.Define(\"LeadingElectronPt_phi\", \"GetLeadingJetVar(LeadingElectronPt_idx, Electron_phi)\")\n",
    "    \n",
    "    df = df.Define(\"nForwardJet\", \"nForwardJet(Jet_pt_nominal, Jet_jetId, Jet_eta)\")\n",
    "    df = df.Define(\"MHT\",\"MHT(GoodJet_idx, Jet_pt_nominal, Jet_phi, Jet_eta, Jet_mass_nominal)\")\n",
    "    df = df.Define(\"JetBTagLoose_idx\", \"GetJetBTag(GoodJet_idx, \"+bTagAlg+\",\"+str(year)+\",\"+str(EE)+\", 0)\")\\\n",
    "                .Define(\"nJetBtagLoose\", \"static_cast<int>(JetBTagLoose_idx.size());\")\n",
    "    df = df.Define(\"JetBTagMedium_idx\", \"GetJetBTag(GoodJet_idx, \"+bTagAlg+\",\"+str(year)+\",\"+str(EE)+\", 1)\")\\\n",
    "                .Define(\"nJetBtagMedium\", \"static_cast<int>(JetBTagMedium_idx.size());\")\n",
    "    df = df.Redefine(\"PuppiMET_T1_pt_nominal\", \"PuppiMET_T1_pt_nominal_vec[0]\")\\\n",
    "                .Redefine(\"PuppiMET_T1_phi_nominal\", \"PuppiMET_T1_phi_nominal_vec[0]\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "############### trigger selection #####################\n",
    "def trigger_filter(df, data, isMC):\n",
    "    hlt_met = \"(HLT_PFMET120_PFMHT120_IDTight || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight)\"\n",
    "    df_trig = df.Filter(hlt_met, \"triggerMET\")\n",
    "    return df_trig\n",
    "\n",
    "############### top selection ########################\n",
    "def select_top(df, isMC):\n",
    "    # return indices of the FatJet with particleNet score over the thresholds \n",
    "    df_goodtopMer = df.Define(\"GoodTopMer_idx\", \"select_TopMer(FatJet_particleNetWithMass_TvsQCD, GoodFatJet_idx)\")\n",
    "    # return indices of the TopMixed over the threshold with any object in common\n",
    "    df_goodtopMix = df_goodtopMer.Define(\"GoodTopMix_idx\", \"select_TopMix(TopMixed_TopScore_nominal, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, GoodJet_idx, GoodFatJet_idx)\")\n",
    "    # return indices of the TopResolved over the threshold with any object in common\n",
    "    df_goodtopRes = df_goodtopMix.Define(\"GoodTopRes_idx\", \"select_TopRes(TopResolved_TopScore_nominal, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, GoodJet_idx)\")\n",
    "    \n",
    "    df_nTops = df_goodtopRes.Define(\"nGoodTopResolved\", \"nTop(GoodTopRes_idx)\")\\\n",
    "                            .Define(\"nGoodTopMixed\", \"nTop(GoodTopMix_idx)\")\\\n",
    "                            .Define(\"nGoodTopMerged\", \"nTop(GoodTopMer_idx)\")\n",
    "    \n",
    "    \n",
    "    # return:  1- Event Resolved, 2- Event Mixed, 3- Event Merged, 4- Event Nothing, ...\n",
    "    df_topcategory = df_nTops.Define(\"EventTopCategory\", \"select_TopCategory(GoodTopMer_idx, GoodTopMix_idx, GoodTopRes_idx)\")\n",
    "    if isMC:\n",
    "        df_topcategory = df_topcategory.Define(\"EventTopCategoryWithTruth\", \"select_TopCategoryWithTruth(EventTopCategory, FatJet_matched, GoodTopMer_idx, TopMixed_truth, GoodTopMix_idx, TopResolved_truth, GoodTopRes_idx)\")\n",
    "    \n",
    "    df_topselected = df_topcategory.Define(\"Top_idx\",\n",
    "                                           \"select_bestTop(EventTopCategory, FatJet_particleNetWithMass_TvsQCD, TopMixed_TopScore_nominal, TopResolved_TopScore_nominal)\")\n",
    "    # return best top idx wrt category --> the idx is referred to the list of candidates fixed by the EventTopCategory\n",
    "    df_topvariables = df_topselected.Define(\"Top_pt\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_pt_nominal, TopMixed_pt_nominal, TopResolved_pt_nominal)\")\\\n",
    "                        .Define(\"Top_eta\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_eta, TopMixed_eta, TopResolved_eta)\")\\\n",
    "                        .Define(\"Top_phi\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_phi, TopMixed_phi, TopResolved_phi)\")\\\n",
    "                        .Define(\"Top_mass\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_mass_nominal, TopMixed_mass_nominal, TopResolved_mass_nominal)\")\\\n",
    "                        .Define(\"Top_score\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_particleNetWithMass_TvsQCD, TopMixed_TopScore_nominal, TopResolved_TopScore_nominal)\")\\\n",
    "                        .Define(\"Top_isolationPtJetsdR04\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 0.4, 1)\")\\\n",
    "                        .Define(\"Top_isolationPtJetsdR06\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 0.6, 1)\")\\\n",
    "                        .Define(\"Top_isolationPtJetsdR08\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 0.8, 1)\")\\\n",
    "                        .Define(\"Top_isolationPtJetsdR12\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 1.2, 1)\")\\\n",
    "                        .Define(\"Top_isolationNJetsdR04\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 0.4, 0)\")\\\n",
    "                        .Define(\"Top_isolationNJetsdR06\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 0.6, 0)\")\\\n",
    "                        .Define(\"Top_isolationNJetsdR08\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 0.8, 0)\")\\\n",
    "                        .Define(\"Top_isolationNJetsdR12\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 1.2, 0)\")\n",
    "\n",
    "    if isMC:\n",
    "        df_topvariables = df_topvariables.Define(\"Top_truth\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_matched, TopMixed_truth, TopResolved_truth)\")\n",
    "    # NB: TopTruth for Merged is replaced with FatJet_matched, the variable is between 0 and 3 \n",
    "    # where 3 means true end less than 3 means false \n",
    "    return df_topvariables\n",
    "def energetic_variations(df):\n",
    "    df_sys = df.Vary([\"Jet_pt_nominal\", \"Jet_mass_nominal\", \"PuppiMET_T1_pt_nominal_vec\", \"PuppiMET_T1_phi_nominal_vec\", \"TopMixed_pt_nominal\", \"TopResolved_pt_nominal\", \"TopMixed_mass_nominal\", \"TopResolved_mass_nominal\",  \"TopMixed_TopScore_nominal\", \"TopResolved_TopScore_nominal\"], \"RVec<RVec<RVec<float>>>{{Jet_pt_jerdown, Jet_pt_jerup}, {Jet_mass_jerdown, Jet_mass_jerup}, {PuppiMET_T1_pt_jerdown_vec, PuppiMET_T1_pt_jerup_vec}, {PuppiMET_T1_phi_jerdown_vec, PuppiMET_T1_phi_jerup_vec}, {TopMixed_pt_jerdown, TopMixed_pt_jerup}, {TopResolved_pt_jerdown, TopResolved_pt_jerup}, {TopMixed_mass_jerdown, TopMixed_mass_jerup}, {TopResolved_mass_jerdown, TopResolved_mass_jerup}, {TopMixed_TopScore_jerdown, TopMixed_TopScore_jerup}, {TopResolved_TopScore_jerdown, TopResolved_TopScore_jerup}}\", variationTags=[\"down\", \"up\"], variationName=\"jer\")\\\n",
    "               .Vary([\"Jet_pt_nominal\", \"Jet_mass_nominal\", \"PuppiMET_T1_pt_nominal_vec\", \"PuppiMET_T1_phi_nominal_vec\", \"TopMixed_pt_nominal\", \"TopResolved_pt_nominal\", \"TopMixed_mass_nominal\", \"TopResolved_mass_nominal\",  \"TopMixed_TopScore_nominal\", \"TopResolved_TopScore_nominal\"], \"RVec<RVec<RVec<float>>>{{Jet_pt_jesTotaldown, Jet_pt_jesTotalup}, {Jet_mass_jesTotaldown, Jet_mass_jesTotalup}, {PuppiMET_T1_pt_jesTotaldown_vec, PuppiMET_T1_pt_jesTotalup_vec}, {PuppiMET_T1_phi_jesTotaldown_vec, PuppiMET_T1_phi_jesTotalup_vec}, {TopMixed_pt_jesTotaldown, TopMixed_pt_jesTotalup}, {TopResolved_pt_jesTotaldown, TopResolved_pt_jesTotalup}, {TopMixed_mass_jesTotaldown, TopMixed_mass_jesTotalup}, {TopResolved_mass_jesTotaldown, TopResolved_mass_jesTotalup}, {TopMixed_TopScore_jesTotaldown, TopMixed_TopScore_jesTotalup}, {TopResolved_TopScore_jesTotaldown, TopResolved_TopScore_jesTotalup}}\", variationTags=[\"down\", \"up\"], variationName=\"jesTotal\")\n",
    "    return df_sys\n",
    "def SF_variations(df):\n",
    "    df_sys = df.Vary(\"puWeight\", \"RVec<float>{puWeightDown, puWeightUp}\", variationTags=[\"down\", \"up\"], variationName=\"pu\")\n",
    "    return df_sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f0bf61-b2ab-422e-8baf-9abba2b984cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookhisto(df, regions_def, var, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var:\n",
    "            if v._MConly and not sampleflag: \n",
    "                continue\n",
    "            else:\n",
    "                # print(v._name+\"_\"+reg+\"_\"+s_cut)\n",
    "                if regions_def[reg] == \"\":\n",
    "                    if \"NoPu\" in reg: \n",
    "                        h_[reg][v._name]= df.Histo1D((v._name+\"_\"+reg,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                    else: \n",
    "                        h_[reg][v._name]= df.Histo1D((v._name+\"_\"+reg,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                else:\n",
    "                    if \"NoPu\" in reg: \n",
    "                        h_[reg][v._name]= df.Filter(regions_def[reg]).Histo1D((v._name+\"_\"+reg,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                    else: h_[reg][v._name]= df.Filter(regions_def[reg]).Histo1D((v._name+\"_\"+reg,\" ;\"+v._title, v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "    return h_\n",
    "\n",
    "def bookhisto2D(df, regions_def, var2d, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var2d:\n",
    "            if regions_def[reg]==\"\":\n",
    "                h_[reg][v._name] = df.Redefine(v._xname, \"UnOvBin(\"+v._xname+\",\"+str(v._nxbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\")\\\n",
    "                                     .Redefine(v._yname, \"UnOvBin(\"+v._yname+\",\"+str(v._nybins)+\",\"+str(v._ymin)+\",\"+str(v._ymax)+\")\")\\\n",
    "                                     .Histo2D((v._xname+\"Vs\"+v._yname+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax), v._xname, v._yname)\n",
    "            else:\n",
    "                h_[reg][v._name] = df.Filter(regions_def[reg])\\\n",
    "                                     .Redefine(v._xname, \"UnOvBin(\"+v._xname+\",\"+str(v._nxbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\")\\\n",
    "                                     .Redefine(v._yname, \"UnOvBin(\"+v._yname+\",\"+str(v._nybins)+\",\"+str(v._ymin)+\",\"+str(v._ymax)+\")\")\\\n",
    "                                     .Histo2D((v._xname+\"Vs\"+v._yname+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax), v._xname, v._yname)\n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37967949-f502-4c77-9ceb-dd04f8062cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def savehisto(d, dict_h, regions_def, var, s_cut):\n",
    "    histo = {reg: {v._name: ROOT.TH1D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax) for v in var} for reg in regions_def.keys()}\n",
    "    isMC=True\n",
    "    if \"Data\" in d.label: isMC = False\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    \n",
    "    for s in s_list:\n",
    "        outfile = ROOT.TFile.Open(repohisto+s.label+'.root', \"RECREATE\")\n",
    "\n",
    "        for n, vari in enumerate(variations):\n",
    "            for reg in regions_def.keys():\n",
    "                for v in var:\n",
    "                    if \"SFbtag\" in v._name: continue\n",
    "                    if v._MConly and not isMC:\n",
    "                        continue\n",
    "                    else:\n",
    "                        # da capire come fare il getvalue e dividere le variazioni\n",
    "                        if isMC:\n",
    "                            if do_variations:\n",
    "                                if vari=='nominal':\n",
    "                                    h1 = dict_h[d.label][s.label][reg][v._name][\"nominal\"]\n",
    "                                    h1.SetName(h1.GetName()+\"_nominal\")\n",
    "                                    nbins = h1.GetNbinsX()\n",
    "                                    if not v._noUnOvFlowbin:\n",
    "                                        h1.SetBinContent(1, h1.GetBinContent(0) + h1.GetBinContent(1))\n",
    "                                        h1.SetBinError(1, math.sqrt(pow(h1.GetBinError(0),2) + pow(h1.GetBinError(1),2)))\n",
    "                                        h1.SetBinContent(nbins, h1.GetBinContent(nbins) + h1.GetBinContent(nbins+1))\n",
    "                                        h1.SetBinError(nbins, math.sqrt(pow(h1.GetBinError(nbins),2) + pow(h1.GetBinError(nbins+1),2)))\n",
    "                                    if isMC:\n",
    "                                        h1.Scale(s.sigma*10**3/ntot_events[d.label][s.label])\n",
    "                                    histo_name = h1.GetName()\n",
    "                                    if \"nominal\" not in histo_name : h1.SetName(histo_name+\"_nominal\")\n",
    "                                    outfile.cd()\n",
    "                                    h1.Write()\n",
    "                                else:\n",
    "                                    for var_type in ['up', 'down']:\n",
    "                                        h1 = dict_h[d.label][s.label][reg][v._name][vari+\":\"+var_type]\n",
    "                                        # h1.SetName(h1.GetName()+\"_\"+vari+var_type.capitalize())\n",
    "                                        histo_name = h1.GetName()\n",
    "                                        if vari+\"_\"+var_type not in histo_name:\n",
    "                                            h1.SetName(h1.GetName()+\"_\"+vari+\"_\"+var_type)\n",
    "                                        if not v._noUnOvFlowbin:\n",
    "                                            nbins = h1.GetNbinsX()\n",
    "                                            h1.SetBinContent(1, h1.GetBinContent(0) + h1.GetBinContent(1))\n",
    "                                            h1.SetBinError(1, math.sqrt(pow(h1.GetBinError(0),2) + pow(h1.GetBinError(1),2)))\n",
    "                                            h1.SetBinContent(nbins, h1.GetBinContent(nbins) + h1.GetBinContent(nbins+1))\n",
    "                                            h1.SetBinError(nbins, math.sqrt(pow(h1.GetBinError(nbins),2) + pow(h1.GetBinError(nbins+1),2)))\n",
    "                                            \n",
    "                                            # Tommaso aggiunge anche questo loop, ma non so bene a cosa serve\n",
    "                                            # for i in range(0, nbins + 1):          \n",
    "                                            #     if h1.GetBinContent(i) < 0:\n",
    "                                            #         h1.SetBinContent(i, 0.)\n",
    "                                        \n",
    "                                        if isMC:\n",
    "                                            h1.Scale(s.sigma*10**3/ntot_events[d.label][s.label])\n",
    "                                        outfile.cd()\n",
    "                                        h1.Write()\n",
    "                            else:\n",
    "                                histo[reg][v._name] = dict_h[d.label][s.label][reg][v._name].GetValue()      \n",
    "                                if not v._noUnOvFlowbin:\n",
    "                                    nbins = histo[reg][v._name].GetNbinsX()\n",
    "                                    histo[reg][v._name].SetBinContent(1, histo[reg][v._name].GetBinContent(0) + histo[reg][v._name].GetBinContent(1))\n",
    "                                    histo[reg][v._name].SetBinError(1, math.sqrt(pow(histo[reg][v._name].GetBinError(0),2) + pow(histo[reg][v._name].GetBinError(1),2)))\n",
    "                                    histo[reg][v._name].SetBinContent(nbins, histo[reg][v._name].GetBinContent(nbins) + histo[reg][v._name].GetBinContent(nbins+1))\n",
    "                                    histo[reg][v._name].SetBinError(nbins, math.sqrt(pow(histo[reg][v._name].GetBinError(nbins),2) + pow(histo[reg][v._name].GetBinError(nbins+1),2)))\n",
    "                                if isMC:\n",
    "                                    histo[reg][v._name].Scale(s.sigma*10**3/ntot_events[d.label][s.label])\n",
    "                                outfile.cd()\n",
    "                                histo[reg][v._name].Write()\n",
    "                        else:\n",
    "                            histo[reg][v._name] = dict_h[d.label][s.label][reg][v._name].GetValue()\n",
    "                            if not v._noUnOvFlowbin:\n",
    "                                nbins = histo[reg][v._name].GetNbinsX()\n",
    "                                histo[reg][v._name].SetBinContent(1, histo[reg][v._name].GetBinContent(0) + histo[reg][v._name].GetBinContent(1))\n",
    "                                histo[reg][v._name].SetBinError(1, math.sqrt(pow(histo[reg][v._name].GetBinError(0),2) + pow(histo[reg][v._name].GetBinError(1),2)))\n",
    "                                histo[reg][v._name].SetBinContent(nbins, histo[reg][v._name].GetBinContent(nbins) + histo[reg][v._name].GetBinContent(nbins+1))\n",
    "                                histo[reg][v._name].SetBinError(nbins, math.sqrt(pow(histo[reg][v._name].GetBinError(nbins),2) + pow(histo[reg][v._name].GetBinError(nbins+1),2)))\n",
    "                            if isMC:\n",
    "                                histo[reg][v._name].Scale(s.sigma*10**3/ntot_events[d.label][s.label])\n",
    "                            outfile.cd()\n",
    "                            histo[reg][v._name].Write()\n",
    "        outfile.Close()\n",
    "\n",
    "# i plot2d per il momento non ci servono, si deve trovare un modo più intelligente di farli\n",
    "def savehisto2d(d, h, regions_def, var2d, s_cut):\n",
    "    histo = {reg: {v._name: ROOT.TH2D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax,) for v in var2d} for reg in regions_def.keys()}\n",
    "        \n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    \n",
    "    for s in s_list:\n",
    "        outfile = ROOT.TFile.Open(repohisto+s.label+'_2D.root', \"RECREATE\")\n",
    "        for reg in regions_def.keys():\n",
    "            for v in histo[reg].keys():\n",
    "                histo[reg][v] = h[d.label][s.label][reg][v].GetValue()\n",
    "                if isMC:\n",
    "                    histo[reg][v._name].Scale(s.sigma*10**3/ntot_events[d.label][s.label])\n",
    "                outfile.cd()\n",
    "                histo[reg][v].Write()\n",
    "        outfile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c96d28-45f4-4877-8d00-680e095a3016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#samples['DataHTF_2022']['DataHTF_2022']['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab036200-65f9-44a0-abbb-8a04003fc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples[d.label][d.components[0]]['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b72ddfd-d641-437d-a78c-8568a0a17a30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loop on datasets:  ['TprimeToTZ_1500_2022', 'TprimeToTZ_1600_2022', 'TprimeToTZ_1700_2022', 'WJets_2jets_2022EE']\n",
      "Local time : 2024-10-27 17:18:56.350849\n",
      "Initializing DataFrame for TprimeToTZ_1500_2022 chain len =  1\n",
      "['davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/TprimeToTZ_1500_2022/20241026_174656/tree_hadd_0.root']\n",
      "applying vary\n",
      "Initializing DataFrame for TprimeToTZ_1600_2022 chain len =  2\n",
      "applying vary\n",
      "Initializing DataFrame for TprimeToTZ_1700_2022 chain len =  1\n",
      "['davs://stwebdav.pi.infn.it:8443/cms//store/user/acagnott/Run3Analysis_Tprime/TprimeToTZ_1700_2022/20241026_174706/tree_hadd_0.root']\n",
      "applying vary\n",
      "Initializing DataFrame for WJets_2jets0J_2022EE chain len =  2510\n",
      "applying vary\n",
      "Initializing DataFrame for WJets_2jets1J_2022EE chain len =  2669\n",
      "applying vary\n",
      "Initializing DataFrame for WJets_2jets2J_2022EE chain len =  2135\n",
      "applying vary\n",
      "All histos booked !\n",
      "dict_keys(['TprimeToTZ_1500_2022', 'TprimeToTZ_1600_2022', 'TprimeToTZ_1700_2022', 'WJets_2jets_2022EE'])\n",
      "TprimeToTZ_1500_2022 histos saved\n",
      "dict_keys(['TprimeToTZ_1500_2022', 'TprimeToTZ_1600_2022', 'TprimeToTZ_1700_2022', 'WJets_2jets_2022EE'])\n",
      "TprimeToTZ_1600_2022 histos saved\n",
      "dict_keys(['TprimeToTZ_1500_2022', 'TprimeToTZ_1600_2022', 'TprimeToTZ_1700_2022', 'WJets_2jets_2022EE'])\n",
      "TprimeToTZ_1700_2022 histos saved\n",
      "dict_keys(['TprimeToTZ_1500_2022', 'TprimeToTZ_1600_2022', 'TprimeToTZ_1700_2022', 'WJets_2jets_2022EE'])\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[11], line 114\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# print(h_varied[d.label].keys())\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     \u001b[43msavehisto\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_varied\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregions_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_cut\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36msavehisto\u001b[0;34m(d, dict_h, regions_def, var, s_cut)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vari\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnominal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 24\u001b[0m     h1 \u001b[38;5;241m=\u001b[39m \u001b[43mdict_h\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnominal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m     h1\u001b[38;5;241m.\u001b[39mSetName(h1\u001b[38;5;241m.\u001b[39mGetName()\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_nominal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/DistRDF/Proxy.py:135\u001b[0m, in \u001b[0;36mResultMapProxy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03mEquivalent of 'operator[]' of the RResultMap. Triggers the computation\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03mgraph, then returns the varied value linked to the 'key' name.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m \u001b[43mexecute_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxied_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/DistRDF/Proxy.py:58\u001b[0m, in \u001b[0;36mexecute_graph\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _managed_tcontext():\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# All the information needed to reconstruct the computation graph on\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# the workers is contained in the head node\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/DistRDF/HeadNode.py:247\u001b[0m, in \u001b[0;36mHeadNode.execute_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     returned_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcessAndMerge\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_ranges\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistrdf_reducer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Perform any extra checks that may be needed according to the\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# type of the head node\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/DistRDF/Backends/Dask/Backend.py:205\u001b[0m, in \u001b[0;36mDaskBackend.ProcessAndMerge\u001b[0;34m(self, ranges, mapper, reducer)\u001b[0m\n\u001b[1;32m    203\u001b[0m progress(final_results)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfinal_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/base.py:375\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03mThis turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask/base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask_jobqueue/runner.py:18\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(*_)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Close gracefully when receiving a SIGINT\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m signal\u001b[38;5;241m.\u001b[39msignal(signal\u001b[38;5;241m.\u001b[39mSIGINT, \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39m_: \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRole\u001b[39;00m(Enum):\n",
      "\u001b[0;31mSystemExit\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2186\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2183\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2185\u001b[0m         \u001b[38;5;66;03m# Actually show the traceback\u001b[39;00m\n\u001b[0;32m-> 2186\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   2189\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_exception_only(), file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ipykernel/zmqshell.py:553\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_showtraceback\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, evalue, stb):\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;66;03m# try to preserve ordering of tracebacks and print statements\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m     exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(evalue),\n\u001b[1;32m    560\u001b[0m     }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ipykernel/iostream.py:604\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"trigger actual zmq send\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03msend will happen in the background thread\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mthread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    602\u001b[0m ):\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;66;03m# request flush on the background thread\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# wait for flush to actually get through, if we can.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m     evt \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mEvent()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ipykernel/iostream.py:267\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     f()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/zmq/sugar/socket.py:701\u001b[0m, in \u001b[0;36mSocket.send\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[1;32m    695\u001b[0m             data,\n\u001b[1;32m    696\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[1;32m    697\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    698\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[1;32m    699\u001b[0m         )\n\u001b[1;32m    700\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m--> 701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m_zmq.py:1092\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_zmq.py:1140\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_zmq.py:1339\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._send_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_zmq.py:160\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._check_rc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dask_jobqueue/runner.py:18\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(*_)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Worker\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Close gracefully when receiving a SIGINT\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m signal\u001b[38;5;241m.\u001b[39msignal(signal\u001b[38;5;241m.\u001b[39mSIGINT, \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39m_: \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRole\u001b[39;00m(Enum):\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    This Enum contains the various roles processes can be.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mSystemExit\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TClass::Init>: no dictionary for class edm::Hash<1> is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessHistory is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessConfiguration is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ParameterSetBlob is available\n",
      "Warning in <TClass::Init>: no dictionary for class pair<edm::Hash<1>,edm::ParameterSetBlob> is available\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "print(\"starting loop on datasets: \",[d.label for d in datasets])\n",
    "print(\"Local time :\", t0)\n",
    "# print(\"requirements: \"+cut)\n",
    "\n",
    "h = {}\n",
    "h_2D = {}\n",
    "if do_variations:\n",
    "    h_varied = {}\n",
    "\n",
    "for d in datasets:\n",
    "    s_list = []\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    if 'Data' in d.label : sampleflag = 0\n",
    "    else: sampleflag = 1\n",
    "    c_ = cut\n",
    "    h[d.label] = {}\n",
    "    h_2D[d.label] = {}\n",
    "    if do_variations:\n",
    "        h_varied[d.label]={}\n",
    "    for s in s_list:\n",
    "        #-------------------------------------------------------------------------\n",
    "        ############# Fixing variables for 2018-2022 #############################\n",
    "        #-------------------------------------------------------------------------\n",
    "        if s.year == 2018:\n",
    "            bTagAlg = \"Jet_btagDeepB\"\n",
    "        elif s.year == 2022:\n",
    "            bTagAlg = \"Jet_btagPNetB\"\n",
    "        if hasattr(s,\"EE\"):\n",
    "            EE = s.EE\n",
    "        else:\n",
    "            EE = 0\n",
    "        #-------------------------------------------------------------------------\n",
    "        #########################  DF initialization #############################\n",
    "        #-------------------------------------------------------------------------\n",
    "        \n",
    "        print(\"Initializing DataFrame for \"+ s.label +\" chain len = \", len(chain[d.label][s.label]))\n",
    "        if len(chain[d.label][s.label])==1: print(chain[d.label][s.label])\n",
    "        if distributed ==True:\n",
    "            df = RDataFrame(\"Events\", chain[d.label][s.label], npartitions=nmaxpartition, \n",
    "                            daskclient=client, monitor_label = \"main\" )\n",
    "        else:\n",
    "            df = RDataFrame(\"Events\", chain[d.label][s.label])\n",
    "        df = df.Define(\"PuppiMET_T1_pt_nominal_vec\", \"RVec<float>{ (float) PuppiMET_T1_pt_nominal}\").Define(\"PuppiMET_T1_phi_nominal_vec\", \"RVec<float>{ (float) PuppiMET_T1_phi_nominal}\")\\\n",
    "               .Define(\"PuppiMET_T1_pt_jerdown_vec\", \"RVec<float>{ (float) PuppiMET_T1_pt_jerdown}\").Define(\"PuppiMET_T1_phi_jerdown_vec\", \"RVec<float>{ (float) PuppiMET_T1_phi_jerdown}\")\\\n",
    "               .Define(\"PuppiMET_T1_pt_jerup_vec\", \"RVec<float>{ (float) PuppiMET_T1_pt_jerup}\").Define(\"PuppiMET_T1_phi_jerup_vec\", \"RVec<float>{ (float) PuppiMET_T1_phi_jerup}\")\\\n",
    "               .Define(\"PuppiMET_T1_pt_jesTotaldown_vec\", \"RVec<float>{ (float) PuppiMET_T1_pt_jesTotaldown}\").Define(\"PuppiMET_T1_phi_jesTotaldown_vec\", \"RVec<float>{ (float) PuppiMET_T1_phi_jesTotaldown}\")\\\n",
    "               .Define(\"PuppiMET_T1_pt_jesTotalup_vec\", \"RVec<float>{ (float) PuppiMET_T1_pt_jesTotalup}\").Define(\"PuppiMET_T1_phi_jesTotalup_vec\", \"RVec<float>{ (float) PuppiMET_T1_phi_jesTotalup}\")\n",
    "        if do_variations:\n",
    "            df              = SF_variations(df)\n",
    "            df              = energetic_variations(df)\n",
    "        else:\n",
    "            df              = df\n",
    "        df_ismc         = df.Define(\"isMC\", \"isMC(\"+str(sampleflag)+\")\")\n",
    "        df_year         = df_ismc.Define(\"year\", str(s.year))\n",
    "        df_hemveto      = df_year.Define(\"HEMVeto\", \"hemveto(Jet_eta, Jet_phi, Electron_eta, Electron_phi)\")\n",
    "        df_hemveto      = df_hemveto.Filter(\"(isMC || (year != 2018) || (HEMVeto || run<319077.))\")\n",
    "        df_hlt          = trigger_filter(df_hemveto, s.label, sampleflag)\n",
    "        \n",
    "        if \"ZJets\" in s.label: \n",
    "            df_hlt = df_hlt.Define(\"w_nominal\", \"nloewcorrectionZ(1., GenPart_pdgId, GenPart_pt, GenPart_statusFlags)\")\n",
    "        elif \"WJets\" in s.label:\n",
    "            df_hlt = df_hlt.Define(\"w_nominal\", \"nloewcorrectionW(1., GenPart_pdgId, GenPart_pt, GenPart_statusFlags)\")\n",
    "        else:\n",
    "            df_hlt = df_hlt.Define(\"w_nominal\", \"1\")\n",
    "            \n",
    "        if sampleflag: df_wnom = df_hlt.Redefine('w_nominal', 'w_nominal*puWeight*SFbtag_nominal*(LHEWeight_originalXWGTUP/abs(LHEWeight_originalXWGTUP))')  \n",
    "        else: df_wnom           = df_hlt.Redefine('w_nominal', '1')\n",
    "\n",
    "            \n",
    "        # df_wnom           = df_hlt.Define('w_nominal', '1')\n",
    "        df_presel       = preselection(df_wnom, bTagAlg, s.year, EE)\n",
    "        df_topsel       = select_top(df_presel, sampleflag)\n",
    "        df_topsel       = df_topsel.Define(\"MT_T\", \"sqrt(2 * Top_pt * PuppiMET_T1_pt_nominal * (1 - cos(Top_phi - PuppiMET_T1_phi_nominal)))\")\n",
    "        \n",
    "        if do_snapshot:\n",
    "            opts = ROOT.RDF.RSnapshotOptions()\n",
    "            opts.fLazy = True\n",
    "            if distributed: fold = \"./\"\n",
    "            else: fold = folder\n",
    "            snapshot_df = df_topsel.Snapshot(\"events_nominal\", fold+\"snap_\"+s.label+\".root\", branches, opts)\n",
    "            # print(\"./\"+s.label+\".root\")\n",
    "        if do_histos:\n",
    "            s_cut = cut_string(cut)\n",
    "            if len(var) != 0 :\n",
    "                h[d.label][s.label] = bookhisto(df_topsel, regions_def, var, s_cut)\n",
    "            if len(var2d) != 0 :\n",
    "                h_2D[d.label][s.label] = bookhisto2D(df_topsel, regions_def, var2d, s_cut)\n",
    "\n",
    "        \n",
    "        if do_variations:\n",
    "            # h [dataset][label][region][variable]\n",
    "            print(\"applying vary\")\n",
    "            h_varied[d.label][s.label]={}\n",
    "            for reg in regions_def.keys():\n",
    "                h_varied[d.label][s.label][reg] = {}\n",
    "                for v in var:\n",
    "                    if \"SFbtag\" in v._name: continue\n",
    "                    if distributed == True:\n",
    "                        h_varied[d.label][s.label][reg][v._name] = ROOT.RDF.Experimental.Distributed.VariationsFor(h[d.label][s.label][reg][v._name])\n",
    "                    else:\n",
    "                        h_varied[d.label][s.label][reg][v._name] = ROOT.RDF.Experimental.VariationsFor(h[d.label][s.label][reg][v._name])\n",
    "\n",
    "if do_histos:\n",
    "    print(\"All histos booked !\")\n",
    "    for d in datasets:\n",
    "        if len(var):\n",
    "            if do_variations:\n",
    "                print(h_varied.keys())\n",
    "                # print(h_varied[d.label].keys())\n",
    "                savehisto(d, h_varied, regions_def, var, s_cut)\n",
    "            else:\n",
    "                savehisto(d, h, regions_def, var, s_cut)\n",
    "        if len(var2d) != 0 :\n",
    "            savehisto2d(d, h_2D, regions_def, var2d, s_cut)\n",
    "        print(d.label + \" histos saved\")\n",
    "if do_snapshot:\n",
    "    snapshot_df.GetValue()\n",
    "    if distributed: \n",
    "        client.run(transfer_to_tier)\n",
    "        print(\"Snapshots saved and trasfered to tier\")\n",
    "    print(\"Sanpshot done!\")\n",
    "t1 = datetime.now()\n",
    "print(\"Job finished in: \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16fe229-69f3-4a0e-ae7c-c754c3fcda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "repohisto+s.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573b747-9a5a-4501-b084-481b10d418ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = ROOT.TFile.Open(repohisto+s.label+\".root\")\n",
    "print(file)\n",
    "# for a in file.GetListOfKeys(): print(a)\n",
    "for reg in regions_def.keys():\n",
    "    # for v in var:\n",
    "    hist = file.Get(var[22]._name+\"_\"+reg+\"_nominal\")\n",
    "    print(reg, hist.Integral())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3606a5b-e3ce-491d-a86a-6c02431ca435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syst = \"jer\"\n",
    "\n",
    "# h = file.Get(\"PuppiMET_T1_pt_SR__nominal\")\n",
    "# h.SetLineColor(ROOT.kBlue)\n",
    "# h1 = file.Get(\"PuppiMET_T1_pt_SR__\"+syst+\"Down\")\n",
    "# h1.SetLineColor(ROOT.kRed)\n",
    "# h2 = file.Get(\"PuppiMET_T1_pt_SR__\"+syst+\"Up\")\n",
    "# h2.SetLineColor(ROOT.kGreen)\n",
    "# c = ROOT.TCanvas()\n",
    "# c.Draw()\n",
    "# h.Draw(\"HIST\")\n",
    "# h1.Draw(\"HIST same\")\n",
    "# h2.Draw(\"HIST same\")\n",
    "# c.SetLogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269963fc-638b-4a19-8c8d-af26bad6e37f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h_varied[d.label][s.label][\"SR\"][\"PuppiMET_pt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa57df-4a5e-46d7-b758-84edcebca76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if do_snapshot and distributed:\n",
    "    print(os.popen(\"davix-ls davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{}/ -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name, remote_subfolder_name)).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890dba80-c7ab-4901-8df6-401b227b9b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba5c96-ad75-4d34-a8b9-b5e06746282a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05068ab-ea06-42da-a3ee-32893dbd6fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4ed52-c2be-454a-9867-a3a616bbcc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56665013-ff33-46db-90e1-d479c5088f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity kernel - Base ROOT 6.32.02 + CMSJMECalculator 0.2.0 + Correctionlib 2.6.1",
   "language": "python",
   "name": "singularity-kernel-root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
