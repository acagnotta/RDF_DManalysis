{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd081e3",
   "metadata": {},
   "source": [
    "**Framework version January 2024**\n",
    "- Log :\n",
    "    - understood bug in data (the problem was in CRAB not here)\n",
    "    - added Snapshot \n",
    "- Planned update :\n",
    "    - standalone code, prepare a couple of input parameters and make the code working with a single command\n",
    "    \n",
    "    \n",
    "__________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98655626-5252-41be-b111-76e638cecddf",
   "metadata": {},
   "source": [
    "**Code**\n",
    "\n",
    "Folder definition on Tier:\n",
    "- in the main folder */acagnott/* added folder 'remote_folder_name';\n",
    "- in \"Snapshots\" added the folder 'remote_subfolder_name';\n",
    "- in the subfolder through dask the snapshot will be copied with name \"snap_\"+label+\"_*.root\"\n",
    "\n",
    "\n",
    "Es: se lancio \"DataMETA_2018\", gli snapshot vengono salvati in ../acagnott/Snapshot/20231229/snap_DataMET_2018_*.root\n",
    "se viene lanciato \"QCD_2018\" viene creata la cartella /acagnott/Snapshot/20231229/snap_QCDHT_100to200_2018_*.root e così via per ogni components\n",
    "\n",
    "---> Viene usato solo il giorno in modo che tutti i sample lanciati lo stesso giorno verranno salvati nella stessa cartella con nomi diversi, visto che vengono lanciati in momenti diversi della giornata lo stesso tipo di job. Forse va modificato il formato se i singoli job iniziano a durare più di un giorno dato che viene comunque lanciato un sampel per volta (in tal caso verrebbero salvati in cartelle diverse. Si potrebbe pensare di mettere la data a mano, cioé invece di usare datetime.now() si potrebbe inserire la data manualmente per fare in modo di mettere tutti i file nella stessa folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf1ce47-e2f0-48f4-8e4b-b0fa9f1bacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "sched_port = 22703 #Dask port\n",
    "nmaxpartition = 100 \n",
    "distributed = True#False#\n",
    "do_histos = True\n",
    "hist_folder = \"run2018_regions_v0\"\n",
    "do_snapshot = False\n",
    "remote_subfolder_name = datetime.now().strftime(\"%Y%m%d\") #20231229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e16aaf3-78d5-4a3c-851c-21f771dcf586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.27/01\n",
      "/tmp/x509up_u0 /cvmfs/grid.cern.ch/etc/grid-security/certificates/\n",
      "You are producing histograms\n",
      "local folder histos: ./results/run2018_regions_v0/\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import os\n",
    "from utils.samples import *\n",
    "from utils.variables import *\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from dask.distributed import Client\n",
    "ROOT.RDF.Experimental.Distributed.open_files_locally = False\n",
    "\n",
    "os.environ['X509_CERT_DIR'] = \"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "os.environ['X509_USER_PROXY'] = \"/tmp/x509up_u0\"\n",
    "print(os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\"))\n",
    "\n",
    "\n",
    "if distributed:\n",
    "    nfiles_max = 1000\n",
    "else:\n",
    "    nfiles_max = 1  #######\n",
    "\n",
    "# Cosa aggiungere, modificare la cartella sul tier in questo modo ../acagnott/Snapshot_rdf/*dataset_name*/*data di processamente con orario*/\n",
    "# insomma come fa crab\n",
    "\n",
    "\n",
    "if do_histos: print(\"You are producing histograms\")\n",
    "if do_snapshot: print(\"You are producing snapshot\")\n",
    "\n",
    "remote_folder_name = \"Snapshots\"\n",
    "#output histos folder\n",
    "folder = \"./results/\"+hist_folder+\"/\"\n",
    "# eos_folder = \"/eos/home-a/acagnott/DarkMatter/nosynch/\"+hist_folder\n",
    "\n",
    "if do_snapshot and remote_subfolder_name == datetime.now().strftime(\"%Y%m%d\") and distributed: \n",
    "    print(\"You are naming the tier subfolder using the current day \\n\")\n",
    "    print(\"Snapshots folder name : ~/acagnott/{}/{}\".format(remote_folder_name, remote_subfolder_name))\n",
    "elif do_snapshot and distributed:\n",
    "    print(\"You are naming the tier subfolder manually\")\n",
    "    print(\"Snapshots folder name : ~/acagnott/{}/{}\".format(remote_folder_name, remote_subfolder_name))\n",
    "elif do_snapshot:\n",
    "    print(\"You are saving snapshots in local\")\n",
    "    print(\"folder name : \" + folder)\n",
    "\n",
    "\n",
    "if do_histos : \n",
    "    print(\"local folder histos: {}\".format(folder))\n",
    "    # print(\"At the end of the job the histos will be transfer to eos storage\")\n",
    "    # print(\"eos folder : {}\".format(eos_folder))\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "repohisto = folder+\"plots/\"\n",
    "if not os.path.exists(repohisto):\n",
    "    os.mkdir(repohisto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813dd925-401d-42be-b0e1-125c082bdc74",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "in_dataset = [    \n",
    "    # \"DataMETA_2018\", \n",
    "    # \"DataMETB_2018\", \n",
    "    # \"DataMETC_2018\",\n",
    "    # \"DataMETD_2018\",\n",
=======
    "cut = requirements # ---> vedi variables.py\n",
    "in_dataset = [\n",
    "    # \"DataMETA_2018\", #ReReco\n",
    "    \n",
    "    \n",
    "    \"DataHTA_2018\", \n",
    "    # \"DataHTB_2018\", \n",
    "    # \"DataHTC_2018\",\n",
    "    # \"DataHTD_2018\",\n",
>>>>>>> 399b32f64ec473bbad4af6ac469218ccd9f0958d
    "    # \"DataSingleMuA_2018\", \n",
    "    # \"DataSingleMuB_2018\", \n",
    "    # \"DataSingleMuC_2018\",\n",
    "    # \"DataSingleMuD_2018\",\n",
    "    \n",
    "    # \"TprimeToTZ_700_2018\", \n",
    "    # \"TprimeToTZ_1000_2018\", \n",
    "    # \"TprimeToTZ_1800_2018\",\n",
    "    \n",
    "    # \"QCD_2018\",\n",
<<<<<<< HEAD
    "    \"TT_2018\",\n",
    "    # \"ZJetsToNuNu_2018\",\n",
    "    # \"WJets_2018\",\n",
    "    \n",
    "    # \"QCDHT_100to200_2018\", \"QCDHT_200to300_2018\",\"QCDHT_300to500_2018\", \"QCDHT_500to700_2018\", \"QCDHT_700to1000_2018\",\"QCDHT_1000to1500_2018\", \"QCDHT_1500to2000_2018\",\"QCDHT_2000toInf_2018\",\"TT_hadr_2018\", \"TT_semilep_2018\", \"TT_Mtt700to1000_2018\", \"TT_Mtt1000toInf_2018\",\"ZJetsToNuNu_HT100to200_2018\", \"ZJetsToNuNu_HT200to400_2018\", \"ZJetsToNuNu_HT400to600_2018\", \"ZJetsToNuNu_HT600to800_2018\", \"ZJetsToNuNu_HT800to1200_2018\", \"ZJetsToNuNu_HT1200to2500_2018\", \"ZJetsToNuNu_HT2500toInf_2018\"\"WJetsHT100to200_2018\", \"WJetsHT200to400_2018\", \"WJetsHT400to600_2018\", \"WJetsHT600to800_2018\", \"WJetsHT800to1200_2018\", \"WJetsHT1200to2500_2018\", \"WJetsHT2500toInf_2018\"\n",
    "]\n",
    "\n",
    "branches = {\"event\", \"run\", \"HT_eventHT\",  \n",
    "            \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60\", \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight\", \n",
    "            \"HLT_Ele32_WPTight_Gsf\", \"HLT_Ele115_CaloIdVT_GsfTrkIdT\", \"HLT_Photon200\", \n",
    "            \"HLT_IsoMu24\",\"HLT_PFMET120_PFMHT120_IDTight\", \"HLT_PFMETNoMu140_PFMHTNoMu140_IDTight\", \n",
    "            \"HLT_PFMET140_PFMHT140_IDTight\", \"MET_pt\", \"nVetoMuon\", \"nVetoElectron\", \"nJetBtag\", \n",
    "            \"nGoodJet\", \"nTightElectron\", \"nTightMuon\", \"MT\"}"
=======
    "    # \"TT_2018\",\n",
    "    # \"ZJetsToNuNu_2018\",\n",
    "    # \"WJets_2018\",\n",
    "    \n",
    "    # \"QCDHT_100to200_2018\", \n",
    "    # \"QCDHT_200to300_2018\",\n",
    "    # \"QCDHT_300to500_2018\", \n",
    "    # \"QCDHT_500to700_2018\", \n",
    "    # \"QCDHT_700to1000_2018\",\n",
    "    # \"QCDHT_1000to1500_2018\", \n",
    "    # \"QCDHT_1500to2000_2018\",\n",
    "    # \"QCDHT_2000toInf_2018\",\n",
    "    # \"TT_hadr_2018\", \n",
    "    # \"TT_semilep_2018\", \n",
    "    # \"TT_Mtt700to1000_2018\", \n",
    "    # \"TT_Mtt1000toInf_2018\",\n",
    "    # \"ZJetsToNuNu_HT100to200_2018\", \n",
    "    # \"ZJetsToNuNu_HT200to400_2018\", \n",
    "    # \"ZJetsToNuNu_HT400to600_2018\", \n",
    "    # \"ZJetsToNuNu_HT600to800_2018\", \n",
    "    # \"ZJetsToNuNu_HT800to1200_2018\", \n",
    "    # \"ZJetsToNuNu_HT1200to2500_2018\", \n",
    "    # \"ZJetsToNuNu_HT2500toInf_2018\"\n",
    "    # \"WJetsHT100to200_2018\", \n",
    "    # \"WJetsHT200to400_2018\", \n",
    "    # \"WJetsHT400to600_2018\", \n",
    "    # \"WJetsHT600to800_2018\", \n",
    "    # \"WJetsHT800to1200_2018\", \n",
    "    # \"WJetsHT1200to2500_2018\", \n",
    "    # \"WJetsHT2500toInf_2018\"\n",
    "]"
>>>>>>> 399b32f64ec473bbad4af6ac469218ccd9f0958d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe0ba0b-23a0-4b47-96f6-4dc0917ec6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating folders on Tier\n",
    "if do_snapshot and distributed:\n",
    "    tier_main_folder = \"davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/\"\n",
    "    os.popen(\"davix-mkdir davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{} -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name))\n",
    "    os.popen(\"davix-mkdir davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{} -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name, remote_subfolder_name))\n",
    "    \n",
    "# transfer function for dask worker\n",
    "def transfer_to_tier(dask_worker):\n",
    "    import os\n",
    "    os.popen('for filename in snap_*.root; do davix-put $filename davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{}/$filename -E ./proxy --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/; done'.format(remote_folder_name, remote_subfolder_name))\n",
    "    return True #, os.popen(\"for filename in ./test_*.root; do echo $filename; done\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3271c9-56c5-4ee1-a501-f85177699ab8",
   "metadata": {},
   "source": [
    "- Import of utils from variables.py\n",
    "Cut (if any), Regions, Variables\n",
    "\n",
<<<<<<< HEAD
    "- syncro between in_dataset and sample_dict (from sample.py) to syncronize labels and ather featurs of the dataset (as sigma if needed)\n",
    "- import of samples_dict.json to load files list (path to reach them on tier)\n"
=======
    "sched_port = 23853 #Dask port\n",
    "nmaxpartition = 100 # to set at lower value\n",
    "distributed = False#True#\n",
    "if distributed:\n",
    "    nfiles_max = 1000\n",
    "else:\n",
    "    nfiles_max = 1  #######"
>>>>>>> 399b32f64ec473bbad4af6ac469218ccd9f0958d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f446ef-90d9-46e8-ad16-8ded4fa2fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions to book: \n",
      "  Presel\n",
      "  AH\n",
      "  SL\n",
      "  SEl\n",
      "  SMu\n",
      "  AH1lWR\n",
      "  AH1lWREl\n",
      "  AH1lWRMu\n",
      "Variables for histograms :\n",
      "['MET_pt', 'MET_phi', 'PuppiMET_pt', 'LeadingJetPt_pt', 'LeadingFatJetPt_pt', 'LeadingMuonPt_pt', 'LeadingElectronPt_pt', 'nJet', 'nJetBtag', 'nFatJet', 'MinDelta_phi', 'MaxEta_jet', 'HT_eventHT', 'PV_npvsGood']\n",
      "Datasets to process :  ['TT_2018']\n",
      "Dataset : TT_hadr_2018\n",
      "# of files to process :  826\n",
      "files strings :\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TTToHadronic_TuneCP5_13TeV-powheg-pythia8/TT_hadr_2018/231222_110836/0000/tree_hadd_755.root\n",
      "\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TTToHadronic_TuneCP5_13TeV-powheg-pythia8/TT_hadr_2018/231222_110836/0000/tree_hadd_378.root\n",
      "\n",
      "  ... \n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TTToHadronic_TuneCP5_13TeV-powheg-pythia8/TT_hadr_2018/231222_110836/0000/tree_hadd_135.root\n",
      "\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TTToHadronic_TuneCP5_13TeV-powheg-pythia8/TT_hadr_2018/231222_110836/0000/tree_hadd_182.root\n",
      "\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  339180000.0\n",
      "Dataset : TT_semilep_2018\n",
      "# of files to process :  1029\n",
      "files strings :\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/TT_semilep_2018/231222_110917/0000/tree_hadd_755.root\n",
      "\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/TT_semilep_2018/231222_110917/0000/tree_hadd_378.root\n",
      "\n",
      "  ... \n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/TT_semilep_2018/231222_110917/0001/tree_hadd_1021.root\n",
      "\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/TT_semilep_2018/231222_110917/0001/tree_hadd_1009.root\n",
      "\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  447918000.0\n",
      "Dataset : TT_Mtt1000toInf_2018\n",
      "# of files to process :  71\n",
      "files strings :\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TT_Mtt-1000toInf_TuneCP5_13TeV-powheg-pythia8/TT_Mtt1000toInf_2018/231222_110958/0000/tree_hadd_23.root\n",
      "\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TT_Mtt-1000toInf_TuneCP5_13TeV-powheg-pythia8/TT_Mtt1000toInf_2018/231222_110958/0000/tree_hadd_50.root\n",
      "\n",
      "  ... \n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TT_Mtt-1000toInf_TuneCP5_13TeV-powheg-pythia8/TT_Mtt1000toInf_2018/231222_110958/0000/tree_hadd_71.root\n",
      "\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TT_Mtt-1000toInf_TuneCP5_13TeV-powheg-pythia8/TT_Mtt1000toInf_2018/231222_110958/0000/tree_hadd_5.root\n",
      "\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  22800395.0\n",
      "Dataset : TT_Mtt700to1000_2018\n",
      "# of files to process :  76\n",
      "files strings :\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TT_Mtt-700to1000_TuneCP5_13TeV-powheg-pythia8/TT_Mtt700to1000_2018/231222_111039/0000/tree_hadd_23.root\n",
      "\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TT_Mtt-700to1000_TuneCP5_13TeV-powheg-pythia8/TT_Mtt700to1000_2018/231222_111039/0000/tree_hadd_50.root\n",
      "\n",
      "  ... \n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TT_Mtt-700to1000_TuneCP5_13TeV-powheg-pythia8/TT_Mtt700to1000_2018/231222_111039/0000/tree_hadd_71.root\n",
      "\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/TT_Mtt-700to1000_TuneCP5_13TeV-powheg-pythia8/TT_Mtt700to1000_2018/231222_111039/0000/tree_hadd_5.root\n",
      "\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  28822033.0\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "cut = requirements # ---> see variables.py\n",
    "\n",
    "regions_def = regions # ---> see variables.py\n",
    "print(\"Regions to book: \")\n",
    "for r in regions_def.keys():\n",
    "    print(\"  \"+r)\n",
    "    \n",
    "sample_file = open(\"utils/dict_samples.json\", \"rb\")\n",
    "samples = json.load(sample_file)\n",
    "sample_file.close()\n",
    "\n",
    "var = vars  # ---> variables.py\n",
    "var2d = vars2D \n",
    "\n",
    "print(\"Variables for histograms :\")\n",
    "print([v._name for v in var])\n",
    "\n",
    "datasets = []\n",
    "for in_d in in_dataset:\n",
    "    if not in_d in sample_dict.keys():\n",
    "        print(\"Check the in_dataset string... \", sample_dict.keys())\n",
    "    else : \n",
    "        datasets.append(sample_dict[in_d])\n",
    "print(\"Datasets to process : \", [d.label for d in datasets])\n",
    "\n",
    "\n",
    "chain = {}\n",
    "ntot_events = {}\n",
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        samples_list = d.components\n",
    "    else:\n",
    "        samples_list = [d]\n",
    "    chain[d.label] = {}\n",
    "    ntot_events[d.label] = {}\n",
    "    for s in samples_list:\n",
    "        if distributed: \n",
    "            nfiles = len(samples[d.label][s.label]['strings'])\n",
    "            for i, string in enumerate(samples[d.label][s.label]['strings']): \n",
    "                samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"root://stormgf2.pi.infn.it/\")\n",
    "            chain[d.label][s.label] = samples[d.label][s.label]['strings']\n",
    "        else: \n",
    "            nfiles = nfiles_max\n",
    "            chain[d.label][s.label] = samples[d.label][s.label]['strings'][:nfiles]\n",
    "        ntot_events[d.label][s.label] = np.sum(samples[d.label][s.label]['ntot'][:nfiles])\n",
    "        print(\"Dataset : \"+s.label)\n",
    "        print(\"# of files to process : \", nfiles)\n",
    "        if distributed:\n",
    "            print(\"files strings :\\n  {}\\n  {}\\n  ... \\n  {}\\n  {}\".format(chain[d.label][s.label][0], chain[d.label][s.label][1], chain[d.label][s.label][-2], chain[d.label][s.label][-1]))\n",
    "        else :\n",
    "            print(\"files strings :\\n  {}\".format(chain[d.label][s.label][0]))\n",
    "        print(\"# of total events in the files to process (MC only, if Data the number is None) : \", ntot_events[d.label][s.label])"
=======
    "#output folder ---> waiting to implement davix lib\n",
    "folder = \"./results/run2018_EXO22014_v2/\"# \"./results/run2018_benchmark_deb/\" #\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "repohisto = folder+\"plots/\"\n",
    "if not os.path.exists(repohisto):\n",
    "    os.mkdir(repohisto)\n",
    "reposnap = folder+\"snap/\"\n",
    "if not os.path.exists(reposnap):\n",
    "    os.mkdir(reposnap)\n"
>>>>>>> 399b32f64ec473bbad4af6ac469218ccd9f0958d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54586805-5e2c-4b36-a13b-11497d5f2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invLepveto = \"\"#\"!\" # per non invertirlo basta passare stinga vuota\n",
    "# met_cut = 250\n",
    "# mdphi_cut = 0\n",
    "# HLT_filter = \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight || HLT_Ele32_WPTight_Gsf || HLT_Ele115_CaloIdVT_GsfTrkIdT || HLT_Photon200 || HLT_IsoMu24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a9be5b-2e52-431b-9709-ee1c3e639b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "end of initialization\n"
     ]
    }
   ],
   "source": [
    "# initialization of clusters\n",
    "\n",
    "# upload the proxyfile to the Dask workers to make them able to access data on the grid \n",
    "\n",
    "from distributed.diagnostics.plugin import UploadFile\n",
    "def set_proxy(dask_worker):\n",
    "    import os\n",
    "    import shutil\n",
    "    working_dir = dask_worker.local_directory\n",
    "    proxy_name = 'x509up_u0'\n",
    "    os.environ['X509_USER_PROXY'] = working_dir + '/' + proxy_name\n",
    "    os.environ['X509_CERT_DIR']=\"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "    shutil.copyfile(working_dir + '/' + proxy_name, working_dir + '/../../../proxy')    \n",
    "    os.environ['EXTRA_CLING_ARGS'] = \"-O2\"\n",
    "    return os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\")\n",
    "\n",
    "text_file = open(\"utils/postselection.h\", \"r\")\n",
    "data = text_file.read()\n",
    "def my_initialization_function():\n",
    "    print(ROOT.gInterpreter.ProcessLine(\".O\"))\n",
    "    ROOT.gInterpreter.Declare('{}'.format(data))\n",
    "    print(\"end of initialization\")\n",
    "\n",
    "# set up everything properly\n",
    "if distributed == True:\n",
    "    RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame\n",
    "    client = Client(address=\"tcp://127.0.0.1:\"+str(sched_port))\n",
    "    client.restart()\n",
    "    client.register_worker_plugin(UploadFile(\"/tmp/x509up_u0\"))\n",
    "    client.run(set_proxy)\n",
    "    ROOT.RDF.Experimental.Distributed.initialize(my_initialization_function)\n",
    "else:\n",
    "    RDataFrame = ROOT.RDataFrame\n",
    "    my_initialization_function()"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 8,
=======
   "cell_type": "markdown",
   "id": "41e40aec",
   "metadata": {},
   "source": [
    "Regions definitions for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3b1a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NoCut': '', 'HEMVeto_HLTmet': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight)', 'HEMVeto_HLTmu': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (HLT_IsoMu24)', 'HEMVeto_HLT': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && ((HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight) && (HLT_IsoMu24))', 'HEMVeto_HLTmet_METfilt': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (Flag_goodVertices && Flag_globalSuperTightHalo2016Filter && Flag_HBHENoiseFilter && Flag_HBHENoiseIsoFilter && Flag_EcalDeadCellTriggerPrimitiveFilter && Flag_BadPFMuonFilter && Flag_ecalBadCalibFilter && Flag_eeBadScFilter) && (HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight)', 'HEMVeto_HLTmu_METfilt': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (Flag_goodVertices && Flag_globalSuperTightHalo2016Filter && Flag_HBHENoiseFilter && Flag_HBHENoiseIsoFilter && Flag_EcalDeadCellTriggerPrimitiveFilter && Flag_BadPFMuonFilter && Flag_ecalBadCalibFilter && Flag_eeBadScFilter) && (HLT_IsoMu24)', 'HEMVeto_HLT_METfilt': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (Flag_goodVertices && Flag_globalSuperTightHalo2016Filter && Flag_HBHENoiseFilter && Flag_HBHENoiseIsoFilter && Flag_EcalDeadCellTriggerPrimitiveFilter && Flag_BadPFMuonFilter && Flag_ecalBadCalibFilter && Flag_eeBadScFilter) && ((HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight) && (HLT_IsoMu24))', 'HEMVeto_HLTmet_METcut': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight) && (MET_pt>250)', 'HEMVeto_HLTmu_METcut': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (HLT_IsoMu24) && (MET_pt>250)', 'HEMVeto_HLT_METcut': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && ((HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight) && (HLT_IsoMu24)) && (MET_pt>250)', 'AH_HLT': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && ((HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight) && (HLT_IsoMu24)) && (MET_pt>250) && (nVetoMuon+nVetoElectron) == 0 && nJetBtag > 0 && nGoodJet>3', 'AH1lWRmu_HLT': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && ((HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight) && (HLT_IsoMu24)) && (MET_pt>250) && (nTightElectron == 0 && nVetoElectron == 0 && nTightMuon == 1 && nVetoMuon == 1) && nGoodJet>=3 && MT<=140 && nJetBtag == 0', 'SL_HLT': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && ((HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight) && (HLT_IsoMu24)) && (MET_pt>250) && ((nTightElectron == 1 && nVetoElectron == 1 && nTightMuon == 0 && nVetoMuon == 0)||(nTightElectron == 0 && nVetoElectron == 0 && nTightMuon == 1 && nVetoMuon == 1)) && nJetBtag > 0', 'AH_HLTmet': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight) && (MET_pt>250) && (nVetoMuon+nVetoElectron) == 0 && nJetBtag > 0 && nGoodJet>3', 'AH1lWRmu_HLTmet': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight) && (MET_pt>250) && (nTightElectron == 0 && nVetoElectron == 0 && nTightMuon == 1 && nVetoMuon == 1) && nGoodJet>=3 && MT<=140 && nJetBtag == 0', 'SL_HLTmet': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight) && (MET_pt>250) && ((nTightElectron == 1 && nVetoElectron == 1 && nTightMuon == 0 && nVetoMuon == 0)||(nTightElectron == 0 && nVetoElectron == 0 && nTightMuon == 1 && nVetoMuon == 1)) && nJetBtag > 0', 'AH_HLTmu': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (HLT_IsoMu24) && (MET_pt>250) && (nVetoMuon+nVetoElectron) == 0 && nJetBtag > 0 && nGoodJet>3', 'AH1lWRmu_HLTmu': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (HLT_IsoMu24) && (MET_pt>250) && (nTightElectron == 0 && nVetoElectron == 0 && nTightMuon == 1 && nVetoMuon == 1) && nGoodJet>=3 && MT<=140 && nJetBtag == 0', 'SL_HLTmu': '(isMC || (year != 2018) || (HEMVeto || run<319077.)) && (HLT_IsoMu24) && (MET_pt>250) && ((nTightElectron == 1 && nVetoElectron == 1 && nTightMuon == 0 && nVetoMuon == 0)||(nTightElectron == 0 && nVetoElectron == 0 && nTightMuon == 1 && nVetoMuon == 1)) && nJetBtag > 0'}\n"
     ]
    }
   ],
   "source": [
    "regions_def = regions # ---> vedi variables.py\n",
    "# {\n",
    "#     \"all_regions\" : \"\"\n",
    "#     # \"resolved_1fwjet\": \"EventTopCategory==3 && nForwardJet>0\", \n",
    "#     # \"mixed_1fwjet\": \"EventTopCategory==2 && nForwardJet>0\", \n",
    "#     # \"merged_1fwjet\": \"EventTopCategory==1 && nForwardJet>0\",\n",
    "#     # \"resolved_0fwjet\": \"EventTopCategory==3 && nForwardJet==0\", \n",
    "#     # \"mixed_0fwjet\": \"EventTopCategory==2 && nForwardJet==0\", \n",
    "#     # \"merged_0fwjet\" : \"EventTopCategory==1 && nForwardJet==0\",\n",
    "#     # \"noTopRegion\" : \"EventTopCategory==0\"\n",
    "# }\n",
    "print(regions_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e9de3f-0903-4934-9090-15e12c806b20",
   "metadata": {},
   "source": [
    "Importing dict samples with the info for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2fbdbd1-d1d8-4ddf-9d0d-10d6a7b7d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = open(\"utils/dict_samples.json\", \"rb\")\n",
    "samples = json.load(sample_file)\n",
    "sample_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29546c32",
   "metadata": {},
   "source": [
    "INPUT DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06816863-d201-4852-b127-6bd885b566f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DataHTA_2018']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d23c148-5df3-45ff-ba4b-1ac967f420de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHTA_2018\n"
     ]
    }
   ],
   "source": [
    "if not in_dataset[0] in sample_dict.keys():\n",
    "    datasets = []\n",
    "    print(\"Check the in_dataset string... \", sample_dict.keys())\n",
    "else : \n",
    "    datasets= []\n",
    "    for d in in_dataset:\n",
    "        datasets.append(sample_dict[d])\n",
    "        print(datasets[-1].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "682c7555-c07d-4899-b7d1-12cef3783084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['QCD_2018', 'QCDHT_100to200_2018', 'QCDHT_200to300_2018', 'QCDHT_300to500_2018', 'QCDHT_500to700_2018', 'QCDHT_700to1000_2018', 'QCDHT_1000to1500_2018', 'QCDHT_1500to2000_2018', 'QCDHT_2000toInf_2018', 'ZJetsToNuNu_2018', 'ZJetsToNuNu_HT100to200_2018', 'ZJetsToNuNu_HT200to400_2018', 'ZJetsToNuNu_HT400to600_2018', 'ZJetsToNuNu_HT600to800_2018', 'ZJetsToNuNu_HT800to1200_2018', 'ZJetsToNuNu_HT1200to2500_2018', 'ZJetsToNuNu_HT2500toInf_2018', 'TT_2018', 'TT_hadr_2018', 'TT_semilep_2018', 'TT_Mtt1000toInf_2018', 'TT_Mtt700to1000_2018', 'WJets_2018', 'WJetsHT100to200_2018', 'WJetsHT200to400_2018', 'WJetsHT400to600_2018', 'WJetsHT600to800_2018', 'WJetsHT800to1200_2018', 'WJetsHT1200to2500_2018', 'WJetsHT2500toInf_2018', 'TprimeToTZ_700_2018', 'TprimeToTZ_1000_2018', 'TprimeToTZ_1800_2018', 'DataHT_2018', 'DataHTA_2018', 'DataHTB_2018', 'DataHTC_2018', 'DataHT_2022', 'DataHTC_2022', 'DataHTD_2022', 'DataHTE_2022', 'DataHTF_2022', 'DataHTG_2022', 'DataHTD_2018', 'DataHTH_2016', 'DataMETA_2018', 'DataSingleMuA_2018', 'DataSingleMuB_2018', 'DataSingleMuC_2018'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f503d",
   "metadata": {},
   "source": [
    "Variables to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "616688c8-eb99-452d-b66c-cd938fcc2877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MET_pt', 'MET_phi', 'PuppiMET_pt', 'LeadingJetPt_pt', 'LeadingFatJetPt_pt', 'LeadingMuonPt_pt', 'LeadingElectronPt_pt', 'nJet', 'nJetBtag', 'nFatJet', 'MinDelta_phi', 'MaxEta_jet', 'HT_eventHT', 'PV_npvsGood']\n"
     ]
    }
   ],
   "source": [
    "#Defining variables to plot\n",
    "\n",
    "var = vars  # ---> vedi variables.py\n",
    "\n",
    "# var.append(variable(name = \"MET_pt\", title= \"MET p_{T} [GeV]\", taglio = cut, nbins = 20, xmin = 0, xmax=1000))\n",
    "# var.append(variable(name = \"MET_phi\", title= \"MET #phi\", taglio = cut, nbins = 6, xmin = -math.pi, xmax=math.pi))\n",
    "# var.append(variable(name = \"LeadingJet_pt\", title= \"Leading Jet p_{T} [GeV]\", taglio = cut, nbins = 30, xmin = 50, xmax=950))\n",
    "# var.append(variable(name = \"nTopHighPt\", title= \"# Top Candidate Mix\", taglio = cut, nbins = 80, xmin = -0.5, xmax=80.5))\n",
    "# var.append(variable(name = \"nTopLowPt\", title= \"# Top Candidate Resolved\", taglio = cut, nbins = 50, xmin = -0.5, xmax=50.5))\n",
    "# var.append(variable(name = \"nJet\", title= \"# Jet\", taglio = cut, nbins = 25, xmin = -0.5, xmax=25.5))\n",
    "# var.append(variable(name = \"nFatJet\", title= \"# FatJet\", taglio = cut, nbins = 25, xmin = -0.5, xmax=25.5))\n",
    "# var.append(variable(name = \"MinDelta_phi\", title= \"min #Delta #phi\", taglio = cut, nbins = 20, xmin = 0, xmax = 4))\n",
    "# var.append(variable(name = \"MaxEta_jet\", title= \"max #eta jet\", taglio = cut, nbins = 24, xmin = 0, xmax = 6))\n",
    "\n",
    "print([v._name for v in var])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
>>>>>>> 399b32f64ec473bbad4af6ac469218ccd9f0958d
   "id": "963af90a-de0c-44a7-9b64-3927c34eeffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### utils ###################\n",
    "def cut_string(cut):\n",
    "    return cut.replace(\" \", \"\").replace(\"&&\",\"_\").replace(\">\",\"_g_\").replace(\".\",\"_\").replace(\"==\",\"_e_\")\n",
    "\n",
    "################### preselection ###############\n",
    "def preselection(df):\n",
    "    df = df.Define(\"GoodJet_idx\", \"GetGoodJet(Jet_pt, Jet_eta, Jet_jetId)\")\n",
    "    df = df.Define(\"nGoodJet\", \"nGoodJet(GoodJet_idx)\")\n",
    "    \n",
    "    # if 'leptonveto' in cut:\n",
    "    #     df = df.Filter(invLepveto+\"LepVeto(Electron_pt, Electron_eta, Electron_cutBased, Muon_pt, Muon_eta, Muon_looseId )\", \"Lepton Veto\")\n",
    "    #     if \"&& leptonveto\" in cut:\n",
    "    #         c_ = cut.replace(\"&& leptonveto\",\"\")\n",
    "    #     elif \"leptonveto &&\" in cut:\n",
    "    #         c_ = cut.replace(\"leptonveto &&\",\"\")\n",
    "    #     elif \"leptonveto\" in cut:\n",
    "    #         c_ = cut.replace(\"leptonveto\",\"\")    \n",
    "    # else: \n",
    "    #     df = df\n",
    "    #     c_ = cut\n",
    "    df = df.Define(\"nTightElectron\", \"nTightElectron(Electron_pt, Electron_eta, Electron_cutBased)\")\n",
    "    df = df.Define(\"TightElectron_idx\", \"TightElectron_idx(Electron_pt, Electron_eta, Electron_cutBased)\")\n",
    "    df = df.Define(\"nVetoElectron\", \"nVetoElectron(Electron_pt, Electron_cutBased)\")\n",
    "    df = df.Define(\"nTightMuon\", \"nTightMuon(Muon_pt, Muon_eta, Muon_tightId)\")\n",
    "    df = df.Define(\"TightMuon_idx\", \"TightMuon_idx(Muon_pt, Muon_eta, Muon_tightId)\")\n",
    "    df = df.Define(\"nVetoMuon\", \"nVetoMuon(Muon_pt, Muon_eta, Muon_looseId)\")\n",
    "    df = df.Define(\"Lepton_flavour\", \"Lepton_flavour(nTightElectron, nTightMuon)\").Define(\"Lep_pt\", \"Lepton_var(Lepton_flavour, Electron_pt, TightElectron_idx, Muon_pt, TightMuon_idx)\").Define(\"Lep_phi\", \"Lepton_var(Lepton_flavour, Electron_phi, TightElectron_idx, Muon_phi, TightMuon_idx)\")\n",
    "    df = df.Define(\"MT\", \"sqrt(2 * Lep_pt * MET_pt * (1 - cos(Lep_phi - MET_phi)))\")\n",
    "    \n",
    "    # df = df.Filter(\"MET_pt>\"+ str(met_cut), \"MET_pt>\"+ str(met_cut))\n",
    "    # df = df.Filter(\"MinDelta_phi>\"+ str(mdphi_cut), \"MinDeltaPhi>\"+ str(mdphi_cut))\n",
    "    \n",
    "    # df = df.Filter(\"atLeast1jet_setparams(Jet_pt, Jet_eta, Jet_mass, Jet_jetId, 30, 4, 0, 1)\", \"At_least_1Ak4\")\n",
    "    # df = df.Filter(\"atLeast1fatjet_setparams(FatJet_pt, FatJet_msoftdrop, FatJet_eta, FatJet_jetId, 200, 6, 40, 1)\", \"at_least_1Ak8\")\n",
    "\n",
    "    df = df.Define(\"LeadingJetPt_idx\", \"GetLeadingPtJet(Jet_pt)\")\n",
    "    df = df.Define(\"LeadingJetPt_pt\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_pt)\")\n",
    "    df = df.Define(\"LeadingJetPt_eta\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_eta)\")\n",
    "    df = df.Define(\"LeadingJetPt_phi\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_phi)\")\n",
    "    df = df.Define(\"LeadingJetPt_mass\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_mass)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_idx\", \"GetLeadingPtJet(FatJet_pt)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_pt\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_pt)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_eta\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_eta)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_phi\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_phi)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_mass\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_mass)\")\n",
    "    df = df.Define(\"LeadingMuonPt_idx\", \"GetLeadingPtLep(Muon_pt, Muon_eta, Muon_looseId)\")\n",
    "    df = df.Define(\"LeadingMuonPt_pt\", \"GetLeadingJetVar(LeadingMuonPt_idx, Muon_pt)\")\n",
    "    df = df.Define(\"LeadingMuonPt_eta\", \"GetLeadingJetVar(LeadingMuonPt_idx, Muon_eta)\")\n",
    "    df = df.Define(\"LeadingMuonPt_phi\", \"GetLeadingJetVar(LeadingMuonPt_idx, Muon_phi)\")\n",
    "    df = df.Define(\"LeadingElectronPt_idx\", \"GetLeadingPtLep(Electron_pt, Electron_eta, Electron_cutBased)\")\n",
    "    df = df.Define(\"LeadingElectronPt_pt\", \"GetLeadingJetVar(LeadingElectronPt_idx, Electron_pt)\")\n",
    "    df = df.Define(\"LeadingElectronPt_eta\", \"GetLeadingJetVar(LeadingElectronPt_idx, Electron_eta)\")\n",
    "    df = df.Define(\"LeadingElectronPt_phi\", \"GetLeadingJetVar(LeadingElectronPt_idx, Electron_phi)\")\n",
    "    \n",
    "    df = df.Define(\"nForwardJet\", \"nForwardJet(GoodJet_idx, Jet_eta)\")\n",
    "    df = df.Define(\"nJetBtag\", \"njetbtag(GoodJet_idx, Jet_btagDeepFlavB)\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "############### trigger selection #####################\n",
    "def trigger_filter(df):\n",
    "    df_trig = df.Filter(\"HLT_PFMET120_PFMHT120_IDTight || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight\", \"trigger\")\n",
    "    return df_trig\n",
    "\n",
    "############### top selection ########################\n",
    "def select_top(df):\n",
    "    df_goodtopMer = df.Define(\"GoodTopMer_idx\", \"select_TopMer(FatJet_deepTag_TvsQCD, FatJet_pt, FatJet_eta, FatJet_phi)\")\n",
    "    # ritorna gli indici dei FatJet che superano la trs del Top Merged (no overlap)\n",
    "    df_goodtopMix = df_goodtopMer.Define(\"GoodTopMix_idx\", \"select_TopMix(TopHighPt_score2, TopHighPt_pt, TopHighPt_eta, TopHighPt_phi)\")\n",
    "    # ritorna gli indici dei FatJet che superano la trs del Top Merged (no overlap)\n",
    "    df_goodtopRes = df_goodtopMix.Define(\"GoodTopRes_idx\", \"select_TopRes(TopLowPt_scoreDNN, TopLowPt_pt, TopLowPt_eta, TopLowPt_phi)\")\n",
    "    # ritorna gli indici dei Fatche superano la trs del Top Merged (no overlap)\n",
    "    df_topcategory = df_goodtopRes.Define(\"EventTopCategory\", \"select_TopCategory(GoodTopMer_idx, GoodTopMix_idx, GoodTopRes_idx)\")\n",
    "    # return:  0- no top sel, 1- top merged, 2- top mix, 3- top resolved\n",
    "    df_topselected = df_topcategory.Define(\"Top_idx\",\n",
    "                                           \"select_bestTop(EventTopCategory, GoodTopMer_idx, GoodTopMix_idx, GoodTopRes_idx, FatJet_deepTag_TvsQCD, TopHighPt_score2, TopLowPt_scoreDNN)\")\n",
    "    # return best top idx wrt category --> the idx is referred to the list of candidates fixed by the EventTopCategory\n",
    "    df_topvariables = df_topselected.Define(\"Top_pt\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_pt, TopHighPt_pt, TopLowPt_pt)\")\\\n",
    "                        .Define(\"Top_eta\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_eta, TopHighPt_eta, TopLowPt_eta)\")\\\n",
    "                        .Define(\"Top_phi\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_phi, TopHighPt_phi, TopLowPt_phi)\")\\\n",
    "                        .Define(\"Top_mass\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_mass, TopHighPt_mass, TopLowPt_mass)\")\\\n",
    "                        .Define(\"Top_score\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_deepTag_TvsQCD, TopHighPt_score2, TopLowPt_scoreDNN)\")\n",
    "\n",
    "    return df_topvariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69f0bf61-b2ab-422e-8baf-9abba2b984cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookhisto(df, regions_def, var, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var:\n",
    "            if regions_def[reg] == \"\":\n",
<<<<<<< HEAD
    "                h_[reg][v._name]= df.Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                # h_[reg][v._name]= df.Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "            else:\n",
    "                h_[reg][v._name]= df.Filter(regions_def[reg]).Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                # h_[reg][v._name]= df.Filter(regions_def[reg]).Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)            \n",
    "    return h_\n",
    "\n",
    "def bookhisto2D(df, regions_def, var2d, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var2d:\n",
    "            if regions_def[reg]==\"\":\n",
    "                h_[reg][v._name] = df.Redefine(v._xname, \"UnOvBin(\"+v._xname+\",\"+str(v._nxbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\")\\\n",
    "                                     .Redefine(v._yname, \"UnOvBin(\"+v._yname+\",\"+str(v._nybins)+\",\"+str(v._ymin)+\",\"+str(v._ymax)+\")\")\\\n",
    "                                     .Histo2D((v._xname+\"Vs\"+v._yname+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax), v._xname, v._yname)\n",
    "            else:\n",
    "                h_[reg][v._name] = df.Filter(regions_def[reg])\\\n",
    "                                     .Redefine(v._xname, \"UnOvBin(\"+v._xname+\",\"+str(v._nxbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\")\\\n",
    "                                     .Redefine(v._yname, \"UnOvBin(\"+v._yname+\",\"+str(v._nybins)+\",\"+str(v._ymin)+\",\"+str(v._ymax)+\")\")\\\n",
    "                                     .Histo2D((v._xname+\"Vs\"+v._yname+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax), v._xname, v._yname)\n",
=======
    "                # h_[reg][v._name]= df.Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                h_[reg][v._name]= df.Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "            else:\n",
    "                # h_[reg][v._name]= df.Filter(regions_def[reg]).Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                h_[reg][v._name]= df.Filter(regions_def[reg]).Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)            \n",
>>>>>>> 399b32f64ec473bbad4af6ac469218ccd9f0958d
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37967949-f502-4c77-9ceb-dd04f8062cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def savehisto(d, h, regions_def, var, s_cut):\n",
    "    histo = {reg: {v._name: ROOT.TH1D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax) for v in var} for reg in regions_def.keys()}\n",
    "    \n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    \n",
    "    for s in s_list:\n",
    "        outfile = ROOT.TFile.Open(repohisto+s.label+'.root', \"RECREATE\")\n",
    "        for reg in regions_def.keys():\n",
    "            for v in var:\n",
    "                histo[reg][v._name] = h[d.label][s.label][reg][v._name].GetValue()      \n",
    "                outfile.cd()\n",
    "                histo[reg][v._name].Write()\n",
<<<<<<< HEAD
    "        outfile.Close()\n",
    "\n",
    "# i plot2d per il momento non ci servono, si deve trovare un modo più intelligente di farli\n",
    "def savehisto2d(d, h, regions_def, var2d, s_cut):\n",
    "    histo = {reg: {v._name: ROOT.TH2D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax,) for v in var2d} for reg in regions_def.keys()}\n",
    "        \n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    \n",
    "    for s in s_list:\n",
    "        outfile = ROOT.TFile.Open(repohisto+s.label+'_2D.root', \"RECREATE\")\n",
    "        for reg in regions_def.keys():\n",
    "            for v in histo[reg].keys():\n",
    "                histo[reg][v] = h[d.label][s.label][reg][v].GetValue()      \n",
    "                outfile.cd()\n",
    "                histo[reg][v].Write()\n",
=======
>>>>>>> 399b32f64ec473bbad4af6ac469218ccd9f0958d
    "        outfile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c96d28-45f4-4877-8d00-680e095a3016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#samples['DataHTF_2022']['DataHTF_2022']['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab036200-65f9-44a0-abbb-8a04003fc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples[d.label][d.components[0]]['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b72ddfd-d641-437d-a78c-8568a0a17a30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "starting loop on datasets:  ['TT_2018']\n",
      "Local time : 2024-01-04 17:29:32.432929\n",
      "Initializing DataFrame for TT_hadr_2018 chain len =  826\n",
      "Initializing DataFrame for TT_semilep_2018 chain len =  1029\n",
      "Initializing DataFrame for TT_Mtt1000toInf_2018 chain len =  71\n",
      "Initializing DataFrame for TT_Mtt700to1000_2018 chain len =  76\n",
      "All histos booked !\n"
=======
      "starting loop on datasets:  ['DataHTA_2018']\n",
      "Local time : 2023-11-30 10:51:19.081165\n",
      "Initializing DataFrame for DataHTA_2018 chain len =  1\n",
      "['root://cms-xrd-global.cern.ch//store/user/acagnott/DM_Run3_v0/MET/DataHTA_2018/231107_135602/0000/tree_hadd_317.root']\n",
      "All histos booked !\n",
      "DataHTA_2018 finished!\n",
      "Job finished in:  0:18:04.768580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TClass::Init>: no dictionary for class edm::Hash<1> is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ParameterSetBlob is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessHistory is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessConfiguration is available\n",
      "Warning in <TClass::Init>: no dictionary for class pair<edm::Hash<1>,edm::ParameterSetBlob> is available\n"
>>>>>>> 399b32f64ec473bbad4af6ac469218ccd9f0958d
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "print(\"starting loop on datasets: \",[d.label for d in datasets])\n",
    "print(\"Local time :\", t0)\n",
    "# print(\"requirements: \"+cut)\n",
<<<<<<< HEAD
    "\n",
    "h = {}\n",
    "h_2D = {}\n",
    "\n",
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
=======
    "\n",
    "d = datasets[0]\n",
    "s = datasets[0]\n",
    "\n",
    "h = {}\n",
    "\n",
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "\n",
    "    if 'Data' in s.label : sampleflag = 0\n",
    "    else: sampleflag = 1\n",
    "    c_ = cut\n",
    "    h[d.label] = {}\n",
    "    for s in s_list:\n",
    "        if nfiles_max > len(samples[d.label][s.label]['strings']): \n",
    "            nfiles = len(samples[d.label][s.label]['strings'])\n",
    "            for i, string in enumerate(samples[d.label][s.label]['strings']): \n",
    "                samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"root://stormgf2.pi.infn.it/\")\n",
    "            chain = samples[d.label][s.label]['strings']\n",
    "        else: \n",
    "            nfiles = nfiles_max\n",
    "            # for i, string in enumerate(samples[d.label][s.label]['strings']): \n",
    "            #     samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"root://stormgf2.pi.infn.it/\")\n",
    "            chain = samples[d.label][s.label]['strings'][:nfiles]\n",
    "            \n",
    "        print(\"Initializing DataFrame for \"+ s.label +\" chain len = \", len(chain))\n",
    "        if len(chain)==1: print(chain)\n",
    "        if distributed ==True:\n",
    "            df = RDataFrame(\"Events\", chain, npartitions=nmaxpartition, \n",
    "                            daskclient=client, monitor_label = \"main\" )\n",
    "        else:\n",
    "            df = RDataFrame(\"Events\", chain)\n",
    "\n",
    "        df_ismc         = df.Define(\"isMC\", \"isMC(\"+str(sampleflag)+\")\")\n",
    "        df_year         = df_ismc.Define(\"year\", str(s.year))\n",
    "        df_hemveto      = df_year.Define(\"HEMVeto\", \"hemveto(Jet_eta, Jet_phi, Electron_eta, Electron_phi)\")\n",
    "        df_wnom         = df_hemveto.Define('w_nominal', '1')\n",
    "        df_presel       = preselection(df_wnom)\n",
    "        \n",
    "#         opts = ROOT.RDF.RSnapshotOptions()\n",
    "#         opts.fLazy = True\n",
    "#         snapshot_df = df_presel.Snapshot(\"tree\", reposnap+s.label+\".root\", {\"isMC\", \"year\", \"HEMVeto\", \"run\", \"Flag_goodVertices\", \"Flag_globalSuperTightHalo2016Filter\", \"Flag_HBHENoiseFilter\", \"Flag_HBHENoiseIsoFilter\", \"Flag_EcalDeadCellTriggerPrimitiveFilter\", \"Flag_BadPFMuonFilter\", \"Flag_ecalBadCalibFilter\", \"Flag_eeBadScFilter\", \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60\", \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight\", \"HLT_Ele32_WPTight_Gsf\", \"HLT_Ele115_CaloIdVT_GsfTrkIdT\", \"HLT_Photon200\", \"HLT_IsoMu24\",\"HLT_PFMET120_PFMHT120_IDTight\", \"HLT_PFMETNoMu140_PFMHTNoMu140_IDTight\", \"HLT_PFMET140_PFMHT140_IDTight\", \"MET_pt\", \"nVetoMuon\", \"nVetoElectron\", \"nJetBtag\", \"nGoodJet\", \"nTightElectron\", \"nTightMuon\", \"MT\"}\n",
    "# , opts)\n",
    "    \n",
    "        # print(\"snapshot saved\")\n",
    "        s_cut = cut_string(cut)\n",
    "        h[d.label][s.label] = bookhisto(df_presel, regions_def, var, s_cut)\n",
>>>>>>> 399b32f64ec473bbad4af6ac469218ccd9f0958d
    "\n",
    "    if 'Data' in d.label : sampleflag = 0\n",
    "    else: sampleflag = 1\n",
    "    c_ = cut\n",
    "    h[d.label] = {}\n",
    "    h_2D[d.label] = {}\n",
    "    for s in s_list:\n",
    "        \n",
    "        print(\"Initializing DataFrame for \"+ s.label +\" chain len = \", len(chain[d.label][s.label]))\n",
    "        if len(chain[d.label][s.label])==1: print(chain[d.label][s.label])\n",
    "        if distributed ==True:\n",
    "            df = RDataFrame(\"Events\", chain[d.label][s.label], npartitions=nmaxpartition, \n",
    "                            daskclient=client, monitor_label = \"main\" )\n",
    "        else:\n",
    "            df = RDataFrame(\"Events\", chain[d.label][s.label])\n",
    "        \n",
    "        \n",
    "        df_ismc         = df.Define(\"isMC\", \"isMC(\"+str(sampleflag)+\")\")\n",
    "        df_year         = df_ismc.Define(\"year\", str(s.year))\n",
    "        df_hemveto      = df_year.Define(\"HEMVeto\", \"hemveto(Jet_eta, Jet_phi, Electron_eta, Electron_phi)\")\n",
    "        df_hemveto      = df_hemveto.Filter(\"(isMC || (year != 2018) || (HEMVeto || run<319077.))\")\n",
    "        df_hlt          = trigger_filter(df_hemveto)        \n",
    "        ### per gli HLT potremmo volerli modificare a seconda della regione (?) in tal caso la funzione\n",
    "        ### va spostata nel book histos e va passata la regione per attivare trigger diversi\n",
    "        df_wnom         = df_hlt.Define('w_nominal', '1')\n",
    "        df_presel       = preselection(df_wnom)\n",
    "        \n",
    "        if do_snapshot:\n",
    "            opts = ROOT.RDF.RSnapshotOptions()\n",
    "            opts.fLazy = True\n",
    "            if distributed: fold = \"./\"\n",
    "            else: fold = folder\n",
    "            snapshot_df = df_presel.Snapshot(\"events_nominal\", fold+\"snap_\"+s.label+\".root\", branches, opts)\n",
    "            # print(\"./\"+s.label+\".root\")\n",
    "        if do_histos:\n",
    "            s_cut = cut_string(cut)\n",
    "            if len(var) != 0 :\n",
    "                h[d.label][s.label] = bookhisto(df_presel, regions_def, var, s_cut)\n",
    "            if len(var2d) != 0 :\n",
    "                h_2D[d.label][s.label] = bookhisto2D(df_presel, regions_def, var2d, s_cut)\n",
    "# if not distributed:\n",
    "#     df_presel.Report().Print()\n",
    "\n",
<<<<<<< HEAD
    "if do_histos:\n",
    "    print(\"All histos booked !\")\n",
    "    for d in datasets:\n",
    "        if len(var):\n",
    "            savehisto(d, h, regions_def, var, s_cut)\n",
    "        if len(var2d) != 0 :\n",
    "            savehisto2d(d, h_2D, regions_def, var2d, s_cut)\n",
    "        print(d.label + \" histos saved\")\n",
    "if do_snapshot:\n",
    "    snapshot_df.GetValue()\n",
    "    if distributed: \n",
    "        client.run(transfer_to_tier)\n",
    "        print(\"Snapshots saved and trasfered to tier\")\n",
    "    print(\"Sanpshot done!\")\n",
=======
    "print(\"All histos booked !\")\n",
    "for d in datasets:\n",
    "    savehisto(d, h, regions_def, var, s_cut)\n",
    "    print(d.label + \" finished!\")\n",
>>>>>>> 399b32f64ec473bbad4af6ac469218ccd9f0958d
    "t1 = datetime.now()\n",
    "print(\"Job finished in: \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9573b747-9a5a-4501-b084-481b10d418ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# file = ROOT.TFile.Open(repohisto+s.label+\".root\")\n",
    "# for a in file.GetListOfKeys(): print(a)\n",
    "# for reg in regions_def.keys():\n",
    "#     hist = file.Get(var+\"_\"+reg+\"_\")\n",
    "#     print(reg, hist.Integral())"
=======
    "file = ROOT.TFile.Open(repohisto+s.label+\".root\")\n",
    "# for a in file.GetListOfKeys(): print(a)"
>>>>>>> 399b32f64ec473bbad4af6ac469218ccd9f0958d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3606a5b-e3ce-491d-a86a-6c02431ca435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['NoCut', 'HEMVeto_HLTmet', 'HEMVeto_HLTmu', 'HEMVeto_HLT', 'HEMVeto_HLTmet_METfilt', 'HEMVeto_HLTmu_METfilt', 'HEMVeto_HLT_METfilt', 'HEMVeto_HLTmet_METcut', 'HEMVeto_HLTmu_METcut', 'HEMVeto_HLT_METcut', 'AH_HLT', 'AH1lWRmu_HLT', 'SL_HLT', 'AH_HLTmet', 'AH1lWRmu_HLTmet', 'SL_HLTmet', 'AH_HLTmu', 'AH1lWRmu_HLTmu', 'SL_HLTmu'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regions_def.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "269963fc-638b-4a19-8c8d-af26bad6e37f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoCut 84797.0\n",
      "HEMVeto_HLTmet 82389.0\n",
      "HEMVeto_HLTmu 3860.0\n",
      "HEMVeto_HLT 3797.0\n",
      "HEMVeto_HLTmet_METfilt 80320.0\n",
      "HEMVeto_HLTmu_METfilt 3846.0\n",
      "HEMVeto_HLT_METfilt 3784.0\n",
      "HEMVeto_HLTmet_METcut 26140.0\n",
      "HEMVeto_HLTmu_METcut 1553.0\n",
      "HEMVeto_HLT_METcut 1531.0\n",
      "AH_HLT 1.0\n",
      "AH1lWRmu_HLT 539.0\n",
      "SL_HLT 336.0\n",
      "AH_HLTmet 1251.0\n",
      "AH1lWRmu_HLTmet 682.0\n",
      "SL_HLTmet 871.0\n",
      "AH_HLTmu 1.0\n",
      "AH1lWRmu_HLTmu 541.0\n",
      "SL_HLTmu 340.0\n"
     ]
    }
   ],
   "source": [
    "# region = \"NoCut\"\n",
    "# var = \"MET_pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa57df-4a5e-46d7-b758-84edcebca76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if do_snapshot and distributed:\n",
    "    print(os.popen(\"davix-ls davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{}/ -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name, remote_subfolder_name)).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "e3af25cc-cc36-437c-94e7-42aaa09cbfcd",
=======
   "id": "8daa57df-4a5e-46d7-b758-84edcebca76a",
>>>>>>> 399b32f64ec473bbad4af6ac469218ccd9f0958d
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity kernel",
   "language": "python",
   "name": "singularity-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
