{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4213965a-36f5-4afb-aa9a-f4168cbb483b",
   "metadata": {},
   "source": [
    "**Run 3 analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd081e3",
   "metadata": {},
   "source": [
    "**Framework version January 2024**\n",
    "- Log :\n",
    "    - understood bug in data (the problem was in CRAB not here)\n",
    "    - added Snapshot \n",
    "- Planned update :\n",
    "    - standalone code, prepare a couple of input parameters and make the code working with a single command\n",
    "    \n",
    "    \n",
    "__________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98655626-5252-41be-b111-76e638cecddf",
   "metadata": {},
   "source": [
    "**Code**\n",
    "\n",
    "Folder definition on Tier:\n",
    "- in the main folder */acagnott/* added folder 'remote_folder_name';\n",
    "- in \"Snapshots\" added the folder 'remote_subfolder_name';\n",
    "- in the subfolder through dask the snapshot will be copied with name \"snap_\"+label+\"_*.root\"\n",
    "\n",
    "\n",
    "Es: se lancio \"DataMETA_2018\", gli snapshot vengono salvati in ../acagnott/Snapshot/20231229/snap_DataMET_2018_*.root\n",
    "se viene lanciato \"QCD_2018\" viene creata la cartella /acagnott/Snapshot/20231229/snap_QCDHT_100to200_2018_*.root e così via per ogni components\n",
    "\n",
    "---> Viene usato solo il giorno in modo che tutti i sample lanciati lo stesso giorno verranno salvati nella stessa cartella con nomi diversi, visto che vengono lanciati in momenti diversi della giornata lo stesso tipo di job. Forse va modificato il formato se i singoli job iniziano a durare più di un giorno dato che viene comunque lanciato un sampel per volta (in tal caso verrebbero salvati in cartelle diverse. Si potrebbe pensare di mettere la data a mano, cioé invece di usare datetime.now() si potrebbe inserire la data manualmente per fare in modo di mettere tutti i file nella stessa folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf1ce47-e2f0-48f4-8e4b-b0fa9f1bacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "sched_port = 22996 #Dask port\n",
    "nmaxpartition = 30\n",
    "distributed = True#False#\n",
    "do_histos = True\n",
    "hist_folder = \"run2022_sel\"\n",
    "do_snapshot = False\n",
    "remote_subfolder_name = datetime.now().strftime(\"%Y%m%d\") #20231229\n",
    "\n",
    "in_dataset = [\n",
    "    # \"DataMETA_2018\", \n",
    "    # \"DataMETB_2018\", \n",
    "    # \"DataMETC_2018\",\n",
    "    # \"DataMETD_2018\",\n",
    "    # \"DataSingleMuA_2018\", \n",
    "    # \"DataSingleMuB_2018\", \n",
    "    # \"DataSingleMuC_2018\",\n",
    "    # \"DataSingleMuD_2018\",\n",
    "    \n",
    "    # \"TprimeToTZ_700_2018\",\n",
    "    # \"TprimeToTZ_1000_2018\", \n",
    "    # \"TprimeToTZ_1800_2018\",\n",
    "    \n",
    "    # \"QCD_2018\",\n",
    "    # \"TT_2018\",\n",
    "    # \"ZJetsToNuNu_2018\",\n",
    "    # \"WJets_2018\",\n",
    "    \n",
    "    # \"QCDHT_100to200_2018\", \"QCDHT_200to300_2018\",\"QCDHT_300to500_2018\", \"QCDHT_500to700_2018\", \"QCDHT_700to1000_2018\",\"QCDHT_1000to1500_2018\", \"QCDHT_1500to2000_2018\",\"QCDHT_2000toInf_2018\",\"TT_hadr_2018\", \"TT_semilep_2018\", \"TT_Mtt700to1000_2018\", \"TT_Mtt1000toInf_2018\",\"ZJetsToNuNu_HT100to200_2018\", \"ZJetsToNuNu_HT200to400_2018\", \"ZJetsToNuNu_HT400to600_2018\", \"ZJetsToNuNu_HT600to800_2018\", \"ZJetsToNuNu_HT800to1200_2018\", \"ZJetsToNuNu_HT1200to2500_2018\", \"ZJetsToNuNu_HT2500toInf_2018\"\"WJetsHT100to200_2018\", \"WJetsHT200to400_2018\", \"WJetsHT400to600_2018\", \"WJetsHT600to800_2018\", \"WJetsHT800to1200_2018\", \"WJetsHT1200to2500_2018\", \"WJetsHT2500toInf_2018\"\n",
    "    \n",
    "    # \"WJets_2022\",\n",
    "    # \"ZJetsToNuNu_2022\",\n",
    "    \n",
    "    # \"QCD_2022\",\n",
    "    # \"ZJetsToNuNu_2jets_2022\",\n",
    "    # \"TT_2022\",\n",
    "    # \"WJets_2jets_2022\"\n",
    "    # \"TprimeToTZ_700_2022\", \"TprimeToTZ_1000_2022\", \"TprimeToTZ_1800_2022\"\n",
    "    \"DataJetMET_2022\"\n",
    "\n",
    "]\n",
    "\n",
    "# \"event\", \"run\", \n",
    "branches = {\"HT_eventHT\", \"Top_mass\", \"Top_pt\", \"Top_score\", \"Top_isolationPtJetsdR04\", \"Top_isolationPtJetsdR06\", \"Top_isolationPtJetsdR08\", \"Top_isolationPtJetsdR12\", \"Top_isolationNJetsdR04\", \"Top_isolationNJetsdR06\", \"Top_isolationNJetsdR08\", \"Top_isolationNJetsdR12\",\n",
    "            \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60\", \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight\",  \n",
    "            \"MET_pt\", \"nVetoMuon\", \"nVetoElectron\", \"nJetBtag\", \n",
    "            \"nGoodJet\", \"nTightElectron\", \"nTightMuon\", \"MT\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e16aaf3-78d5-4a3c-851c-21f771dcf586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.27/01\n",
      "/tmp/x509up_u0 /cvmfs/grid.cern.ch/etc/grid-security/certificates/\n",
      "You are producing histograms\n",
      "local folder histos: ./results/run2022_sel/\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import os\n",
    "from utils.samples import *\n",
    "from utils.variables import *\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from dask.distributed import Client\n",
    "ROOT.RDF.Experimental.Distributed.open_files_locally = False\n",
    "\n",
    "os.environ['X509_CERT_DIR'] = \"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "os.environ['X509_USER_PROXY'] = \"/tmp/x509up_u0\"\n",
    "print(os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\"))\n",
    "\n",
    "\n",
    "if distributed:\n",
    "    nfiles_max = 1000\n",
    "else:\n",
    "    nfiles_max = 1  #######\n",
    "\n",
    "# Cosa aggiungere, modificare la cartella sul tier in questo modo ../acagnott/Snapshot_rdf/*dataset_name*/*data di processamente con orario*/\n",
    "# insomma come fa crab\n",
    "\n",
    "\n",
    "if do_histos: print(\"You are producing histograms\")\n",
    "if do_snapshot: print(\"You are producing snapshot\")\n",
    "\n",
    "remote_folder_name = \"Snapshots\"\n",
    "#output histos folder\n",
    "folder = \"./results/\"+hist_folder+\"/\"\n",
    "# eos_folder = \"/eos/home-a/acagnott/DarkMatter/nosynch/\"+hist_folder\n",
    "\n",
    "if do_snapshot and remote_subfolder_name == datetime.now().strftime(\"%Y%m%d\") and distributed: \n",
    "    print(\"You are naming the tier subfolder using the current day \\n\")\n",
    "    print(\"Snapshots folder name : ~/acagnott/{}/{}\".format(remote_folder_name, remote_subfolder_name))\n",
    "elif do_snapshot and distributed:\n",
    "    print(\"You are naming the tier subfolder manually\")\n",
    "    print(\"Snapshots folder name : ~/acagnott/{}/{}\".format(remote_folder_name, remote_subfolder_name))\n",
    "elif do_snapshot:\n",
    "    print(\"You are saving snapshots in local\")\n",
    "    print(\"folder name : \" + folder)\n",
    "\n",
    "\n",
    "if do_histos : \n",
    "    print(\"local folder histos: {}\".format(folder))\n",
    "    # print(\"At the end of the job the histos will be transfer to eos storage\")\n",
    "    # print(\"eos folder : {}\".format(eos_folder))\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "repohisto = folder+\"plots/\"\n",
    "if not os.path.exists(repohisto):\n",
    "    os.mkdir(repohisto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe0ba0b-23a0-4b47-96f6-4dc0917ec6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating folders on Tier\n",
    "if do_snapshot and distributed:\n",
    "    tier_main_folder = \"davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/\"\n",
    "    os.popen(\"davix-mkdir davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{} -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name))\n",
    "    os.popen(\"davix-mkdir davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{} -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name, remote_subfolder_name))\n",
    "    \n",
    "# transfer function for dask worker\n",
    "def transfer_to_tier(dask_worker):\n",
    "    import os\n",
    "    os.popen('for filename in snap_*.root; do davix-put $filename davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{}/$filename -E ./proxy --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/; done'.format(remote_folder_name, remote_subfolder_name))\n",
    "    return True #, os.popen(\"for filename in ./test_*.root; do echo $filename; done\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3271c9-56c5-4ee1-a501-f85177699ab8",
   "metadata": {},
   "source": [
    "- Import of utils from variables.py\n",
    "Cut (if any), Regions, Variables\n",
    "\n",
    "- syncro between in_dataset and sample_dict (from sample.py) to syncronize labels and ather featurs of the dataset (as sigma if needed)\n",
    "- import of samples_dict.json to load files list (path to reach them on tier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00f446ef-90d9-46e8-ad16-8ded4fa2fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions to book: \n",
      "  SR\n",
      "  SRmhtg100\n",
      "  SRmhtl100\n",
      "  SRnoPU\n",
      "  SR0fjets\n",
      "  SRatleast1fjets\n",
      "  ResSR\n",
      "  ResSR0fjets\n",
      "  ResSRatleast1fjets\n",
      "  MixSR\n",
      "  MixSR0fjets\n",
      "  MixSRatleast1fjets\n",
      "  MerSR\n",
      "  MerSR0fjets\n",
      "  MerSRatleast1fjets\n",
      "  NoTop\n",
      "  SRTop\n",
      "  SRTop0fjets\n",
      "  SRTopatleast1fjets\n",
      "  Presel\n",
      "  PreselNoPu\n",
      "  AH\n",
      "  AHNoPu\n",
      "  SL\n",
      "  SEl\n",
      "  SMu\n",
      "  AH1lWR\n",
      "  AH1lWREl\n",
      "  AH1lWRMu\n",
      "  AH0lZR\n",
      "  AH0lQCDR\n",
      "  PreselResolved\n",
      "  PreselMixed\n",
      "  PreselMerged\n",
      "  PreselNoTop\n",
      "Variables for histograms :\n",
      "['PuppiMET_pt', 'PuppiMET_phi', 'PuppiMET_T1_pt_nominal', 'PuppiMET_T1_phi_nominal', 'LeadingJetPt_pt', 'LeadingFatJetPt_pt', 'nTopMixed', 'nTopResolved', 'nJet', 'nJetBtagMedium', 'nJetBtagLoose', 'nFatJet', 'MinDelta_phi', 'HT_eventHT', 'MHT', 'PV_npvsGood', 'TopMixed_TopScore_nominal', 'TopResolved_TopScore_nominal', 'EventTopCategory', 'Top_mass', 'Top_pt', 'Top_score', 'MT_T']\n",
      "Datasets to process :  ['DataJetMET_2022']\n",
      "Dataset : DataJetMETC_2022\n",
      "# of files to process :  206\n",
      "files strings :\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/Run3Analysis_Tprime/DataJetMETC_2022/20240704_122343/tree_hadd_23.root\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/Run3Analysis_Tprime/DataJetMETC_2022/20240704_122343/tree_hadd_95.root\n",
      "  ... \n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/Run3Analysis_Tprime/DataJetMETC_2022/20240704_122343/tree_hadd_135.root\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/Run3Analysis_Tprime/DataJetMETC_2022/20240704_122343/tree_hadd_182.root\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  None\n",
      "Dataset : DataJetMETD_2022\n",
      "# of files to process :  100\n",
      "files strings :\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/Run3Analysis_Tprime/DataJetMETD_2022/20240704_122343/tree_hadd_23.root\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/Run3Analysis_Tprime/DataJetMETD_2022/20240704_122343/tree_hadd_95.root\n",
      "  ... \n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/Run3Analysis_Tprime/DataJetMETD_2022/20240704_122343/tree_hadd_5.root\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/Run3Analysis_Tprime/DataJetMETD_2022/20240704_122343/tree_hadd_86.root\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  None\n"
     ]
    }
   ],
   "source": [
    "cut = requirements # ---> see variables.py\n",
    "\n",
    "regions_def = regions # ---> see variables.py\n",
    "print(\"Regions to book: \")\n",
    "for r in regions_def.keys():\n",
    "    print(\"  \"+r)\n",
    "    \n",
    "sample_file = open(\"utils/dict_samples_2022.json\", \"rb\")\n",
    "samples = json.load(sample_file)\n",
    "sample_file.close()\n",
    "\n",
    "var = vars  # ---> variables.py\n",
    "var2d = vars2D \n",
    "\n",
    "print(\"Variables for histograms :\")\n",
    "print([v._name for v in var])\n",
    "\n",
    "datasets = []\n",
    "for in_d in in_dataset:\n",
    "    if not in_d in sample_dict.keys():\n",
    "        print(\"Check the in_dataset string... \", sample_dict.keys())\n",
    "    else : \n",
    "        datasets.append(sample_dict[in_d])\n",
    "print(\"Datasets to process : \", [d.label for d in datasets])\n",
    "\n",
    "\n",
    "chain = {}\n",
    "ntot_events = {}\n",
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        samples_list = d.components\n",
    "    else:\n",
    "        samples_list = [d]\n",
    "    chain[d.label] = {}\n",
    "    ntot_events[d.label] = {}\n",
    "    for s in samples_list:\n",
    "        if distributed: \n",
    "            nfiles = len(samples[d.label][s.label]['strings'])\n",
    "            for i, string in enumerate(samples[d.label][s.label]['strings']): \n",
    "                samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"root://stormgf2.pi.infn.it/\")\n",
    "            chain[d.label][s.label] = samples[d.label][s.label]['strings']\n",
    "        else: \n",
    "            nfiles = nfiles_max\n",
    "            chain[d.label][s.label] = samples[d.label][s.label]['strings'][:nfiles]\n",
    "        if not \"Data\" in s.label: ntot_events[d.label][s.label] = np.sum(samples[d.label][s.label]['ntot'][:nfiles])\n",
    "        else: ntot_events[d.label][s.label] = None\n",
    "        print(\"Dataset : \"+s.label)\n",
    "        print(\"# of files to process : \", nfiles)\n",
    "        if distributed and len(chain[d.label][s.label])>2:\n",
    "            print(\"files strings :\\n  {}\\n  {}\\n  ... \\n  {}\\n  {}\".format(chain[d.label][s.label][0], chain[d.label][s.label][1], chain[d.label][s.label][-2], chain[d.label][s.label][-1]))\n",
    "        else :\n",
    "            print(\"files strings :\\n  {}\".format(chain[d.label][s.label][0]))\n",
    "        print(\"# of total events in the files to process (MC only, if Data the number is None) : \", ntot_events[d.label][s.label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54586805-5e2c-4b36-a13b-11497d5f2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invLepveto = \"\"#\"!\" # per non invertirlo basta passare stinga vuota\n",
    "# met_cut = 250\n",
    "# mdphi_cut = 0\n",
    "# HLT_filter = \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight || HLT_Ele32_WPTight_Gsf || HLT_Ele115_CaloIdVT_GsfTrkIdT || HLT_Photon200 || HLT_IsoMu24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a9be5b-2e52-431b-9709-ee1c3e639b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "end of initialization\n"
     ]
    }
   ],
   "source": [
    "# initialization of clusters\n",
    "\n",
    "# upload the proxyfile to the Dask workers to make them able to access data on the grid \n",
    "\n",
    "from distributed.diagnostics.plugin import UploadFile\n",
    "def set_proxy(dask_worker):\n",
    "    import os\n",
    "    import shutil\n",
    "    working_dir = dask_worker.local_directory\n",
    "    proxy_name = 'x509up_u0'\n",
    "    os.environ['X509_USER_PROXY'] = working_dir + '/' + proxy_name\n",
    "    os.environ['X509_CERT_DIR']=\"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "    shutil.copyfile(working_dir + '/' + proxy_name, working_dir + '/../../../proxy')    \n",
    "    os.environ['EXTRA_CLING_ARGS'] = \"-O2\"\n",
    "    return os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\")\n",
    "\n",
    "text_file = open(\"utils/postselection.h\", \"r\")\n",
    "data = text_file.read()\n",
    "def my_initialization_function():\n",
    "    print(ROOT.gInterpreter.ProcessLine(\".O\"))\n",
    "    ROOT.gInterpreter.Declare('{}'.format(data))\n",
    "    print(\"end of initialization\")\n",
    "\n",
    "# set up everything properly\n",
    "if distributed == True:\n",
    "    RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame\n",
    "    client = Client(address=\"tcp://127.0.0.1:\"+str(sched_port))\n",
    "    client.restart()\n",
    "    client.register_worker_plugin(UploadFile(\"/tmp/x509up_u0\"))\n",
    "    client.run(set_proxy)\n",
    "    ROOT.RDF.Experimental.Distributed.initialize(my_initialization_function)\n",
    "else:\n",
    "    RDataFrame = ROOT.RDataFrame\n",
    "    my_initialization_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "963af90a-de0c-44a7-9b64-3927c34eeffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### utils ###################\n",
    "def cut_string(cut):\n",
    "    return cut.replace(\" \", \"\").replace(\"&&\",\"_\").replace(\">\",\"_g_\").replace(\".\",\"_\").replace(\"==\",\"_e_\")\n",
    "\n",
    "################### preselection ###############\n",
    "def preselection(df, btagAlg, year, EE):\n",
    "    \n",
    "    # df = df.Filter(\"MET_pt>250\")\n",
    "    \n",
    "    df = df.Define(\"GoodJet_idx\", \"GetGoodJet(Jet_pt_nominal, Jet_eta, Jet_jetId)\")\n",
    "    df = df.Define(\"nGoodJet\", \"nGoodJet(GoodJet_idx)\")\n",
    "    df = df.Define(\"GoodFatJet_idx\", \"GetGoodJet(FatJet_pt_nominal, FatJet_eta, FatJet_jetId)\")\n",
    "    df = df.Define(\"nGoodFatJet\", \"GoodFatJet_idx.size()\")\n",
    "    df = df.Filter(\"nGoodJet>2 || nGoodFatJet>0 \", \"jet presel\")\n",
    "\n",
    "    # df = df.Redefine(\"MaxEta_jet\", \"max_etajet(Jet_eta, GoodJet_idx)\")\n",
    "    df = df.Redefine(\"MinDelta_phi\", \"min_DeltaPhi(PuppiMET_T1_phi_nominal, Jet_phi, GoodJet_idx)\")\n",
    "    \n",
    "    # if 'leptonveto' in cut:\n",
    "    #     df = df.Filter(invLepveto+\"LepVeto(Electron_pt, Electron_eta, Electron_cutBased, Muon_pt, Muon_eta, Muon_looseId )\", \"Lepton Veto\")\n",
    "    #     if \"&& leptonveto\" in cut:\n",
    "    #         c_ = cut.replace(\"&& leptonveto\",\"\")\n",
    "    #     elif \"leptonveto &&\" in cut:\n",
    "    #         c_ = cut.replace(\"leptonveto &&\",\"\")\n",
    "    #     elif \"leptonveto\" in cut:\n",
    "    #         c_ = cut.replace(\"leptonveto\",\"\")    \n",
    "    # else: \n",
    "    #     df = df\n",
    "    #     c_ = cut\n",
    "    df = df.Define(\"nTightElectron\", \"nTightElectron(Electron_pt, Electron_eta, Electron_cutBased)\")\n",
    "    df = df.Define(\"TightElectron_idx\", \"TightElectron_idx(Electron_pt, Electron_eta, Electron_cutBased)\")\n",
    "    df = df.Define(\"nVetoElectron\", \"nVetoElectron(Electron_pt, Electron_cutBased, Electron_eta)\")\n",
    "    df = df.Define(\"nTightMuon\", \"nTightMuon(Muon_pt, Muon_eta, Muon_tightId)\")\n",
    "    df = df.Define(\"TightMuon_idx\", \"TightMuon_idx(Muon_pt, Muon_eta, Muon_tightId)\")\n",
    "    df = df.Define(\"nVetoMuon\", \"nVetoMuon(Muon_pt, Muon_eta, Muon_looseId)\")\n",
    "    df = df.Define(\"Lepton_flavour\", \"Lepton_flavour(nTightElectron, nTightMuon)\").Define(\"Lep_pt\", \"Lepton_var(Lepton_flavour, Electron_pt, TightElectron_idx, Muon_pt, TightMuon_idx)\").Define(\"Lep_phi\", \"Lepton_var(Lepton_flavour, Electron_phi, TightElectron_idx, Muon_phi, TightMuon_idx)\")\n",
    "    df = df.Define(\"MT\", \"sqrt(2 * Lep_pt * PuppiMET_T1_pt_nominal * (1 - cos(Lep_phi - PuppiMET_T1_phi_nominal)))\")\n",
    "    \n",
    "    # df = df.Filter(\"MET_pt>\"+ str(met_cut), \"MET_pt>\"+ str(met_cut))\n",
    "    # df = df.Filter(\"MinDelta_phi>\"+ str(mdphi_cut), \"MinDeltaPhi>\"+ str(mdphi_cut))\n",
    "    \n",
    "    # df = df.Filter(\"atLeast1jet_setparams(Jet_pt, Jet_eta, Jet_mass, Jet_jetId, 30, 4, 0, 1)\", \"At_least_1Ak4\")\n",
    "    # df = df.Filter(\"atLeast1fatjet_setparams(FatJet_pt, FatJet_msoftdrop, FatJet_eta, FatJet_jetId, 200, 6, 40, 1)\", \"at_least_1Ak8\")\n",
    "\n",
    "    df = df.Define(\"LeadingJetPt_idx\", \"GetLeadingPtJet(Jet_pt_nominal)\")\n",
    "    df = df.Define(\"LeadingJetPt_pt\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_pt_nominal)\")\n",
    "    df = df.Define(\"LeadingJetPt_eta\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_eta)\")\n",
    "    df = df.Define(\"LeadingJetPt_phi\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_phi)\")\n",
    "    df = df.Define(\"LeadingJetPt_mass\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_mass_nominal)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_idx\", \"GetLeadingPtJet(FatJet_pt)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_pt\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_pt_nominal)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_eta\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_eta)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_phi\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_phi)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_mass\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_mass_nominal)\")\n",
    "    df = df.Define(\"LeadingMuonPt_idx\", \"GetLeadingPtLep(Muon_pt, Muon_eta, Muon_looseId)\")\n",
    "    df = df.Define(\"LeadingMuonPt_pt\", \"GetLeadingJetVar(LeadingMuonPt_idx, Muon_pt)\")\n",
    "    df = df.Define(\"LeadingMuonPt_eta\", \"GetLeadingJetVar(LeadingMuonPt_idx, Muon_eta)\")\n",
    "    df = df.Define(\"LeadingMuonPt_phi\", \"GetLeadingJetVar(LeadingMuonPt_idx, Muon_phi)\")\n",
    "    df = df.Define(\"LeadingElectronPt_idx\", \"GetLeadingPtLep(Electron_pt, Electron_eta, Electron_cutBased)\")\n",
    "    df = df.Define(\"LeadingElectronPt_pt\", \"GetLeadingJetVar(LeadingElectronPt_idx, Electron_pt)\")\n",
    "    df = df.Define(\"LeadingElectronPt_eta\", \"GetLeadingJetVar(LeadingElectronPt_idx, Electron_eta)\")\n",
    "    df = df.Define(\"LeadingElectronPt_phi\", \"GetLeadingJetVar(LeadingElectronPt_idx, Electron_phi)\")\n",
    "    \n",
    "    df = df.Define(\"nForwardJet\", \"nForwardJet(Jet_pt_nominal, Jet_jetId, Jet_eta)\")\n",
    "    df = df.Define(\"MHT\",\"MHT(GoodJet_idx, Jet_pt_nominal, Jet_phi, Jet_eta, Jet_mass_nominal)\")\n",
    "    # df = df.Define(\"JetBTag_idx\", \"GetJetBTag(GoodJet_idx, \"+bTagAlg+\",\"+str(year)+\",\"+str(EE)+\")\")\\\n",
    "    #             .Define(\"nJetBtag\", \"static_cast<int>(JetBTag_idx.size());\")\n",
    "    df = df.Define(\"JetBTagLoose_idx\", \"GetJetBTag(GoodJet_idx, \"+bTagAlg+\",\"+str(year)+\",\"+str(EE)+\", 0)\")\\\n",
    "                .Define(\"nJetBtagLoose\", \"static_cast<int>(JetBTagLoose_idx.size());\")\n",
    "    df = df.Define(\"JetBTagMedium_idx\", \"GetJetBTag(GoodJet_idx, \"+bTagAlg+\",\"+str(year)+\",\"+str(EE)+\", 1)\")\\\n",
    "                .Define(\"nJetBtagMedium\", \"static_cast<int>(JetBTagMedium_idx.size());\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "############### trigger selection #####################\n",
    "def trigger_filter(df, data, isMC):\n",
    "    hlt_met = \"(HLT_PFMET120_PFMHT120_IDTight || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight)\"\n",
    "    df_trig = df.Filter(hlt_met, \"triggerMET\")\n",
    "    return df_trig\n",
    "\n",
    "############### top selection ########################\n",
    "def select_top(df, isMC):\n",
    "    # return indices of the FatJet with particleNet score over the thresholds \n",
    "    df_goodtopMer = df.Define(\"GoodTopMer_idx\", \"select_TopMer(FatJet_particleNetWithMass_TvsQCD, GoodFatJet_idx)\")\n",
    "    # return indices of the TopMixed over the threshold with any object in common\n",
    "    df_goodtopMix = df_goodtopMer.Define(\"GoodTopMix_idx\", \"select_TopMix(TopMixed_TopScore_nominal, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, GoodJet_idx, GoodFatJet_idx)\")\n",
    "    # return indices of the TopResolved over the threshold with any object in common\n",
    "    df_goodtopRes = df_goodtopMix.Define(\"GoodTopRes_idx\", \"select_TopRes(TopResolved_TopScore_nominal, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, GoodJet_idx)\")\n",
    "    \n",
    "    df_nTops = df_goodtopRes.Define(\"nGoodTopResolved\", \"nTop(GoodTopRes_idx)\")\\\n",
    "                            .Define(\"nGoodTopMixed\", \"nTop(GoodTopMix_idx)\")\\\n",
    "                            .Define(\"nGoodTopMerged\", \"nTop(GoodTopMer_idx)\")\n",
    "    \n",
    "    \n",
    "    # return:  1- Event Resolved, 2- Event Mixed, 3- Event Merged, 4- Event Nothing, ...\n",
    "    df_topcategory = df_nTops.Define(\"EventTopCategory\", \"select_TopCategory(GoodTopMer_idx, GoodTopMix_idx, GoodTopRes_idx)\")\n",
    "    if isMC:\n",
    "        df_topcategory = df_topcategory.Define(\"EventTopCategoryWithTruth\", \"select_TopCategoryWithTruth(EventTopCategory, FatJet_matched, GoodTopMer_idx, TopMixed_truth, GoodTopMix_idx, TopResolved_truth, GoodTopRes_idx)\")\n",
    "    \n",
    "    df_topselected = df_topcategory.Define(\"Top_idx\",\n",
    "                                           \"select_bestTop(EventTopCategory, FatJet_particleNetWithMass_TvsQCD, TopMixed_TopScore_nominal, TopResolved_TopScore_nominal)\")\n",
    "    # return best top idx wrt category --> the idx is referred to the list of candidates fixed by the EventTopCategory\n",
    "    df_topvariables = df_topselected.Define(\"Top_pt\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_pt_nominal, TopMixed_pt_nominal, TopResolved_pt_nominal)\")\\\n",
    "                        .Define(\"Top_eta\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_eta, TopMixed_eta, TopResolved_eta)\")\\\n",
    "                        .Define(\"Top_phi\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_phi, TopMixed_phi, TopResolved_phi)\")\\\n",
    "                        .Define(\"Top_mass\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_mass_nominal, TopMixed_mass_nominal, TopResolved_mass_nominal)\")\\\n",
    "                        .Define(\"Top_score\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_particleNetWithMass_TvsQCD, TopMixed_TopScore_nominal, TopResolved_TopScore_nominal)\")\\\n",
    "                        .Define(\"Top_isolationPtJetsdR04\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 0.4, 1)\")\\\n",
    "                        .Define(\"Top_isolationPtJetsdR06\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 0.6, 1)\")\\\n",
    "                        .Define(\"Top_isolationPtJetsdR08\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 0.8, 1)\")\\\n",
    "                        .Define(\"Top_isolationPtJetsdR12\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 1.2, 1)\")\\\n",
    "                        .Define(\"Top_isolationNJetsdR04\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 0.4, 0)\")\\\n",
    "                        .Define(\"Top_isolationNJetsdR06\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 0.6, 0)\")\\\n",
    "                        .Define(\"Top_isolationNJetsdR08\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 0.8, 0)\")\\\n",
    "                        .Define(\"Top_isolationNJetsdR12\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt_nominal, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt_nominal, TopResolved_phi, TopResolved_eta, FatJet_pt_nominal, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt_nominal, Jet_eta, Jet_phi, Jet_jetId, 1.2, 0)\")\n",
    "\n",
    "    if isMC:\n",
    "        df_topvariables = df_topvariables.Define(\"Top_truth\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_matched, TopMixed_truth, TopResolved_truth)\")\n",
    "    # NB: TopTruth for Merged is replaced with FatJet_matched, the variable is between 0 and 3 \n",
    "    # where 3 means true end less than 3 means false \n",
    "    return df_topvariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f0bf61-b2ab-422e-8baf-9abba2b984cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookhisto(df, regions_def, var, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var:\n",
    "            if \"SFbtag\" in v._name: continue\n",
    "\n",
    "            if v._MConly and not sampleflag: \n",
    "                continue\n",
    "            else:\n",
    "                if regions_def[reg] == \"\":\n",
    "                    if v._noUnOvFlowbin:\n",
    "                        # if not v._xarray is None: h_[reg][v._name]= df.Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xarray), v._name, \"w_nominal\")\n",
    "                        # else: h_[reg][v._name]= df.Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                        if \"NoPu\" in reg: \n",
    "                            h_[reg][v._name]= df.Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                        else: \n",
    "                            h_[reg][v._name]= df.Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                    else:\n",
    "                        # if not v._xarray is None: h_[reg][v._name]= df.Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xarray[0])+\",\"+str(v._xarray[-1])+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xarray), v._name, \"w_nominal\")\n",
    "                        # else: h_[reg][v._name]= df.Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                        if \"NoPu\" in reg: \n",
    "                            h_[reg][v._name]= df.Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                        else: h_[reg][v._name]= df.Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                else:\n",
    "                    if v._noUnOvFlowbin:\n",
    "                        # if not v._xarray is None: h_[reg][v._name]= df.Filter(regions_def[reg]).Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xarray), v._name, \"w_nominal\")\n",
    "                        # else : h_[reg][v._name]= df.Filter(regions_def[reg]).Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                        if \"NoPu\" in reg: \n",
    "                            h_[reg][v._name]= df.Filter(regions_def[reg]).Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                        else: h_[reg][v._name]= df.Filter(regions_def[reg]).Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title, v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                    else:\n",
    "                        # if not v._xarray is None: h_[reg][v._name]= df.Filter(regions_def[reg]).Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xarray[0])+\",\"+str(v._xarray[-1])+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xarray), v._name, \"w_nominal\")\n",
    "                        # else : h_[reg][v._name]= df.Filter(regions_def[reg]).Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                        if \"NoPu\" in reg: \n",
    "                            h_[reg][v._name]= df.Filter(regions_def[reg]).Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                        else: h_[reg][v._name]= df.Filter(regions_def[reg]).Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "    h_[\"btagSF\"] = df.Filter(\"PuppiMET_T1_pt_nominal>250 && MinDelta_phi>0.6 && (nVetoElectron==0 && nVetoMuon ==0)\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "    # print(h_[\"btagSF\"])\n",
    "    return h_\n",
    "\n",
    "def bookhisto2D(df, regions_def, var2d, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var2d:\n",
    "            if regions_def[reg]==\"\":\n",
    "                h_[reg][v._name] = df.Redefine(v._xname, \"UnOvBin(\"+v._xname+\",\"+str(v._nxbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\")\\\n",
    "                                     .Redefine(v._yname, \"UnOvBin(\"+v._yname+\",\"+str(v._nybins)+\",\"+str(v._ymin)+\",\"+str(v._ymax)+\")\")\\\n",
    "                                     .Histo2D((v._xname+\"Vs\"+v._yname+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax), v._xname, v._yname)\n",
    "            else:\n",
    "                h_[reg][v._name] = df.Filter(regions_def[reg])\\\n",
    "                                     .Redefine(v._xname, \"UnOvBin(\"+v._xname+\",\"+str(v._nxbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\")\\\n",
    "                                     .Redefine(v._yname, \"UnOvBin(\"+v._yname+\",\"+str(v._nybins)+\",\"+str(v._ymin)+\",\"+str(v._ymax)+\")\")\\\n",
    "                                     .Histo2D((v._xname+\"Vs\"+v._yname+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax), v._xname, v._yname)\n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37967949-f502-4c77-9ceb-dd04f8062cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def savehisto(d, h, regions_def, var, s_cut, isMC):\n",
    "    histo = {reg: {v._name: ROOT.TH1D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax) for v in var} for reg in regions_def.keys()}\n",
    "    isMC=True\n",
    "    if \"Data\" in d.label: isMC = False\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    \n",
    "    for s in s_list:\n",
    "        outfile = ROOT.TFile.Open(repohisto+s.label+'.root', \"RECREATE\")\n",
    "        for reg in regions_def.keys():\n",
    "            for v in var:\n",
    "                if \"SFbtag\" in v._name: continue\n",
    "                if v._MConly and not isMC:\n",
    "                    continue\n",
    "                else:\n",
    "                    histo[reg][v._name] = h[d.label][s.label][reg][v._name].GetValue()\n",
    "                    if isMC:\n",
    "                        histo[reg][v._name].Scale(s.sigma*10**3/ntot_events[d.label][s.label])\n",
    "                    outfile.cd()\n",
    "                    histo[reg][v._name].Write()\n",
    "        histo_btagSF = h[d.label][s.label][\"btagSF\"].GetValue()\n",
    "        outfile.cd()\n",
    "        histo_btagSF.Write()\n",
    "        outfile.Close()\n",
    "\n",
    "# i plot2d per il momento non ci servono, si deve trovare un modo più intelligente di farli\n",
    "def savehisto2d(d, h, regions_def, var2d, s_cut, isMC):\n",
    "    histo = {reg: {v._name: ROOT.TH2D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax,) for v in var2d} for reg in regions_def.keys()}\n",
    "        \n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    \n",
    "    for s in s_list:\n",
    "        outfile = ROOT.TFile.Open(repohisto+s.label+'_2D.root', \"RECREATE\")\n",
    "        for reg in regions_def.keys():\n",
    "            for v in histo[reg].keys():\n",
    "                histo[reg][v] = h[d.label][s.label][reg][v].GetValue()\n",
    "                if isMC:\n",
    "                        histo[reg][v._name].Scale(s.sigma*10**3/ntot_events[d.label][s.label])\n",
    "                outfile.cd()\n",
    "                histo[reg][v].Write()\n",
    "        outfile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c96d28-45f4-4877-8d00-680e095a3016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#samples['DataHTF_2022']['DataHTF_2022']['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab036200-65f9-44a0-abbb-8a04003fc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples[d.label][d.components[0]]['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b72ddfd-d641-437d-a78c-8568a0a17a30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loop on datasets:  ['DataJetMET_2022']\n",
      "Local time : 2024-07-19 11:51:10.936997\n",
      "Initializing DataFrame for DataJetMETC_2022 chain len =  206\n",
      "Initializing DataFrame for DataJetMETD_2022 chain len =  100\n",
      "All histos booked !\n",
      "DataJetMET_2022 histos saved\n",
      "Job finished in:  0:10:13.854577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TClass::Init>: no dictionary for class edm::Hash<1> is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ParameterSetBlob is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessHistory is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessConfiguration is available\n",
      "Warning in <TClass::Init>: no dictionary for class pair<edm::Hash<1>,edm::ParameterSetBlob> is available\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9aebe0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9f3af0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9bb920) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0x8a0bb00) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb8a3e50) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb99a640) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9a1210) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb94da50) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb94dd80) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb94e090) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9413f0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0x8a15410) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9e6d10) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9e33e0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9e3830) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9e46b0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b0190) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b0650) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b1040) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b1270) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b16d0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b1e70) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b23f0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b28f0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b2e10) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b33b0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b3950) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b3ef0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b4490) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b4a30) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b4fd0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b5340) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b5670) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b5a10) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b5d40) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9e2510) already deleted (list name = TList)\n",
      "Error in <THashList::Delete>: A list is accessing an object (0x9bfe160) already deleted (list name = THashList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0x8a15410) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9e6d10) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9e33e0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b1040) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbd0b9d0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbc50950) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbca8530) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbca8420) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb44a5b0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb44a4c0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb8916c0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb3f5a60) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb3ac950) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb3ac7f0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb439470) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbca8620) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbcc0740) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbca89b0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb3d45a0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb49b860) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb410d50) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb88f260) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb01a910) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xafef390) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0x99574d0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbc50d60) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xc227240) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbcbb890) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb417500) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbc21d50) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb454ac0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbcb88e0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb894610) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb3f5110) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb3b00f0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb40b570) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb3ba500) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb445340) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb457290) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0x88173f0) already deleted (list name = TList)\n",
      "Error in <THashList::Delete>: A list is accessing an object (0xbc08480) already deleted (list name = THashList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0x8a15410) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9e6d10) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9e33e0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xb9b1040) already deleted (list name = TList)\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "print(\"starting loop on datasets: \",[d.label for d in datasets])\n",
    "print(\"Local time :\", t0)\n",
    "# print(\"requirements: \"+cut)\n",
    "\n",
    "h = {}\n",
    "h_2D = {}\n",
    "\n",
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "\n",
    "    if 'Data' in d.label : sampleflag = 0\n",
    "    else: sampleflag = 1\n",
    "    c_ = cut\n",
    "    h[d.label] = {}\n",
    "    h_2D[d.label] = {}\n",
    "    for s in s_list:\n",
    "        #-------------------------------------------------------------------------\n",
    "        ############# Fixing variables for 2018-2022 #############################\n",
    "        #-------------------------------------------------------------------------\n",
    "        if s.year == 2018:\n",
    "            bTagAlg = \"Jet_btagDeepB\"\n",
    "        elif s.year == 2022:\n",
    "            bTagAlg = \"Jet_btagPNetB\"\n",
    "        if hasattr(s,\"EE\"):\n",
    "            EE = s.EE\n",
    "        else:\n",
    "            EE = 0\n",
    "        #-------------------------------------------------------------------------\n",
    "        #########################  DF initialization #############################\n",
    "        #-------------------------------------------------------------------------\n",
    "        \n",
    "        print(\"Initializing DataFrame for \"+ s.label +\" chain len = \", len(chain[d.label][s.label]))\n",
    "        if len(chain[d.label][s.label])==1: print(chain[d.label][s.label])\n",
    "        if distributed ==True:\n",
    "            df = RDataFrame(\"Events\", chain[d.label][s.label], npartitions=nmaxpartition, \n",
    "                            daskclient=client, monitor_label = \"main\" )\n",
    "        else:\n",
    "            df = RDataFrame(\"Events\", chain[d.label][s.label])\n",
    "        \n",
    "        \n",
    "        df_ismc         = df.Define(\"isMC\", \"isMC(\"+str(sampleflag)+\")\")\n",
    "        df_year         = df_ismc.Define(\"year\", str(s.year))\n",
    "        df_hemveto      = df_year.Define(\"HEMVeto\", \"hemveto(Jet_eta, Jet_phi, Electron_eta, Electron_phi)\")\n",
    "        df_hemveto      = df_hemveto.Filter(\"(isMC || (year != 2018) || (HEMVeto || run<319077.))\")\n",
    "        df_hlt          = trigger_filter(df_hemveto, s.label, sampleflag)    \n",
    "        ### per gli HLT potremmo volerli modificare a seconda della regione (?) in tal caso la funzione\n",
    "        ### va spostata nel book histos e va passata la regione per attivare trigger diversi\n",
    "        if sampleflag: df_wnom = df_hlt.Define('w_nominal', 'puWeight*SFbtag_nominal') # controllare questi pesi nominal \n",
    "        else: df_wnom           = df_hlt.Define('w_nominal', '1')\n",
    "        # df_wnom           = df_hlt.Define('w_nominal', '1')\n",
    "        df_presel       = preselection(df_wnom, bTagAlg, s.year, EE)\n",
    "        df_topsel       = select_top(df_presel, sampleflag)\n",
    "        df_topsel       = df_topsel.Define(\"MT_T\", \"sqrt(2 * Top_pt * PuppiMET_T1_pt_nominal * (1 - cos(Top_phi - PuppiMET_T1_phi_nominal)))\")\n",
    "        \n",
    "        \n",
    "        if do_snapshot:\n",
    "            opts = ROOT.RDF.RSnapshotOptions()\n",
    "            opts.fLazy = True\n",
    "            if distributed: fold = \"./\"\n",
    "            else: fold = folder\n",
    "            snapshot_df = df_topsel.Snapshot(\"events_nominal\", fold+\"snap_\"+s.label+\".root\", branches, opts)\n",
    "            # print(\"./\"+s.label+\".root\")\n",
    "        if do_histos:\n",
    "            s_cut = cut_string(cut)\n",
    "            if len(var) != 0 :\n",
    "                h[d.label][s.label] = bookhisto(df_topsel, regions_def, var, s_cut)\n",
    "            if len(var2d) != 0 :\n",
    "                h_2D[d.label][s.label] = bookhisto2D(df_topsel, regions_def, var2d, s_cut)\n",
    "# if not distributed:\n",
    "#     df_presel.Report().Print()\n",
    "\n",
    "if do_histos:\n",
    "    print(\"All histos booked !\")\n",
    "    for d in datasets:\n",
    "        if len(var):\n",
    "            savehisto(d, h, regions_def, var, s_cut, sampleflag)\n",
    "        if len(var2d) != 0 :\n",
    "            savehisto2d(d, h_2D, regions_def, var2d, s_cut, sampleflag)\n",
    "        print(d.label + \" histos saved\")\n",
    "if do_snapshot:\n",
    "    snapshot_df.GetValue()\n",
    "    if distributed: \n",
    "        client.run(transfer_to_tier)\n",
    "        print(\"Snapshots saved and trasfered to tier\")\n",
    "    print(\"Sanpshot done!\")\n",
    "t1 = datetime.now()\n",
    "print(\"Job finished in: \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9573b747-9a5a-4501-b084-481b10d418ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ./results/run2022_sel/plots/DataJetMETD_2022.root Title: \n",
      "SR 1470.0\n",
      "SRmhtg100 310.0\n",
      "SRmhtl100 1160.0\n",
      "SRnoPU 1470.0\n",
      "SR0fjets 1038.0\n",
      "SRatleast1fjets 432.0\n",
      "ResSR 0.0\n",
      "ResSR0fjets 0.0\n",
      "ResSRatleast1fjets 0.0\n",
      "MixSR 36.0\n",
      "MixSR0fjets 15.0\n",
      "MixSRatleast1fjets 21.0\n",
      "MerSR 3.0\n",
      "MerSR0fjets 0.0\n",
      "MerSRatleast1fjets 3.0\n",
      "NoTop 1431.0\n",
      "SRTop 39.0\n",
      "SRTop0fjets 15.0\n",
      "SRTopatleast1fjets 24.0\n",
      "Presel 106910.0\n",
      "PreselNoPu 106910.0\n",
      "AH 16333.0\n",
      "AHNoPu 16333.0\n",
      "SL 139728.0\n",
      "SEl 5542.0\n",
      "SMu 134186.0\n",
      "AH1lWR 62842.0\n",
      "AH1lWREl 4798.0\n",
      "AH1lWRMu 58044.0\n",
      "AH0lZR 8807.0\n",
      "AH0lQCDR 879370.0\n",
      "PreselResolved 175.0\n",
      "PreselMixed 9447.0\n",
      "PreselMerged 1211.0\n",
      "PreselNoTop 96077.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in <TList::Clear>: A list is accessing an object (0xbd68e50) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbd69220) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbd695f0) already deleted (list name = TList)\n",
      "Error in <TList::Clear>: A list is accessing an object (0xbd69ac0) already deleted (list name = TList)\n"
     ]
    }
   ],
   "source": [
    "file = ROOT.TFile.Open(repohisto+s.label+\".root\")\n",
    "print(file)\n",
    "# for a in file.GetListOfKeys(): print(a)\n",
    "for reg in regions_def.keys():\n",
    "    # for v in var:\n",
    "    hist = file.Get(var[1]._name+\"_\"+reg+\"_\")\n",
    "    print(reg, hist.Integral())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3606a5b-e3ce-491d-a86a-6c02431ca435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions_def.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "269963fc-638b-4a19-8c8d-af26bad6e37f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# region = \"NoCut\"\n",
    "# var = \"MET_pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8daa57df-4a5e-46d7-b758-84edcebca76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if do_snapshot and distributed:\n",
    "    print(os.popen(\"davix-ls davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{}/ -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name, remote_subfolder_name)).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af25cc-cc36-437c-94e7-42aaa09cbfcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16461c9-cba2-4f38-b676-4f5ee2562665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity kernel",
   "language": "python",
   "name": "singularity-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
