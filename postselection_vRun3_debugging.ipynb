{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd081e3",
   "metadata": {},
   "source": [
    "**Framework version January 2024**\n",
    "- Log :\n",
    "    - understood bug in data (the problem was in CRAB not here)\n",
    "    - added Snapshot \n",
    "- Planned update :\n",
    "    - standalone code, prepare a couple of input parameters and make the code working with a single command\n",
    "    \n",
    "    \n",
    "__________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98655626-5252-41be-b111-76e638cecddf",
   "metadata": {},
   "source": [
    "**Code**\n",
    "\n",
    "Folder definition on Tier:\n",
    "- in the main folder */acagnott/* added folder 'remote_folder_name';\n",
    "- in \"Snapshots\" added the folder 'remote_subfolder_name';\n",
    "- in the subfolder through dask the snapshot will be copied with name \"snap_\"+label+\"_*.root\"\n",
    "\n",
    "\n",
    "Es: se lancio \"DataMETA_2018\", gli snapshot vengono salvati in ../acagnott/Snapshot/20231229/snap_DataMET_2018_*.root\n",
    "se viene lanciato \"QCD_2018\" viene creata la cartella /acagnott/Snapshot/20231229/snap_QCDHT_100to200_2018_*.root e così via per ogni components\n",
    "\n",
    "---> Viene usato solo il giorno in modo che tutti i sample lanciati lo stesso giorno verranno salvati nella stessa cartella con nomi diversi, visto che vengono lanciati in momenti diversi della giornata lo stesso tipo di job. Forse va modificato il formato se i singoli job iniziano a durare più di un giorno dato che viene comunque lanciato un sampel per volta (in tal caso verrebbero salvati in cartelle diverse. Si potrebbe pensare di mettere la data a mano, cioé invece di usare datetime.now() si potrebbe inserire la data manualmente per fare in modo di mettere tutti i file nella stessa folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf1ce47-e2f0-48f4-8e4b-b0fa9f1bacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "sched_port = 26486 #Dask port\n",
    "nmaxpartition = 100\n",
    "distributed = True#False#\n",
    "do_histos = True\n",
    "hist_folder = \"run2018_regions_v0\"\n",
    "do_snapshot = False\n",
    "remote_subfolder_name = datetime.now().strftime(\"%Y%m%d\") #20231229\n",
    "\n",
    "in_dataset = [\n",
    "    \"DataMETA_2018\", \n",
    "    # \"DataMETB_2018\", \n",
    "    # \"DataMETC_2018\",\n",
    "    # \"DataMETD_2018\",\n",
    "    # \"DataSingleMuA_2018\", \n",
    "    # \"DataSingleMuB_2018\", \n",
    "    # \"DataSingleMuC_2018\",\n",
    "    # \"DataSingleMuD_2018\",\n",
    "    \n",
    "    # \"TprimeToTZ_700_2018\",\n",
    "    # \"TprimeToTZ_1000_2018\", \n",
    "    # \"TprimeToTZ_1800_2018\",\n",
    "    \n",
    "    # \"QCD_2018\",\n",
    "    # \"TT_2018\",\n",
    "    # \"ZJetsToNuNu_2018\",\n",
    "    # \"WJets_2018\",\n",
    "    \n",
    "    # \"QCDHT_100to200_2018\", \"QCDHT_200to300_2018\",\"QCDHT_300to500_2018\", \"QCDHT_500to700_2018\", \"QCDHT_700to1000_2018\",\"QCDHT_1000to1500_2018\", \"QCDHT_1500to2000_2018\",\"QCDHT_2000toInf_2018\",\"TT_hadr_2018\", \"TT_semilep_2018\", \"TT_Mtt700to1000_2018\", \"TT_Mtt1000toInf_2018\",\"ZJetsToNuNu_HT100to200_2018\", \"ZJetsToNuNu_HT200to400_2018\", \"ZJetsToNuNu_HT400to600_2018\", \"ZJetsToNuNu_HT600to800_2018\", \"ZJetsToNuNu_HT800to1200_2018\", \"ZJetsToNuNu_HT1200to2500_2018\", \"ZJetsToNuNu_HT2500toInf_2018\"\"WJetsHT100to200_2018\", \"WJetsHT200to400_2018\", \"WJetsHT400to600_2018\", \"WJetsHT600to800_2018\", \"WJetsHT800to1200_2018\", \"WJetsHT1200to2500_2018\", \"WJetsHT2500toInf_2018\"\n",
    "]\n",
    "\n",
    "# \"event\", \"run\", \n",
    "branches = {\"HT_eventHT\", \"Top_mass\", \"Top_pt\", \"Top_score\", \"Top_isolationPtJetsdR04\", \"Top_isolationPtJetsdR06\", \"Top_isolationPtJetsdR08\", \"Top_isolationPtJetsdR12\", \"Top_isolationNJetsdR04\", \"Top_isolationNJetsdR06\", \"Top_isolationNJetsdR08\", \"Top_isolationNJetsdR12\",\n",
    "            \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60\", \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight\", \n",
    "            \"HLT_Ele32_WPTight_Gsf\", \"HLT_Ele115_CaloIdVT_GsfTrkIdT\", \"HLT_Photon200\", \n",
    "            \"HLT_IsoMu24\",\"HLT_PFMET120_PFMHT120_IDTight\", \"HLT_PFMETNoMu140_PFMHTNoMu140_IDTight\", \n",
    "            \"HLT_PFMET140_PFMHT140_IDTight\", \"MET_pt\", \"nVetoMuon\", \"nVetoElectron\", \"nJetBtag\", \n",
    "            \"nGoodJet\", \"nTightElectron\", \"nTightMuon\", \"MT\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e16aaf3-78d5-4a3c-851c-21f771dcf586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.27/01\n",
      "/tmp/x509up_u0 /cvmfs/grid.cern.ch/etc/grid-security/certificates/\n",
      "You are producing histograms\n",
      "local folder histos: ./results/run2018_regions_v0/\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import os\n",
    "from utils.samples import *\n",
    "from utils.variables import *\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from dask.distributed import Client\n",
    "ROOT.RDF.Experimental.Distributed.open_files_locally = False\n",
    "\n",
    "os.environ['X509_CERT_DIR'] = \"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "os.environ['X509_USER_PROXY'] = \"/tmp/x509up_u0\"\n",
    "print(os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\"))\n",
    "\n",
    "\n",
    "if distributed:\n",
    "    nfiles_max = 1000\n",
    "else:\n",
    "    nfiles_max = 1  #######\n",
    "\n",
    "# Cosa aggiungere, modificare la cartella sul tier in questo modo ../acagnott/Snapshot_rdf/*dataset_name*/*data di processamente con orario*/\n",
    "# insomma come fa crab\n",
    "\n",
    "\n",
    "if do_histos: print(\"You are producing histograms\")\n",
    "if do_snapshot: print(\"You are producing snapshot\")\n",
    "\n",
    "remote_folder_name = \"Snapshots\"\n",
    "#output histos folder\n",
    "folder = \"./results/\"+hist_folder+\"/\"\n",
    "# eos_folder = \"/eos/home-a/acagnott/DarkMatter/nosynch/\"+hist_folder\n",
    "\n",
    "if do_snapshot and remote_subfolder_name == datetime.now().strftime(\"%Y%m%d\") and distributed: \n",
    "    print(\"You are naming the tier subfolder using the current day \\n\")\n",
    "    print(\"Snapshots folder name : ~/acagnott/{}/{}\".format(remote_folder_name, remote_subfolder_name))\n",
    "elif do_snapshot and distributed:\n",
    "    print(\"You are naming the tier subfolder manually\")\n",
    "    print(\"Snapshots folder name : ~/acagnott/{}/{}\".format(remote_folder_name, remote_subfolder_name))\n",
    "elif do_snapshot:\n",
    "    print(\"You are saving snapshots in local\")\n",
    "    print(\"folder name : \" + folder)\n",
    "\n",
    "\n",
    "if do_histos : \n",
    "    print(\"local folder histos: {}\".format(folder))\n",
    "    # print(\"At the end of the job the histos will be transfer to eos storage\")\n",
    "    # print(\"eos folder : {}\".format(eos_folder))\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "repohisto = folder+\"plots/\"\n",
    "if not os.path.exists(repohisto):\n",
    "    os.mkdir(repohisto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe0ba0b-23a0-4b47-96f6-4dc0917ec6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating folders on Tier\n",
    "if do_snapshot and distributed:\n",
    "    tier_main_folder = \"davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/\"\n",
    "    os.popen(\"davix-mkdir davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{} -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name))\n",
    "    os.popen(\"davix-mkdir davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{} -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name, remote_subfolder_name))\n",
    "    \n",
    "# transfer function for dask worker\n",
    "def transfer_to_tier(dask_worker):\n",
    "    import os\n",
    "    os.popen('for filename in snap_*.root; do davix-put $filename davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{}/$filename -E ./proxy --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/; done'.format(remote_folder_name, remote_subfolder_name))\n",
    "    return True #, os.popen(\"for filename in ./test_*.root; do echo $filename; done\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3271c9-56c5-4ee1-a501-f85177699ab8",
   "metadata": {},
   "source": [
    "- Import of utils from variables.py\n",
    "Cut (if any), Regions, Variables\n",
    "\n",
    "- syncro between in_dataset and sample_dict (from sample.py) to syncronize labels and ather featurs of the dataset (as sigma if needed)\n",
    "- import of samples_dict.json to load files list (path to reach them on tier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f446ef-90d9-46e8-ad16-8ded4fa2fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions to book: \n",
      "  All\n",
      "  Presel\n",
      "  AH\n",
      "  SL\n",
      "  SEl\n",
      "  SMu\n",
      "  AH1lWR\n",
      "  AH1lWREl\n",
      "  AH1lWRMu\n",
      "  PreselResolved\n",
      "  PreselMixed\n",
      "  PreselMerged\n",
      "  PreselNoTop\n",
      "Variables for histograms :\n",
      "['MET_pt', 'MET_phi', 'PuppiMET_pt', 'LeadingJetPt_pt', 'LeadingFatJetPt_pt', 'LeadingMuonPt_pt', 'LeadingElectronPt_pt', 'nJet', 'nJetBtag', 'nFatJet', 'MinDelta_phi', 'MaxEta_jet', 'HT_eventHT', 'PV_npvsGood', 'EventTopCategory', 'Top_truth', 'EventTopCategoryWithTruth', 'Top_mass', 'Top_pt', 'Top_score', 'Top_isolationPtJetsdR04', 'Top_isolationPtJetsdR06', 'Top_isolationPtJetsdR08', 'Top_isolationPtJetsdR12', 'Top_isolationNJetsdR04', 'Top_isolationNJetsdR06', 'Top_isolationNJetsdR08', 'Top_isolationNJetsdR12']\n",
      "Datasets to process :  ['DataMETA_2018']\n",
      "Dataset : DataMETA_2018\n",
      "# of files to process :  43\n",
      "files strings :\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/MET/DataMETA_2018/240109_105451/0000/tree_hadd_23.root\n",
      "\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/MET/DataMETA_2018/240109_105451/0000/tree_hadd_29.root\n",
      "\n",
      "  ... \n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/MET/DataMETA_2018/240109_105451/0000/tree_hadd_36.root\n",
      "\n",
      "  root://stormgf2.pi.infn.it//store/user/acagnott/DM_Run3_v0/MET/DataMETA_2018/240109_105451/0000/tree_hadd_5.root\n",
      "\n",
      "# of total events in the files to process (MC only, if Data the number is None) :  None\n"
     ]
    }
   ],
   "source": [
    "cut = requirements # ---> see variables.py\n",
    "\n",
    "regions_def = regions # ---> see variables.py\n",
    "print(\"Regions to book: \")\n",
    "for r in regions_def.keys():\n",
    "    print(\"  \"+r)\n",
    "    \n",
    "sample_file = open(\"utils/dict_samples.json\", \"rb\")\n",
    "samples = json.load(sample_file)\n",
    "sample_file.close()\n",
    "\n",
    "var = vars  # ---> variables.py\n",
    "var2d = vars2D \n",
    "\n",
    "print(\"Variables for histograms :\")\n",
    "print([v._name for v in var])\n",
    "\n",
    "datasets = []\n",
    "for in_d in in_dataset:\n",
    "    if not in_d in sample_dict.keys():\n",
    "        print(\"Check the in_dataset string... \", sample_dict.keys())\n",
    "    else : \n",
    "        datasets.append(sample_dict[in_d])\n",
    "print(\"Datasets to process : \", [d.label for d in datasets])\n",
    "\n",
    "\n",
    "chain = {}\n",
    "ntot_events = {}\n",
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        samples_list = d.components\n",
    "    else:\n",
    "        samples_list = [d]\n",
    "    chain[d.label] = {}\n",
    "    ntot_events[d.label] = {}\n",
    "    for s in samples_list:\n",
    "        if distributed: \n",
    "            nfiles = len(samples[d.label][s.label]['strings'])\n",
    "            for i, string in enumerate(samples[d.label][s.label]['strings']): \n",
    "                samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"root://stormgf2.pi.infn.it/\")\n",
    "            chain[d.label][s.label] = samples[d.label][s.label]['strings']\n",
    "        else: \n",
    "            nfiles = nfiles_max\n",
    "            chain[d.label][s.label] = samples[d.label][s.label]['strings'][:nfiles]\n",
    "        if not \"Data\" in s.label: ntot_events[d.label][s.label] = np.sum(samples[d.label][s.label]['ntot'][:nfiles])\n",
    "        else: ntot_events[d.label][s.label] = None\n",
    "        print(\"Dataset : \"+s.label)\n",
    "        print(\"# of files to process : \", nfiles)\n",
    "        if distributed:\n",
    "            print(\"files strings :\\n  {}\\n  {}\\n  ... \\n  {}\\n  {}\".format(chain[d.label][s.label][0], chain[d.label][s.label][1], chain[d.label][s.label][-2], chain[d.label][s.label][-1]))\n",
    "        else :\n",
    "            print(\"files strings :\\n  {}\".format(chain[d.label][s.label][0]))\n",
    "        print(\"# of total events in the files to process (MC only, if Data the number is None) : \", ntot_events[d.label][s.label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54586805-5e2c-4b36-a13b-11497d5f2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invLepveto = \"\"#\"!\" # per non invertirlo basta passare stinga vuota\n",
    "# met_cut = 250\n",
    "# mdphi_cut = 0\n",
    "# HLT_filter = \"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60 || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight || HLT_Ele32_WPTight_Gsf || HLT_Ele115_CaloIdVT_GsfTrkIdT || HLT_Photon200 || HLT_IsoMu24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31a9be5b-2e52-431b-9709-ee1c3e639b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/distributed/client.py:1128: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+---------+--------+-----------+---------+\n",
      "| Package | client | scheduler | workers |\n",
      "+---------+--------+-----------+---------+\n",
      "| msgpack | 1.0.3  | 1.0.2     | 1.0.3   |\n",
      "| toolz   | 0.11.2 | 0.11.1    | 0.11.2  |\n",
      "+---------+--------+-----------+---------+\n",
      "Notes: \n",
      "-  msgpack: Variation is ok, as long as everything is above 0.6\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "end of initialization\n"
     ]
    }
   ],
   "source": [
    "# initialization of clusters\n",
    "\n",
    "# upload the proxyfile to the Dask workers to make them able to access data on the grid \n",
    "\n",
    "from distributed.diagnostics.plugin import UploadFile\n",
    "def set_proxy(dask_worker):\n",
    "    import os\n",
    "    import shutil\n",
    "    working_dir = dask_worker.local_directory\n",
    "    proxy_name = 'x509up_u0'\n",
    "    os.environ['X509_USER_PROXY'] = working_dir + '/' + proxy_name\n",
    "    os.environ['X509_CERT_DIR']=\"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "    shutil.copyfile(working_dir + '/' + proxy_name, working_dir + '/../../../proxy')    \n",
    "    os.environ['EXTRA_CLING_ARGS'] = \"-O2\"\n",
    "    return os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\")\n",
    "\n",
    "text_file = open(\"utils/postselection.h\", \"r\")\n",
    "data = text_file.read()\n",
    "def my_initialization_function():\n",
    "    print(ROOT.gInterpreter.ProcessLine(\".O\"))\n",
    "    ROOT.gInterpreter.Declare('{}'.format(data))\n",
    "    print(\"end of initialization\")\n",
    "\n",
    "# set up everything properly\n",
    "if distributed == True:\n",
    "    RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame\n",
    "    client = Client(address=\"tcp://127.0.0.1:\"+str(sched_port))\n",
    "    client.restart()\n",
    "    client.register_worker_plugin(UploadFile(\"/tmp/x509up_u0\"))\n",
    "    client.run(set_proxy)\n",
    "    ROOT.RDF.Experimental.Distributed.initialize(my_initialization_function)\n",
    "else:\n",
    "    RDataFrame = ROOT.RDataFrame\n",
    "    my_initialization_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963af90a-de0c-44a7-9b64-3927c34eeffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### utils ###################\n",
    "def cut_string(cut):\n",
    "    return cut.replace(\" \", \"\").replace(\"&&\",\"_\").replace(\">\",\"_g_\").replace(\".\",\"_\").replace(\"==\",\"_e_\")\n",
    "\n",
    "################### preselection ###############\n",
    "def preselection(df):\n",
    "    \n",
    "    # df = df.Filter(\"MET_pt>250\")\n",
    "    \n",
    "    df = df.Define(\"GoodJet_idx\", \"GetGoodJet(Jet_pt, Jet_eta, Jet_jetId)\")\n",
    "    df = df.Define(\"nGoodJet\", \"nGoodJet(GoodJet_idx)\")\n",
    "    \n",
    "    # if 'leptonveto' in cut:\n",
    "    #     df = df.Filter(invLepveto+\"LepVeto(Electron_pt, Electron_eta, Electron_cutBased, Muon_pt, Muon_eta, Muon_looseId )\", \"Lepton Veto\")\n",
    "    #     if \"&& leptonveto\" in cut:\n",
    "    #         c_ = cut.replace(\"&& leptonveto\",\"\")\n",
    "    #     elif \"leptonveto &&\" in cut:\n",
    "    #         c_ = cut.replace(\"leptonveto &&\",\"\")\n",
    "    #     elif \"leptonveto\" in cut:\n",
    "    #         c_ = cut.replace(\"leptonveto\",\"\")    \n",
    "    # else: \n",
    "    #     df = df\n",
    "    #     c_ = cut\n",
    "    df = df.Define(\"nTightElectron\", \"nTightElectron(Electron_pt, Electron_eta, Electron_cutBased)\")\n",
    "    df = df.Define(\"TightElectron_idx\", \"TightElectron_idx(Electron_pt, Electron_eta, Electron_cutBased)\")\n",
    "    df = df.Define(\"nVetoElectron\", \"nVetoElectron(Electron_pt, Electron_cutBased)\")\n",
    "    df = df.Define(\"nTightMuon\", \"nTightMuon(Muon_pt, Muon_eta, Muon_tightId)\")\n",
    "    df = df.Define(\"TightMuon_idx\", \"TightMuon_idx(Muon_pt, Muon_eta, Muon_tightId)\")\n",
    "    df = df.Define(\"nVetoMuon\", \"nVetoMuon(Muon_pt, Muon_eta, Muon_looseId)\")\n",
    "    df = df.Define(\"Lepton_flavour\", \"Lepton_flavour(nTightElectron, nTightMuon)\").Define(\"Lep_pt\", \"Lepton_var(Lepton_flavour, Electron_pt, TightElectron_idx, Muon_pt, TightMuon_idx)\").Define(\"Lep_phi\", \"Lepton_var(Lepton_flavour, Electron_phi, TightElectron_idx, Muon_phi, TightMuon_idx)\")\n",
    "    df = df.Define(\"MT\", \"sqrt(2 * Lep_pt * MET_pt * (1 - cos(Lep_phi - MET_phi)))\")\n",
    "    \n",
    "    # df = df.Filter(\"MET_pt>\"+ str(met_cut), \"MET_pt>\"+ str(met_cut))\n",
    "    # df = df.Filter(\"MinDelta_phi>\"+ str(mdphi_cut), \"MinDeltaPhi>\"+ str(mdphi_cut))\n",
    "    \n",
    "    # df = df.Filter(\"atLeast1jet_setparams(Jet_pt, Jet_eta, Jet_mass, Jet_jetId, 30, 4, 0, 1)\", \"At_least_1Ak4\")\n",
    "    # df = df.Filter(\"atLeast1fatjet_setparams(FatJet_pt, FatJet_msoftdrop, FatJet_eta, FatJet_jetId, 200, 6, 40, 1)\", \"at_least_1Ak8\")\n",
    "\n",
    "    df = df.Define(\"LeadingJetPt_idx\", \"GetLeadingPtJet(Jet_pt)\")\n",
    "    df = df.Define(\"LeadingJetPt_pt\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_pt)\")\n",
    "    df = df.Define(\"LeadingJetPt_eta\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_eta)\")\n",
    "    df = df.Define(\"LeadingJetPt_phi\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_phi)\")\n",
    "    df = df.Define(\"LeadingJetPt_mass\", \"GetLeadingJetVar(LeadingJetPt_idx, Jet_mass)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_idx\", \"GetLeadingPtJet(FatJet_pt)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_pt\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_pt)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_eta\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_eta)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_phi\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_phi)\")\n",
    "    df = df.Define(\"LeadingFatJetPt_mass\", \"GetLeadingJetVar(LeadingFatJetPt_idx, FatJet_mass)\")\n",
    "    df = df.Define(\"LeadingMuonPt_idx\", \"GetLeadingPtLep(Muon_pt, Muon_eta, Muon_looseId)\")\n",
    "    df = df.Define(\"LeadingMuonPt_pt\", \"GetLeadingJetVar(LeadingMuonPt_idx, Muon_pt)\")\n",
    "    df = df.Define(\"LeadingMuonPt_eta\", \"GetLeadingJetVar(LeadingMuonPt_idx, Muon_eta)\")\n",
    "    df = df.Define(\"LeadingMuonPt_phi\", \"GetLeadingJetVar(LeadingMuonPt_idx, Muon_phi)\")\n",
    "    df = df.Define(\"LeadingElectronPt_idx\", \"GetLeadingPtLep(Electron_pt, Electron_eta, Electron_cutBased)\")\n",
    "    df = df.Define(\"LeadingElectronPt_pt\", \"GetLeadingJetVar(LeadingElectronPt_idx, Electron_pt)\")\n",
    "    df = df.Define(\"LeadingElectronPt_eta\", \"GetLeadingJetVar(LeadingElectronPt_idx, Electron_eta)\")\n",
    "    df = df.Define(\"LeadingElectronPt_phi\", \"GetLeadingJetVar(LeadingElectronPt_idx, Electron_phi)\")\n",
    "    \n",
    "    df = df.Define(\"nForwardJet\", \"nForwardJet(Jet_pt, Jet_jetId, Jet_eta)\")\n",
    "    df = df.Define(\"nJetBtag\", \"njetbtag(GoodJet_idx, Jet_btagDeepFlavB)\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "############### trigger selection #####################\n",
    "def trigger_filter(df):\n",
    "    df_trig = df.Filter(\"HLT_PFMET120_PFMHT120_IDTight || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight\", \"trigger\")\n",
    "    return df_trig\n",
    "\n",
    "############### top selection ########################\n",
    "def select_top(df, isMC):\n",
    "    # return indices of the FatJet with particleNet score over the thresholds \n",
    "    df_goodtopMer = df.Define(\"GoodTopMer_idx\", \"select_TopMer(FatJet_particleNet_TvsQCD)\")\n",
    "    # return indices of the TopMixed over the threshold with any object in common\n",
    "    df_goodtopMix = df_goodtopMer.Define(\"GoodTopMix_idx\", \"select_TopMix(TopMixed_TopScore, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2)\")\n",
    "    # return indices of the TopResolved over the threshold with any object in common\n",
    "    df_goodtopRes = df_goodtopMix.Define(\"GoodTopRes_idx\", \"select_TopRes(TopResolved_TopScore, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2)\")\n",
    "    \n",
    "    df_nTops = df_goodtopRes.Define(\"nGoodTopResolved\", \"nTop(GoodTopRes_idx)\")\\\n",
    "                            .Define(\"nGoodTopMixed\", \"nTop(GoodTopMix_idx)\")\\\n",
    "                            .Define(\"nGoodTopMerged\", \"nTop(GoodTopMer_idx)\")\n",
    "    \n",
    "    \n",
    "    # return:  1- Event Resolved, 2- Event Mixed, 3- Event Merged, 4- Event Nothing, ...\n",
    "    df_topcategory = df_nTops.Define(\"EventTopCategory\", \"select_TopCategory(GoodTopMer_idx, GoodTopMix_idx, GoodTopRes_idx)\")\n",
    "    if isMC:\n",
    "        df_topcategory = df_topcategory.Define(\"EventTopCategoryWithTruth\", \"select_TopCategoryWithTruth(EventTopCategory, FatJet_matched, GoodTopMer_idx, TopMixed_truth, GoodTopMix_idx, TopResolved_truth, GoodTopRes_idx)\")\n",
    "    \n",
    "    df_topselected = df_topcategory.Define(\"Top_idx\",\n",
    "                                           \"select_bestTop(EventTopCategory, FatJet_particleNet_TvsQCD, TopMixed_TopScore, TopResolved_TopScore)\")\n",
    "    # return best top idx wrt category --> the idx is referred to the list of candidates fixed by the EventTopCategory\n",
    "    df_topvariables = df_topselected.Define(\"Top_pt\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_pt, TopMixed_pt, TopResolved_pt)\")\\\n",
    "                        .Define(\"Top_eta\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_eta, TopMixed_eta, TopResolved_eta)\")\\\n",
    "                        .Define(\"Top_phi\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_phi, TopMixed_phi, TopResolved_phi)\")\\\n",
    "                        .Define(\"Top_mass\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_mass, TopMixed_mass, TopResolved_mass)\")\\\n",
    "                        .Define(\"Top_score\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_particleNet_TvsQCD, TopMixed_TopScore, TopResolved_TopScore)\")\\\n",
    "                        .Define(\"Top_isolationPtJetsdR04\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 0.4, 1)\")\\\n",
    "                        .Define(\"Top_isolationPtJetsdR06\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 0.6, 1)\")\\\n",
    "                        .Define(\"Top_isolationPtJetsdR08\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 0.8, 1)\")\\\n",
    "                        .Define(\"Top_isolationPtJetsdR12\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 1.2, 1)\")\\\n",
    "                        .Define(\"Top_isolationNJetsdR04\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 0.4, 0)\")\\\n",
    "                        .Define(\"Top_isolationNJetsdR06\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 0.6, 0)\")\\\n",
    "                        .Define(\"Top_isolationNJetsdR08\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 0.8, 0)\")\\\n",
    "                        .Define(\"Top_isolationNJetsdR12\",\"TopIsolation_NJets(EventTopCategory, Top_idx, TopMixed_idxFatJet, TopMixed_idxJet0, TopMixed_idxJet1, TopMixed_idxJet2, TopMixed_pt, TopMixed_phi, TopMixed_eta, TopResolved_idxJet0, TopResolved_idxJet1, TopResolved_idxJet2, TopResolved_pt, TopResolved_phi, TopResolved_eta, FatJet_pt, FatJet_eta, FatJet_phi, FatJet_jetId, Jet_pt, Jet_eta, Jet_phi, Jet_jetId, 1.2, 0)\")\n",
    "\n",
    "    if isMC:\n",
    "        df_topvariables = df_topvariables.Define(\"Top_truth\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_matched, TopMixed_truth, TopResolved_truth)\")\n",
    "    # NB: TopTruth for Merged is replaced with FatJet_matched, the variable is between 0 and 3 \n",
    "    # where 3 means true end less than 3 means false \n",
    "    return df_topvariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69f0bf61-b2ab-422e-8baf-9abba2b984cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookhisto(df, regions_def, var, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var:\n",
    "            if v._MConly and not sampleflag: \n",
    "                continue\n",
    "            else:\n",
    "                if regions_def[reg] == \"\":\n",
    "                    if v._noUnOvFlowbin:\n",
    "                        h_[reg][v._name]= df.Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                    else:\n",
    "                        h_[reg][v._name]= df.Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                else:\n",
    "                    if v._noUnOvFlowbin:\n",
    "                        h_[reg][v._name]= df.Filter(regions_def[reg]).Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                    else:\n",
    "                        h_[reg][v._name]= df.Filter(regions_def[reg]).Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "                    \n",
    "    return h_\n",
    "\n",
    "def bookhisto2D(df, regions_def, var2d, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var2d:\n",
    "            if regions_def[reg]==\"\":\n",
    "                h_[reg][v._name] = df.Redefine(v._xname, \"UnOvBin(\"+v._xname+\",\"+str(v._nxbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\")\\\n",
    "                                     .Redefine(v._yname, \"UnOvBin(\"+v._yname+\",\"+str(v._nybins)+\",\"+str(v._ymin)+\",\"+str(v._ymax)+\")\")\\\n",
    "                                     .Histo2D((v._xname+\"Vs\"+v._yname+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax), v._xname, v._yname)\n",
    "            else:\n",
    "                h_[reg][v._name] = df.Filter(regions_def[reg])\\\n",
    "                                     .Redefine(v._xname, \"UnOvBin(\"+v._xname+\",\"+str(v._nxbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\")\\\n",
    "                                     .Redefine(v._yname, \"UnOvBin(\"+v._yname+\",\"+str(v._nybins)+\",\"+str(v._ymin)+\",\"+str(v._ymax)+\")\")\\\n",
    "                                     .Histo2D((v._xname+\"Vs\"+v._yname+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax), v._xname, v._yname)\n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37967949-f502-4c77-9ceb-dd04f8062cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def savehisto(d, h, regions_def, var, s_cut):\n",
    "    histo = {reg: {v._name: ROOT.TH1D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax) for v in var} for reg in regions_def.keys()}\n",
    "    isMC=True\n",
    "    if \"Data\" in d.label: isMC = False\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    \n",
    "    for s in s_list:\n",
    "        outfile = ROOT.TFile.Open(repohisto+s.label+'.root', \"RECREATE\")\n",
    "        for reg in regions_def.keys():\n",
    "            for v in var:\n",
    "                if v._MConly and not isMC:\n",
    "                    continue\n",
    "                else:\n",
    "                    histo[reg][v._name] = h[d.label][s.label][reg][v._name].GetValue()      \n",
    "                    outfile.cd()\n",
    "                    histo[reg][v._name].Write()\n",
    "        outfile.Close()\n",
    "\n",
    "# i plot2d per il momento non ci servono, si deve trovare un modo più intelligente di farli\n",
    "def savehisto2d(d, h, regions_def, var2d, s_cut):\n",
    "    histo = {reg: {v._name: ROOT.TH2D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._xtitle+\";\"+v._ytitle, v._nxbins, v._xmin, v._xmax, v._nybins, v._ymin, v._ymax,) for v in var2d} for reg in regions_def.keys()}\n",
    "        \n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    \n",
    "    for s in s_list:\n",
    "        outfile = ROOT.TFile.Open(repohisto+s.label+'_2D.root', \"RECREATE\")\n",
    "        for reg in regions_def.keys():\n",
    "            for v in histo[reg].keys():\n",
    "                histo[reg][v] = h[d.label][s.label][reg][v].GetValue()      \n",
    "                outfile.cd()\n",
    "                histo[reg][v].Write()\n",
    "        outfile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54c96d28-45f4-4877-8d00-680e095a3016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#samples['DataHTF_2022']['DataHTF_2022']['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab036200-65f9-44a0-abbb-8a04003fc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples[d.label][d.components[0]]['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b72ddfd-d641-437d-a78c-8568a0a17a30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loop on datasets:  ['DataMETA_2018']\n",
      "Local time : 2024-01-18 15:54:53.869160\n",
      "Initializing DataFrame for DataMETA_2018 chain len =  43\n",
      "All histos booked !\n",
      "DataMETA_2018 histos saved\n",
      "Job finished in:  0:25:24.258631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TClass::Init>: no dictionary for class edm::Hash<1> is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ParameterSetBlob is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessHistory is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessConfiguration is available\n",
      "Warning in <TClass::Init>: no dictionary for class pair<edm::Hash<1>,edm::ParameterSetBlob> is available\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "print(\"starting loop on datasets: \",[d.label for d in datasets])\n",
    "print(\"Local time :\", t0)\n",
    "# print(\"requirements: \"+cut)\n",
    "\n",
    "h = {}\n",
    "h_2D = {}\n",
    "\n",
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "\n",
    "    if 'Data' in d.label : sampleflag = 0\n",
    "    else: sampleflag = 1\n",
    "    c_ = cut\n",
    "    h[d.label] = {}\n",
    "    h_2D[d.label] = {}\n",
    "    for s in s_list:\n",
    "        \n",
    "        print(\"Initializing DataFrame for \"+ s.label +\" chain len = \", len(chain[d.label][s.label]))\n",
    "        if len(chain[d.label][s.label])==1: print(chain[d.label][s.label])\n",
    "        if distributed ==True:\n",
    "            df = RDataFrame(\"Events\", chain[d.label][s.label], npartitions=nmaxpartition, \n",
    "                            daskclient=client, monitor_label = \"main\" )\n",
    "        else:\n",
    "            df = RDataFrame(\"Events\", chain[d.label][s.label])\n",
    "        \n",
    "        \n",
    "        df_ismc         = df.Define(\"isMC\", \"isMC(\"+str(sampleflag)+\")\")\n",
    "        df_year         = df_ismc.Define(\"year\", str(s.year))\n",
    "        df_hemveto      = df_year.Define(\"HEMVeto\", \"hemveto(Jet_eta, Jet_phi, Electron_eta, Electron_phi)\")\n",
    "        df_hemveto      = df_hemveto.Filter(\"(isMC || (year != 2018) || (HEMVeto || run<319077.))\")\n",
    "        df_hlt          = trigger_filter(df_hemveto)        \n",
    "        ### per gli HLT potremmo volerli modificare a seconda della regione (?) in tal caso la funzione\n",
    "        ### va spostata nel book histos e va passata la regione per attivare trigger diversi\n",
    "        df_wnom         = df_hlt.Define('w_nominal', '1')\n",
    "        df_presel       = preselection(df_wnom)\n",
    "        df_topsel       = select_top(df_presel, sampleflag)\n",
    "        \n",
    "        \n",
    "        if do_snapshot:\n",
    "            opts = ROOT.RDF.RSnapshotOptions()\n",
    "            opts.fLazy = True\n",
    "            if distributed: fold = \"./\"\n",
    "            else: fold = folder\n",
    "            snapshot_df = df_topsel.Snapshot(\"events_nominal\", fold+\"snap_\"+s.label+\".root\", branches, opts)\n",
    "            # print(\"./\"+s.label+\".root\")\n",
    "        if do_histos:\n",
    "            s_cut = cut_string(cut)\n",
    "            if len(var) != 0 :\n",
    "                h[d.label][s.label] = bookhisto(df_topsel, regions_def, var, s_cut)\n",
    "            if len(var2d) != 0 :\n",
    "                h_2D[d.label][s.label] = bookhisto2D(df_topsel, regions_def, var2d, s_cut)\n",
    "# if not distributed:\n",
    "#     df_presel.Report().Print()\n",
    "\n",
    "if do_histos:\n",
    "    print(\"All histos booked !\")\n",
    "    for d in datasets:\n",
    "        if len(var):\n",
    "            savehisto(d, h, regions_def, var, s_cut)\n",
    "        if len(var2d) != 0 :\n",
    "            savehisto2d(d, h_2D, regions_def, var2d, s_cut)\n",
    "        print(d.label + \" histos saved\")\n",
    "if do_snapshot:\n",
    "    snapshot_df.GetValue()\n",
    "    if distributed: \n",
    "        client.run(transfer_to_tier)\n",
    "        print(\"Snapshots saved and trasfered to tier\")\n",
    "    print(\"Sanpshot done!\")\n",
    "t1 = datetime.now()\n",
    "print(\"Job finished in: \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9573b747-9a5a-4501-b084-481b10d418ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ./results/run2018_regions_v0/plots/DataMETA_2018.root Title: \n",
      "All 13335272.0\n",
      "Presel 629562.0\n",
      "AH 29375.0\n",
      "SL 576935.0\n",
      "SEl 22264.0\n",
      "SMu 554671.0\n",
      "AH1lWR 347197.0\n",
      "AH1lWREl 28296.0\n",
      "AH1lWRMu 318901.0\n",
      "PreselResolved 5918.0\n",
      "PreselMixed 14959.0\n",
      "PreselMerged 7529.0\n",
      "PreselNoTop 601156.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "file = ROOT.TFile.Open(repohisto+s.label+\".root\")\n",
    "print(file)\n",
    "# for a in file.GetListOfKeys(): print(a)\n",
    "for reg in regions_def.keys():\n",
    "    # for v in var:\n",
    "    hist = file.Get(var[0]._name+\"_\"+reg+\"_\")\n",
    "    print(reg, hist.Integral())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3606a5b-e3ce-491d-a86a-6c02431ca435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions_def.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269963fc-638b-4a19-8c8d-af26bad6e37f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# region = \"NoCut\"\n",
    "# var = \"MET_pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa57df-4a5e-46d7-b758-84edcebca76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if do_snapshot and distributed:\n",
    "    print(os.popen(\"davix-ls davs://stwebdav.pi.infn.it:8443/cms/store/user/acagnott/{}/{}/ -E /tmp/x509up_u0 --capath /cvmfs/cms.cern.ch/grid/etc/grid-security/certificates/\".format(remote_folder_name, remote_subfolder_name)).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af25cc-cc36-437c-94e7-42aaa09cbfcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity kernel",
   "language": "python",
   "name": "singularity-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
